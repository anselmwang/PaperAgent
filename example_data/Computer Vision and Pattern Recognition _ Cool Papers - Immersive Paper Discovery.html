<!DOCTYPE html>
<!-- saved from url=(0040)https://papers.cool/arxiv/cs.CV?show=200 -->
<html><script src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/hm.js.download"></script><script id="allow-copy_script">(function agent() {
    let unlock = false
    document.addEventListener('allow_copy', (event) => {
      unlock = event.detail.unlock
    })

    const copyEvents = [
      'copy',
      'cut',
      'contextmenu',
      'selectstart',
      'mousedown',
      'mouseup',
      'mousemove',
      'keydown',
      'keypress',
      'keyup',
    ]
    const rejectOtherHandlers = (e) => {
      if (unlock) {
        e.stopPropagation()
        if (e.stopImmediatePropagation) e.stopImmediatePropagation()
      }
    }
    copyEvents.forEach((evt) => {
      document.documentElement.addEventListener(evt, rejectOtherHandlers, {
        capture: true,
      })
    })
  })()</script><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Computer Vision and Pattern Recognition | Cool Papers - Immersive Paper Discovery</title>
    <meta name="description" content="Computer Vision and Pattern Recognition | Explore the latest in academic research with Cool Papers. Our platform, powered by Kimi Chat AI, streamlines the discovery of arXiv and top conference papers, offering an interactive FAQ for quick insights.">
    <meta name="keywords" content="Cool Papers, Immersive Discovery, arXiv Research, AI Paper Assistant, Paper FAQ, Kimi Chat, Scholarly Papers, Academic Research, Paper Screening AI, Conference Papers, Research Paper Exploration">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="https://papers.cool/static/favicon.ico">
    <link rel="stylesheet" href="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/font-awesome.min.css">
    <link rel="stylesheet" href="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/flatpickr.min.css">
    <link rel="stylesheet" href="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/style.css">
<style id="luonq0uq.vr6">.pb-toast-main {
  z-index: 2147483639 !important;
  position: fixed !important;
  top: -50px !important;
  left: 0px !important;
  width: 100% !important;
  height: 44px !important;
  border: none !important;
  box-shadow: 0 1px 0 0 #b6b4b6 !important;
  transition: top 0.3s;
}

.pb-toast-main-move {
  top: 0px !important;
}

.pb-toast-main-show {
  transition: none;
  top: 0px !important;
}



#pb_jq_tipsWrapper {
  position: fixed !important;
  width: 230px !important;
  background-color: rgba(0, 0, 0, 0.8);
  box-shadow: 0 8px 20px 0 rgba(0, 0, 0, 0.2);
  font-family: "Lucida Grande", tahoma, verdana, arial, sans-serif !important;
  border-radius: 5px !important;
  color: #ffffff !important;
  z-index: 2147483641 !important;
  padding: 15px !important;
  font-size: 14px !important;
}

#pb_jq_tipsWrapper:before {
  position: absolute !important;
  top: -10px !important;
  right: 60px !important;
  display: inline-block !important;
  border-right: 10px solid transparent !important;
  border-bottom: 10px solid #000 !important;
  border-left: 10px solid transparent !important;
  border-bottom-color: rgba(0, 0, 0, 0.2) !important;
  content: '' !important;
}

#pb_jq_tipsWrapper:after {
  position: absolute !important;
  top: -9px !important;
  right: 60px !important;
  display: inline-block !important;
  border-right: 9px solid transparent !important;
  border-bottom: 9px solid #000 !important;
  border-left: 9px solid transparent !important;
  content: '' !important;
}

#pb-link-copied-message {
  display: none;
  position: fixed;
  width: 90px;
  height: 29px;
  opacity: 0;
  border-radius: 100px;
  background-color: rgba(0, 0, 0, 0.7);
  z-index: 2147483641;
  font-family: "Lucida Grande", tahoma, verdana, arial, sans-serif !important;
  font-size: 13px;
  line-height: 29px;
  text-align: center;
  color: #ffffff;
}</style><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}


.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color), 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color), 110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color), 120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}


.immersive-translate-search-recomend {
  border: 1px solid #dadce0;
  border-radius: 8px;
  padding: 16px;
  margin-bottom: 16px;
  position: relative;
  font-size: 16px;
}

.immersive-translate-search-enhancement-en-title {
  color: #4d5156;
}

/* dark */
@media (prefers-color-scheme: dark) {
  .immersive-translate-search-recomend {
    border: 1px solid #3c4043;
  }

  .immersive-translate-close-action svg {
    fill: #bdc1c6;
  }

  .immersive-translate-search-enhancement-en-title {
    color: #bdc1c6;
  }
}


.immersive-translate-search-settings {
  position: absolute;
  top: 16px;
  right: 16px;
  cursor: pointer;
}

.immersive-translate-search-recomend::before {
  /* content: " "; */
  /* width: 20px; */
  /* height: 20px; */
  /* top: 16px; */
  /* position: absolute; */
  /* background: center / contain url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAxlBMVEUAAADpTInqTIjpSofnSIfqS4nfS4XqS4nqTIjsTYnrTInqTIroS4jvQIDqTIn////+/v7rSYjpTIn8/v7uaZzrTIr9/f3wfansWJL88/b85e73qc39+/v3xNnylrvrVI/98fb62Obva5/8+fr76vH4y9zpSIj74e353Oj1ocTzm77xhK/veKbtYpjsXJTqU47oTInxjrXyh7L99fj40eH2ttH1udD3sc31ssz1rMnykLXucqPtbqD85e/1xdn2u9DzqcXrUY6FaJb8AAAADnRSTlMA34BgIM8Q37/fz7+/EGOHcVQAAAGhSURBVDjLhZPncuowEEZFTW7bXVU7xsYYTO/p7bb3f6lICIOYJOT4h7/VnFmvrBFjrF3/CR/SajBHswafctG0Qg3O8O0Xa8BZ6uw7eLjqr30SofCDVSkemMinfL1ecy20r5ygR5zz3ArcAqJExPTPKhDENEmS30Q9+yo4lEQkqVTiIEAHCT10xWERRdH0Bq0aCOPZNDV3s0xaYce1lHEoDHU8wEh3qRJypNcTAeKUIjgKMeGLDoRCLVLTVf+Ownj8Kk6H9HM6QXPgYjQSB0F00EJEu10ILQrs/QeP77BSSr0MzLOyuJJQbnUoOOIUI/A8EeJk9E4YUHUWiRyTVKGgQUB8/3e/NpdGlfI+FMQyWsCBWyz4A/ZyHXyiiz0Ne5aGZssoxRmcChw8/EFKQ5JwwkUo3FRT5yXS7q+Y/rHDZmFktzpGMvO+5QofA4FPpEmGw+EWRCFvnaof7Zhe8NuYSLR0xErKLThUSs8gnODh87ssy6438yzbLzxl012HS19vfCf3CNhnbWOL1eEsDda+gDPUvri8tSZzNFrwIZf1NmNvqC1I/t8j7nYAAAAASUVORK5CYII='); */
}

.immersive-translate-search-title {}

.immersive-translate-search-title-wrapper {}

.immersive-translate-search-time {
  font-size: 12px;
  margin: 4px 0 24px;
  color: #70757a;
}

.immersive-translate-expand-items {
  display: none;
}

.immersive-translate-search-more {
  margin-top: 16px;
  font-size: 14px;
}

.immersive-translate-modal {
  display: none;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border: 1px solid #888;
  border-radius: 10px;
  width: 80%;
  max-width: 270px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  word-break: break-all;
  margin-top: 24px;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  color: black;
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 16px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}


.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #EA4C89;
  border: 1px solid #EA4C89
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #007bff;
  -webkit-tap-highlight-color: rgba(0, 0, 0, .1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, .1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}</style></head>
<body id="arxiv" style="">
    <h1>Computer Vision and Pattern Recognition</h1>
    <p class="info">
        Date: <a onclick="openCalendar(&#39;&#39;, this, &#39;2024-04-05&#39;, &#39;_self&#39;)" class="date">Fri, 5 Apr 2024</a> | 
        Total: 93
    </p>
    <div class="papers">
        <div id="2404.03658" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03658" target="_blank" title="1/93"><span class="index">#1</span></a>
                <a id="title-2404.03658" class="title-link" href="https://papers.cool/arxiv/2404.03658" target="_blank">Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</a>
                <a id="pdf-2404.03658" class="title-pdf" onclick="togglePdf(&#39;2404.03658&#39;, &#39;https://arxiv.org/pdf/2404.03658.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03658">8</sup>]</a>
                <a id="copy-2404.03658" class="title-copy" onclick="copyToClipboard(&#39;2404.03658&#39;)">[Copy]</a>
                <a id="kimi-2404.03658" class="title-kimi" onclick="chatKimi(&#39;2404.03658&#39;, this)">[Kimi<sup id="kimi-stars-2404.03658">22</sup>]</a>
            </h2>
            <p id="authors-2404.03658" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rui%20Li" target="_blank"><span class="author">Rui Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tobias%20Fischer" target="_blank"><span class="author">Tobias Fischer<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mattia%20Segu" target="_blank"><span class="author">Mattia Segu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Marc%20Pollefeys" target="_blank"><span class="author">Marc Pollefeys<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Luc%20Van%20Gool" target="_blank"><span class="author">Luc Van Gool<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Federico%20Tombari" target="_blank"><span class="author">Federico Tombari<span></span></span></a>
            </p>
            <p id="summary-2404.03658" class="summary">Recovering the 3D scene geometry from a single view is a fundamental yet ill-posed problem in computer vision. While classical depth estimation methods infer only a 2.5D scene representation limited to the image plane, recent approaches based on radiance fields reconstruct a full 3D representation. However, these methods still struggle with occluded regions since inferring geometry without visual observation requires (i) semantic knowledge of the surroundings, and (ii) reasoning about spatial context. We propose KYN, a novel method for single-view scene reconstruction that reasons about semantic and spatial context to predict each point's density. We introduce a vision-language modulation module to enrich point features with fine-grained semantic information. We aggregate point representations across the scene through a language-guided spatial attention mechanism to yield per-point density predictions aware of the 3D semantic context. We show that KYN improves 3D shape recovery compared to predicting density for each 3D point in isolation. We achieve state-of-the-art results in scene and object reconstruction on KITTI-360, and show improved zero-shot generalization compared to prior work. Project page: https://ruili3.github.io/kyn.</p>
            <div id="pdf-container-2404.03658" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03658" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource.html"></iframe>
            </div>
            <div id="kimi-container-2404.03658" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03657" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03657" target="_blank" title="2/93"><span class="index">#2</span></a>
                <a id="title-2404.03657" class="title-link" href="https://papers.cool/arxiv/2404.03657" target="_blank">OW-VISCap: Open-World Video Instance Segmentation and Captioning</a>
                <a id="pdf-2404.03657" class="title-pdf" onclick="togglePdf(&#39;2404.03657&#39;, &#39;https://arxiv.org/pdf/2404.03657.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03657">7</sup>]</a>
                <a id="copy-2404.03657" class="title-copy" onclick="copyToClipboard(&#39;2404.03657&#39;)">[Copy]</a>
                <a id="kimi-2404.03657" class="title-kimi" onclick="chatKimi(&#39;2404.03657&#39;, this)">[Kimi<sup id="kimi-stars-2404.03657">16</sup>]</a>
            </h2>
            <p id="authors-2404.03657" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anwesa%20Choudhuri" target="_blank"><span class="author">Anwesa Choudhuri<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Girish%20Chowdhary" target="_blank"><span class="author">Girish Chowdhary<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alexander%20G.%20Schwing" target="_blank"><span class="author">Alexander G. Schwing<span></span></span></a>
            </p>
            <p id="summary-2404.03657" class="summary">Open-world video instance segmentation is an important video understanding task. Yet most methods either operate in a closed-world setting, require an additional user-input, or use classic region-based proposals to identify never before seen objects. Further, these methods only assign a one-word label to detected objects, and don't generate rich object-centric descriptions. They also often suffer from highly overlapping predictions. To address these issues, we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap), an approach to jointly segment, track, and caption previously seen or unseen objects in a video. For this, we introduce open-world object queries to discover never before seen objects without additional user-input. We generate rich and descriptive object-centric captions for each detected object via a masked attention augmented LLM input. We introduce an inter-query contrastive loss to ensure that the object queries differ from one another. Our generalized approach matches or surpasses state-of-the-art on three tasks: open-world video instance segmentation on the BURST dataset, dense video object captioning on the VidSTG dataset, and closed-world video instance segmentation on the OVIS dataset.</p>
            <div id="pdf-container-2404.03657" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03657" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(1).html"></iframe>
            </div>
            <div id="kimi-container-2404.03657" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03656" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03656" target="_blank" title="3/93"><span class="index">#3</span></a>
                <a id="title-2404.03656" class="title-link" href="https://papers.cool/arxiv/2404.03656" target="_blank">MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</a>
                <a id="pdf-2404.03656" class="title-pdf" onclick="togglePdf(&#39;2404.03656&#39;, &#39;https://arxiv.org/pdf/2404.03656.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03656">6</sup>]</a>
                <a id="copy-2404.03656" class="title-copy" onclick="copyToClipboard(&#39;2404.03656&#39;)">[Copy]</a>
                <a id="kimi-2404.03656" class="title-kimi" onclick="chatKimi(&#39;2404.03656&#39;, this)">[Kimi<sup id="kimi-stars-2404.03656">3</sup>]</a>
            </h2>
            <p id="authors-2404.03656" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hanzhe%20Hu" target="_blank"><span class="author">Hanzhe Hu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhizhuo%20Zhou" target="_blank"><span class="author">Zhizhuo Zhou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Varun%20Jampani" target="_blank"><span class="author">Varun Jampani<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shubham%20Tulsiani" target="_blank"><span class="author">Shubham Tulsiani<span></span></span></a>
            </p>
            <p id="summary-2404.03656" class="summary">We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches.</p>
            <div id="pdf-container-2404.03656" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03656" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(2).html"></iframe>
            </div>
            <div id="kimi-container-2404.03656" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03654" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03654" target="_blank" title="4/93"><span class="index">#4</span></a>
                <a id="title-2404.03654" class="title-link" href="https://papers.cool/arxiv/2404.03654" target="_blank">RaFE: Generative Radiance Fields Restoration</a>
                <a id="pdf-2404.03654" class="title-pdf" onclick="togglePdf(&#39;2404.03654&#39;, &#39;https://arxiv.org/pdf/2404.03654.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03654">3</sup>]</a>
                <a id="copy-2404.03654" class="title-copy" onclick="copyToClipboard(&#39;2404.03654&#39;)">[Copy]</a>
                <a id="kimi-2404.03654" class="title-kimi" onclick="chatKimi(&#39;2404.03654&#39;, this)">[Kimi<sup id="kimi-stars-2404.03654">5</sup>]</a>
            </h2>
            <p id="authors-2404.03654" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhongkai%20Wu" target="_blank"><span class="author">Zhongkai Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziyu%20Wan" target="_blank"><span class="author">Ziyu Wan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jing%20Zhang" target="_blank"><span class="author">Jing Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jing%20Liao" target="_blank"><span class="author">Jing Liao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dong%20Xu" target="_blank"><span class="author">Dong Xu<span></span></span></a>
            </p>
            <p id="summary-2404.03654" class="summary">NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website https://zkaiwu.github.io/RaFE-Project/.</p>
            <div id="pdf-container-2404.03654" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03654" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(3).html"></iframe>
            </div>
            <div id="kimi-container-2404.03654" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03653" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03653" target="_blank" title="5/93"><span class="index">#5</span></a>
                <a id="title-2404.03653" class="title-link" href="https://papers.cool/arxiv/2404.03653" target="_blank">CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</a>
                <a id="pdf-2404.03653" class="title-pdf" onclick="togglePdf(&#39;2404.03653&#39;, &#39;https://arxiv.org/pdf/2404.03653.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03653">5</sup>]</a>
                <a id="copy-2404.03653" class="title-copy" onclick="copyToClipboard(&#39;2404.03653&#39;)">[Copy]</a>
                <a id="kimi-2404.03653" class="title-kimi" onclick="chatKimi(&#39;2404.03653&#39;, this)">[Kimi<sup id="kimi-stars-2404.03653">15</sup>]</a>
            </h2>
            <p id="authors-2404.03653" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dongzhi%20Jiang" target="_blank"><span class="author">Dongzhi Jiang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guanglu%20Song" target="_blank"><span class="author">Guanglu Song<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaoshi%20Wu" target="_blank"><span class="author">Xiaoshi Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Renrui%20Zhang" target="_blank"><span class="author">Renrui Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dazhong%20Shen" target="_blank"><span class="author">Dazhong Shen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhuofan%20Zong" target="_blank"><span class="author">Zhuofan Zong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu%20Liu" target="_blank"><span class="author">Yu Liu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hongsheng%20Li" target="_blank"><span class="author">Hongsheng Li<span></span></span></a>
            </p>
            <p id="summary-2404.03653" class="summary">Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion model's insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance.</p>
            <div id="pdf-container-2404.03653" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03653" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(4).html"></iframe>
            </div>
            <div id="kimi-container-2404.03653" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03652" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03652" target="_blank" title="6/93"><span class="index">#6</span></a>
                <a id="title-2404.03652" class="title-link" href="https://papers.cool/arxiv/2404.03652" target="_blank">The More You See in 2D, the More You Perceive in 3D</a>
                <a id="pdf-2404.03652" class="title-pdf" onclick="togglePdf(&#39;2404.03652&#39;, &#39;https://arxiv.org/pdf/2404.03652.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03652">10</sup>]</a>
                <a id="copy-2404.03652" class="title-copy" onclick="copyToClipboard(&#39;2404.03652&#39;)">[Copy]</a>
                <a id="kimi-2404.03652" class="title-kimi" onclick="chatKimi(&#39;2404.03652&#39;, this)">[Kimi<sup id="kimi-stars-2404.03652">4</sup>]</a>
            </h2>
            <p id="authors-2404.03652" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xinyang%20Han" target="_blank"><span class="author">Xinyang Han<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zelin%20Gao" target="_blank"><span class="author">Zelin Gao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Angjoo%20Kanazawa" target="_blank"><span class="author">Angjoo Kanazawa<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shubham%20Goel" target="_blank"><span class="author">Shubham Goel<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yossi%20Gandelsman" target="_blank"><span class="author">Yossi Gandelsman<span></span></span></a>
            </p>
            <p id="summary-2404.03652" class="summary">Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning. The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods. We demonstrate our system on real images as well as standard synthetic benchmarks. Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.</p>
            <div id="pdf-container-2404.03652" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03652" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(5).html"></iframe>
            </div>
            <div id="kimi-container-2404.03652" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03650" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03650" target="_blank" title="7/93"><span class="index">#7</span></a>
                <a id="title-2404.03650" class="title-link" href="https://papers.cool/arxiv/2404.03650" target="_blank">OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views</a>
                <a id="pdf-2404.03650" class="title-pdf" onclick="togglePdf(&#39;2404.03650&#39;, &#39;https://arxiv.org/pdf/2404.03650.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03650"></sup>]</a>
                <a id="copy-2404.03650" class="title-copy" onclick="copyToClipboard(&#39;2404.03650&#39;)">[Copy]</a>
                <a id="kimi-2404.03650" class="title-kimi" onclick="chatKimi(&#39;2404.03650&#39;, this)">[Kimi<sup id="kimi-stars-2404.03650">2</sup>]</a>
            </h2>
            <p id="authors-2404.03650" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Francis%20Engelmann" target="_blank"><span class="author">Francis Engelmann<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fabian%20Manhardt" target="_blank"><span class="author">Fabian Manhardt<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michael%20Niemeyer" target="_blank"><span class="author">Michael Niemeyer<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Keisuke%20Tateno" target="_blank"><span class="author">Keisuke Tateno<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Marc%20Pollefeys" target="_blank"><span class="author">Marc Pollefeys<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Federico%20Tombari" target="_blank"><span class="author">Federico Tombari<span></span></span></a>
            </p>
            <p id="summary-2404.03650" class="summary">Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.</p>
            <div id="pdf-container-2404.03650" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03650" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(6).html"></iframe>
            </div>
            <div id="kimi-container-2404.03650" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03645" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03645" target="_blank" title="8/93"><span class="index">#8</span></a>
                <a id="title-2404.03645" class="title-link" href="https://papers.cool/arxiv/2404.03645" target="_blank">Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation</a>
                <a id="pdf-2404.03645" class="title-pdf" onclick="togglePdf(&#39;2404.03645&#39;, &#39;https://arxiv.org/pdf/2404.03645.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03645">1</sup>]</a>
                <a id="copy-2404.03645" class="title-copy" onclick="copyToClipboard(&#39;2404.03645&#39;)">[Copy]</a>
                <a id="kimi-2404.03645" class="title-kimi" onclick="chatKimi(&#39;2404.03645&#39;, this)">[Kimi<sup id="kimi-stars-2404.03645">1</sup>]</a>
            </h2>
            <p id="authors-2404.03645" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shuting%20He" target="_blank"><span class="author">Shuting He<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Henghui%20Ding" target="_blank"><span class="author">Henghui Ding<span></span></span></a>
            </p>
            <p id="summary-2404.03645" class="summary">Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues. Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues. However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception. In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues. In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension. Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues. Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales. Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects. These contributions yield state-of-the-art performance across five datasets, including a remarkable $\textbf{9.2%}$ $\mathcal{J\&amp;F}$ improvement on the challenging $\textbf{MeViS}$ dataset. Code is available at https://github.com/heshuting555/DsHmp.</p>
            <div id="pdf-container-2404.03645" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03645" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(7).html"></iframe>
            </div>
            <div id="kimi-container-2404.03645" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03642" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03642" target="_blank" title="9/93"><span class="index">#9</span></a>
                <a id="title-2404.03642" class="title-link" href="https://papers.cool/arxiv/2404.03642" target="_blank">DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior</a>
                <a id="pdf-2404.03642" class="title-pdf" onclick="togglePdf(&#39;2404.03642&#39;, &#39;https://arxiv.org/pdf/2404.03642.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03642">4</sup>]</a>
                <a id="copy-2404.03642" class="title-copy" onclick="copyToClipboard(&#39;2404.03642&#39;)">[Copy]</a>
                <a id="kimi-2404.03642" class="title-kimi" onclick="chatKimi(&#39;2404.03642&#39;, this)">[Kimi<sup id="kimi-stars-2404.03642">4</sup>]</a>
            </h2>
            <p id="authors-2404.03642" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yiming%20Zhang" target="_blank"><span class="author">Yiming Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhe%20Wang" target="_blank"><span class="author">Zhe Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xinjie%20Li" target="_blank"><span class="author">Xinjie Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yunchen%20Yuan" target="_blank"><span class="author">Yunchen Yuan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chengsong%20Zhang" target="_blank"><span class="author">Chengsong Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiao%20Sun" target="_blank"><span class="author">Xiao Sun<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhihang%20Zhong" target="_blank"><span class="author">Zhihang Zhong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jian%20Wang" target="_blank"><span class="author">Jian Wang<span></span></span></a>
            </p>
            <p id="summary-2404.03642" class="summary">Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion model's focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods.</p>
            <div id="pdf-container-2404.03642" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03642" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(8).html"></iframe>
            </div>
            <div id="kimi-container-2404.03642" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03635" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03635" target="_blank" title="10/93"><span class="index">#10</span></a>
                <a id="title-2404.03635" class="title-link" href="https://papers.cool/arxiv/2404.03635" target="_blank">WorDepth: Variational Language Prior for Monocular Depth Estimation</a>
                <a id="pdf-2404.03635" class="title-pdf" onclick="togglePdf(&#39;2404.03635&#39;, &#39;https://arxiv.org/pdf/2404.03635.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03635">2</sup>]</a>
                <a id="copy-2404.03635" class="title-copy" onclick="copyToClipboard(&#39;2404.03635&#39;)">[Copy]</a>
                <a id="kimi-2404.03635" class="title-kimi" onclick="chatKimi(&#39;2404.03635&#39;, this)">[Kimi<sup id="kimi-stars-2404.03635">7</sup>]</a>
            </h2>
            <p id="authors-2404.03635" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziyao%20Zeng" target="_blank"><span class="author">Ziyao Zeng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daniel%20Wang" target="_blank"><span class="author">Daniel Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fengyu%20Yang" target="_blank"><span class="author">Fengyu Yang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hyoungseob%20Park" target="_blank"><span class="author">Hyoungseob Park<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yangchao%20Wu" target="_blank"><span class="author">Yangchao Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Stefano%20Soatto" target="_blank"><span class="author">Stefano Soatto<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Byung-Woo%20Hong" target="_blank"><span class="author">Byung-Woo Hong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dong%20Lao" target="_blank"><span class="author">Dong Lao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alex%20Wong" target="_blank"><span class="author">Alex Wong<span></span></span></a>
            </p>
            <p id="summary-2404.03635" class="summary">Three-dimensional (3D) reconstruction from a single image is an ill-posed problem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text description(s) is similarly ill-posed, i.e. spatial arrangements of objects described. We investigate the question of whether two inherently ambiguous modalities can be used in conjunction to produce metric-scaled reconstructions. To test this, we focus on monocular depth estimation, the problem of predicting a dense depth map from a single image, but with an additional text caption describing the scene. To this end, we begin by encoding the text caption as a mean and standard deviation; using a variational framework, we learn the distribution of the plausible metric reconstructions of 3D scenes corresponding to the text captions as a prior. To "select" a specific reconstruction or depth map, we encode the given image through a conditional sampler that samples from the latent space of the variational text encoder, which is then decoded to the output depth map. Our approach is trained alternatingly between the text and image branches: in one optimization step, we predict the mean and standard deviation from the text description and sample from a standard Gaussian, and in the other, we sample using a (image) conditional sampler. Once trained, we directly predict depth from the encoded text using the conditional sampler. We demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where we show that language can consistently improve performance in both.</p>
            <div id="pdf-container-2404.03635" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03635" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(9).html"></iframe>
            </div>
            <div id="kimi-container-2404.03635" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03632" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03632" target="_blank" title="11/93"><span class="index">#11</span></a>
                <a id="title-2404.03632" class="title-link" href="https://papers.cool/arxiv/2404.03632" target="_blank">Reference-Based 3D-Aware Image Editing with Triplane</a>
                <a id="pdf-2404.03632" class="title-pdf" onclick="togglePdf(&#39;2404.03632&#39;, &#39;https://arxiv.org/pdf/2404.03632.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03632">2</sup>]</a>
                <a id="copy-2404.03632" class="title-copy" onclick="copyToClipboard(&#39;2404.03632&#39;)">[Copy]</a>
                <a id="kimi-2404.03632" class="title-kimi" onclick="chatKimi(&#39;2404.03632&#39;, this)">[Kimi<sup id="kimi-stars-2404.03632"></sup>]</a>
            </h2>
            <p id="authors-2404.03632" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bahri%20Batuhan%20Bilecen" target="_blank"><span class="author">Bahri Batuhan Bilecen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yigit%20Yalin" target="_blank"><span class="author">Yigit Yalin<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ning%20Yu" target="_blank"><span class="author">Ning Yu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aysegul%20Dundar" target="_blank"><span class="author">Aysegul Dundar<span></span></span></a>
            </p>
            <p id="summary-2404.03632" class="summary">Generative Adversarial Networks (GANs) have emerged as powerful tools not only for high-quality image generation but also for real image editing through manipulation of their interpretable latent spaces. Recent advancements in GANs include the development of 3D-aware models such as EG3D, characterized by efficient triplane-based architectures enabling the reconstruction of 3D geometry from single images. However, scant attention has been devoted to providing an integrated framework for high-quality reference-based 3D-aware image editing within this domain. This study addresses this gap by exploring and demonstrating the effectiveness of EG3D's triplane space for achieving advanced reference-based edits, presenting a unique perspective on 3D-aware image editing through our novel pipeline. Our approach integrates the encoding of triplane features, spatial disentanglement and automatic localization of features in the triplane domain, and fusion learning for desired image editing. Moreover, our framework demonstrates versatility across domains, extending its effectiveness to animal face edits and partial stylization of cartoon portraits. The method shows significant improvements over relevant 3D-aware latent editing and 2D reference-based editing methods, both qualitatively and quantitatively. Project page: https://three-bee.github.io/triplane_edit</p>
            <div id="pdf-container-2404.03632" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03632" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(10).html"></iframe>
            </div>
            <div id="kimi-container-2404.03632" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03631" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03631" target="_blank" title="12/93"><span class="index">#12</span></a>
                <a id="title-2404.03631" class="title-link" href="https://papers.cool/arxiv/2404.03631" target="_blank">Robust Concept Erasure Using Task Vectors</a>
                <a id="pdf-2404.03631" class="title-pdf" onclick="togglePdf(&#39;2404.03631&#39;, &#39;https://arxiv.org/pdf/2404.03631.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03631">2</sup>]</a>
                <a id="copy-2404.03631" class="title-copy" onclick="copyToClipboard(&#39;2404.03631&#39;)">[Copy]</a>
                <a id="kimi-2404.03631" class="title-kimi" onclick="chatKimi(&#39;2404.03631&#39;, this)">[Kimi<sup id="kimi-stars-2404.03631">5</sup>]</a>
            </h2>
            <p id="authors-2404.03631" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Minh%20Pham" target="_blank"><span class="author">Minh Pham<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kelly%20O.%20Marshall" target="_blank"><span class="author">Kelly O. Marshall<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chinmay%20Hegde" target="_blank"><span class="author">Chinmay Hegde<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Niv%20Cohen" target="_blank"><span class="author">Niv Cohen<span></span></span></a>
            </p>
            <p id="summary-2404.03631" class="summary">With the rapid growth of text-to-image models, a variety of techniques have been suggested to prevent undesirable image generations. Yet, these methods often only protect against specific user prompts and have been shown to allow unsafe generations with other inputs. Here we focus on unconditionally erasing a concept from a text-to-image model rather than conditioning the erasure on the user's prompt. We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training. However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown. To this end, we propose a method called Diverse Inversion, which we use to estimate the required strength of the TV edit. Diverse Inversion finds within the model input space a large set of word embeddings, each of which induces the generation of the target concept. We find that encouraging diversity in the set makes our estimation more robust to unexpected prompts. Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining the core functionality of the model.</p>
            <div id="pdf-container-2404.03631" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03631" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(11).html"></iframe>
            </div>
            <div id="kimi-container-2404.03631" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03620" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03620" target="_blank" title="13/93"><span class="index">#13</span></a>
                <a id="title-2404.03620" class="title-link" href="https://papers.cool/arxiv/2404.03620" target="_blank">LCM-Lookahead for Encoder-based Text-to-Image Personalization</a>
                <a id="pdf-2404.03620" class="title-pdf" onclick="togglePdf(&#39;2404.03620&#39;, &#39;https://arxiv.org/pdf/2404.03620.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03620">4</sup>]</a>
                <a id="copy-2404.03620" class="title-copy" onclick="copyToClipboard(&#39;2404.03620&#39;)">[Copy]</a>
                <a id="kimi-2404.03620" class="title-kimi" onclick="chatKimi(&#39;2404.03620&#39;, this)">[Kimi<sup id="kimi-stars-2404.03620">3</sup>]</a>
            </h2>
            <p id="authors-2404.03620" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rinon%20Gal" target="_blank"><span class="author">Rinon Gal<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Or%20Lichter" target="_blank"><span class="author">Or Lichter<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Elad%20Richardson" target="_blank"><span class="author">Elad Richardson<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Or%20Patashnik" target="_blank"><span class="author">Or Patashnik<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Amit%20H.%20Bermano" target="_blank"><span class="author">Amit H. Bermano<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gal%20Chechik" target="_blank"><span class="author">Gal Chechik<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daniel%20Cohen-Or" target="_blank"><span class="author">Daniel Cohen-Or<span></span></span></a>
            </p>
            <p id="summary-2404.03620" class="summary">Recent advancements in diffusion models have introduced fast sampling methods that can effectively produce high-quality images in just one or a few denoising steps. Interestingly, when these are distilled from existing diffusion models, they often maintain alignment with the original model, retaining similar outputs for similar prompts and seeds. These properties present opportunities to leverage fast sampling methods as a shortcut-mechanism, using them to create a preview of denoised outputs through which we can backpropagate image-space losses. In this work, we explore the potential of using such shortcut-mechanisms to guide the personalization of text-to-image models to specific facial identities. We focus on encoder-based personalization approaches, and demonstrate that by tuning them with a lookahead identity loss, we can achieve higher identity fidelity, without sacrificing layout diversity or prompt alignment. We further explore the use of attention sharing mechanisms and consistent data generation for the task of personalization, and find that encoder training can benefit from both.</p>
            <div id="pdf-container-2404.03620" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03620" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(12).html"></iframe>
            </div>
            <div id="kimi-container-2404.03620" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03618" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03618" target="_blank" title="14/93"><span class="index">#14</span></a>
                <a id="title-2404.03618" class="title-link" href="https://papers.cool/arxiv/2404.03618" target="_blank">DeViDe: Faceted medical knowledge for improved medical vision-language pre-training</a>
                <a id="pdf-2404.03618" class="title-pdf" onclick="togglePdf(&#39;2404.03618&#39;, &#39;https://arxiv.org/pdf/2404.03618.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03618">3</sup>]</a>
                <a id="copy-2404.03618" class="title-copy" onclick="copyToClipboard(&#39;2404.03618&#39;)">[Copy]</a>
                <a id="kimi-2404.03618" class="title-kimi" onclick="chatKimi(&#39;2404.03618&#39;, this)">[Kimi<sup id="kimi-stars-2404.03618">2</sup>]</a>
            </h2>
            <p id="authors-2404.03618" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haozhe%20Luo" target="_blank"><span class="author">Haozhe Luo<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ziyu%20Zhou" target="_blank"><span class="author">Ziyu Zhou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Corentin%20Royer" target="_blank"><span class="author">Corentin Royer<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anjany%20Sekuboyina" target="_blank"><span class="author">Anjany Sekuboyina<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bjoern%20Menze" target="_blank"><span class="author">Bjoern Menze<span></span></span></a>
            </p>
            <p id="summary-2404.03618" class="summary">Vision-language pre-training for chest X-rays has made sig- nificant strides, primarily by utilizing paired radiographs and radiology reports. However, existing approaches often face challenges in encoding medical knowledge effectively. While radiology reports provide insights into the current disease manifestation, medical definitions (as used by contemporary methods) tend to be overly abstract, creating a gap in knowledge. To address this, we propose DeViDe, a novel transformer- based method that leverages radiographic descriptions from the open web. These descriptions outline general visual characteristics of diseases in radiographs, and when combined with abstract definitions and radiol- ogy reports, provide a holistic snapshot of knowledge. DeViDe incorpo- rates three key features for knowledge-augmented vision language align- ment: First, a large-language model-based augmentation is employed to homogenise medical knowledge from diverse sources. Second, this knowl- edge is aligned with image information at various levels of granularity. Third, a novel projection layer is proposed to handle the complexity of aligning each image with multiple descriptions arising in a multi-label setting. In zero-shot settings, DeViDe performs comparably to fully su- pervised models on external datasets and achieves state-of-the-art results on three large-scale datasets. Additionally, fine-tuning DeViDe on four downstream tasks and six segmentation tasks showcases its superior per- formance across data from diverse distributions.</p>
            <div id="pdf-container-2404.03618" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03618" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(13).html"></iframe>
            </div>
            <div id="kimi-container-2404.03618" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03613" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03613" target="_blank" title="15/93"><span class="index">#15</span></a>
                <a id="title-2404.03613" class="title-link" href="https://papers.cool/arxiv/2404.03613" target="_blank">Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting</a>
                <a id="pdf-2404.03613" class="title-pdf" onclick="togglePdf(&#39;2404.03613&#39;, &#39;https://arxiv.org/pdf/2404.03613.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03613">1</sup>]</a>
                <a id="copy-2404.03613" class="title-copy" onclick="copyToClipboard(&#39;2404.03613&#39;)">[Copy]</a>
                <a id="kimi-2404.03613" class="title-kimi" onclick="chatKimi(&#39;2404.03613&#39;, this)">[Kimi<sup id="kimi-stars-2404.03613">1</sup>]</a>
            </h2>
            <p id="authors-2404.03613" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jeongmin%20Bae" target="_blank"><span class="author">Jeongmin Bae<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Seoha%20Kim" target="_blank"><span class="author">Seoha Kim<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Youngsik%20Yun" target="_blank"><span class="author">Youngsik Yun<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hahyun%20Lee" target="_blank"><span class="author">Hahyun Lee<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gun%20Bang" target="_blank"><span class="author">Gun Bang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Youngjung%20Uh" target="_blank"><span class="author">Youngjung Uh<span></span></span></a>
            </p>
            <p id="summary-2404.03613" class="summary">As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames. However, previous works fail to accurately reconstruct dynamic scenes, especially 1) static parts moving along nearby dynamic parts, and 2) some dynamic areas are blurry. We attribute the failure to the wrong design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce an efficient training strategy for faster convergence and higher quality. Project page: https://jeongminb.github.io/e-d3dgs/</p>
            <div id="pdf-container-2404.03613" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03613" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(14).html"></iframe>
            </div>
            <div id="kimi-container-2404.03613" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03611" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03611" target="_blank" title="16/93"><span class="index">#16</span></a>
                <a id="title-2404.03611" class="title-link" href="https://papers.cool/arxiv/2404.03611" target="_blank">InsectMamba: Insect Pest Classification with State Space Model</a>
                <a id="pdf-2404.03611" class="title-pdf" onclick="togglePdf(&#39;2404.03611&#39;, &#39;https://arxiv.org/pdf/2404.03611.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03611">6</sup>]</a>
                <a id="copy-2404.03611" class="title-copy" onclick="copyToClipboard(&#39;2404.03611&#39;)">[Copy]</a>
                <a id="kimi-2404.03611" class="title-kimi" onclick="chatKimi(&#39;2404.03611&#39;, this)">[Kimi<sup id="kimi-stars-2404.03611">3</sup>]</a>
            </h2>
            <p id="authors-2404.03611" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qianning%20Wang" target="_blank"><span class="author">Qianning Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chenglin%20Wang" target="_blank"><span class="author">Chenglin Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhixin%20Lai" target="_blank"><span class="author">Zhixin Lai<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yucheng%20Zhou" target="_blank"><span class="author">Yucheng Zhou<span></span></span></a>
            </p>
            <p id="summary-2404.03611" class="summary">The classification of insect pests is a critical task in agricultural technology, vital for ensuring food security and environmental sustainability. However, the complexity of pest identification, due to factors like high camouflage and species diversity, poses significant obstacles. Existing methods struggle with the fine-grained feature extraction needed to distinguish between closely related pest species. Although recent advancements have utilized modified network structures and combined deep learning approaches to improve accuracy, challenges persist due to the similarity between pests and their surroundings. To address this problem, we introduce InsectMamba, a novel approach that integrates State Space Models (SSMs), Convolutional Neural Networks (CNNs), Multi-Head Self-Attention mechanism (MSA), and Multilayer Perceptrons (MLPs) within Mix-SSM blocks. This integration facilitates the extraction of comprehensive visual features by leveraging the strengths of each encoding strategy. A selective module is also proposed to adaptively aggregate these features, enhancing the model's ability to discern pest characteristics. InsectMamba was evaluated against strong competitors across five insect pest classification datasets. The results demonstrate its superior performance and verify the significance of each model component by an ablation study.</p>
            <div id="pdf-container-2404.03611" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03611" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(15).html"></iframe>
            </div>
            <div id="kimi-container-2404.03611" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03590" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03590" target="_blank" title="17/93"><span class="index">#17</span></a>
                <a id="title-2404.03590" class="title-link" href="https://papers.cool/arxiv/2404.03590" target="_blank">SemGrasp: Semantic Grasp Generation via Language Aligned Discretization</a>
                <a id="pdf-2404.03590" class="title-pdf" onclick="togglePdf(&#39;2404.03590&#39;, &#39;https://arxiv.org/pdf/2404.03590.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03590">1</sup>]</a>
                <a id="copy-2404.03590" class="title-copy" onclick="copyToClipboard(&#39;2404.03590&#39;)">[Copy]</a>
                <a id="kimi-2404.03590" class="title-kimi" onclick="chatKimi(&#39;2404.03590&#39;, this)">[Kimi<sup id="kimi-stars-2404.03590">6</sup>]</a>
            </h2>
            <p id="authors-2404.03590" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kailin%20Li" target="_blank"><span class="author">Kailin Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingbo%20Wang" target="_blank"><span class="author">Jingbo Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lixin%20Yang" target="_blank"><span class="author">Lixin Yang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Cewu%20Lu" target="_blank"><span class="author">Cewu Lu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bo%20Dai" target="_blank"><span class="author">Bo Dai<span></span></span></a>
            </p>
            <p id="summary-2404.03590" class="summary">Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed SemGrasp, which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of SemGrasp, we have compiled a large-scale, grasp-text-aligned dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse grasps. Experimental findings demonstrate that SemGrasp efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset are available publicly at: https://kailinli.github.io/SemGrasp.</p>
            <div id="pdf-container-2404.03590" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03590" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(16).html"></iframe>
            </div>
            <div id="kimi-container-2404.03590" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03584" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03584" target="_blank" title="18/93"><span class="index">#18</span></a>
                <a id="title-2404.03584" class="title-link" href="https://papers.cool/arxiv/2404.03584" target="_blank">Towards more realistic human motion prediction with attention to motion coordination</a>
                <a id="pdf-2404.03584" class="title-pdf" onclick="togglePdf(&#39;2404.03584&#39;, &#39;https://arxiv.org/pdf/2404.03584.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03584">3</sup>]</a>
                <a id="copy-2404.03584" class="title-copy" onclick="copyToClipboard(&#39;2404.03584&#39;)">[Copy]</a>
                <a id="kimi-2404.03584" class="title-kimi" onclick="chatKimi(&#39;2404.03584&#39;, this)">[Kimi<sup id="kimi-stars-2404.03584">4</sup>]</a>
            </h2>
            <p id="authors-2404.03584" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pengxiang%20Ding" target="_blank"><span class="author">Pengxiang Ding<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jianqin%20Yin" target="_blank"><span class="author">Jianqin Yin<span></span></span></a>
            </p>
            <p id="summary-2404.03584" class="summary">Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To tackle this issue, we learn a medium, called coordination attractor (CA), from the spatiotemporal features of motion to characterize the global motion features, which is subsequently used to build new relative joint relations. Through the CA, all joints are related simultaneously, and thus the motion coordination of all joints can be better learned. Based on this, we further propose a novel joint relation modeling module, Comprehensive Joint Relation Extractor (CJRE), to combine this motion coordination with the local interactions between joint pairs in a unified manner. Additionally, we also present a Multi-timescale Dynamics Extractor (MTDE) to extract enriched dynamics from the raw position information for effective prediction. Extensive experiments show that the proposed framework outperforms state-of-the-art methods in both short- and long-term predictions on H3.6M, CMU-Mocap, and 3DPW.</p>
            <div id="pdf-container-2404.03584" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03584" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(17).html"></iframe>
            </div>
            <div id="kimi-container-2404.03584" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03575" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03575" target="_blank" title="19/93"><span class="index">#19</span></a>
                <a id="title-2404.03575" class="title-link" href="https://papers.cool/arxiv/2404.03575" target="_blank">DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling</a>
                <a id="pdf-2404.03575" class="title-pdf" onclick="togglePdf(&#39;2404.03575&#39;, &#39;https://arxiv.org/pdf/2404.03575.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03575">3</sup>]</a>
                <a id="copy-2404.03575" class="title-copy" onclick="copyToClipboard(&#39;2404.03575&#39;)">[Copy]</a>
                <a id="kimi-2404.03575" class="title-kimi" onclick="chatKimi(&#39;2404.03575&#39;, this)">[Kimi<sup id="kimi-stars-2404.03575">1</sup>]</a>
            </h2>
            <p id="authors-2404.03575" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haoran%20Li" target="_blank"><span class="author">Haoran Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haolin%20Shi" target="_blank"><span class="author">Haolin Shi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenli%20Zhang" target="_blank"><span class="author">Wenli Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenjun%20Wu" target="_blank"><span class="author">Wenjun Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yong%20Liao" target="_blank"><span class="author">Yong Liao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lin%20Wang" target="_blank"><span class="author">Lin Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lik-hang%20Lee" target="_blank"><span class="author">Lik-hang Lee<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pengyuan%20Zhou" target="_blank"><span class="author">Pengyuan Zhou<span></span></span></a>
            </p>
            <p id="summary-2404.03575" class="summary">Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io .</p>
            <div id="pdf-container-2404.03575" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03575" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(18).html"></iframe>
            </div>
            <div id="kimi-container-2404.03575" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03574" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03574" target="_blank" title="20/93"><span class="index">#20</span></a>
                <a id="title-2404.03574" class="title-link" href="https://papers.cool/arxiv/2404.03574" target="_blank">TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices</a>
                <a id="pdf-2404.03574" class="title-pdf" onclick="togglePdf(&#39;2404.03574&#39;, &#39;https://arxiv.org/pdf/2404.03574.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03574">2</sup>]</a>
                <a id="copy-2404.03574" class="title-copy" onclick="copyToClipboard(&#39;2404.03574&#39;)">[Copy]</a>
                <a id="kimi-2404.03574" class="title-kimi" onclick="chatKimi(&#39;2404.03574&#39;, this)">[Kimi<sup id="kimi-stars-2404.03574">3</sup>]</a>
            </h2>
            <p id="authors-2404.03574" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hasib-Al%20Rashid" target="_blank"><span class="author">Hasib-Al Rashid<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Argho%20Sarkar" target="_blank"><span class="author">Argho Sarkar<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aryya%20Gangopadhyay" target="_blank"><span class="author">Aryya Gangopadhyay<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Maryam%20Rahnemoonfar" target="_blank"><span class="author">Maryam Rahnemoonfar<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tinoosh%20Mohsenin" target="_blank"><span class="author">Tinoosh Mohsenin<span></span></span></a>
            </p>
            <p id="summary-2404.03574" class="summary">Traditional machine learning models often require powerful hardware, making them unsuitable for deployment on resource-limited devices. Tiny Machine Learning (tinyML) has emerged as a promising approach for running machine learning models on these devices, but integrating multiple data modalities into tinyML models still remains a challenge due to increased complexity, latency, and power consumption. This paper proposes TinyVQA, a novel multimodal deep neural network for visual question answering tasks that can be deployed on resource-constrained tinyML hardware. TinyVQA leverages a supervised attention-based model to learn how to answer questions about images using both vision and language modalities. Distilled knowledge from the supervised attention-based VQA model trains the memory aware compact TinyVQA model and low bit-width quantization technique is employed to further compress the model for deployment on tinyML devices. The TinyVQA model was evaluated on the FloodNet dataset, which is used for post-disaster damage assessment. The compact model achieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA for real-world applications. Additionally, the model was deployed on a Crazyflie 2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA model achieved low latencies of 56 ms and consumes 693 mW power while deployed on the tiny drone, showcasing its suitability for resource-constrained embedded systems.</p>
            <div id="pdf-container-2404.03574" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03574" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(19).html"></iframe>
            </div>
            <div id="kimi-container-2404.03574" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03572" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03572" target="_blank" title="21/93"><span class="index">#21</span></a>
                <a id="title-2404.03572" class="title-link" href="https://papers.cool/arxiv/2404.03572" target="_blank">Terrain Point Cloud Inpainting via Signal Decomposition</a>
                <a id="pdf-2404.03572" class="title-pdf" onclick="togglePdf(&#39;2404.03572&#39;, &#39;https://arxiv.org/pdf/2404.03572.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03572">1</sup>]</a>
                <a id="copy-2404.03572" class="title-copy" onclick="copyToClipboard(&#39;2404.03572&#39;)">[Copy]</a>
                <a id="kimi-2404.03572" class="title-kimi" onclick="chatKimi(&#39;2404.03572&#39;, this)">[Kimi<sup id="kimi-stars-2404.03572"></sup>]</a>
            </h2>
            <p id="authors-2404.03572" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yizhou%20Xie" target="_blank"><span class="author">Yizhou Xie<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiangning%20Xie" target="_blank"><span class="author">Xiangning Xie<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuran%20Wang" target="_blank"><span class="author">Yuran Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yanci%20Zhang" target="_blank"><span class="author">Yanci Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zejun%20Lv" target="_blank"><span class="author">Zejun Lv<span></span></span></a>
            </p>
            <p id="summary-2404.03572" class="summary">The rapid development of 3D acquisition technology has made it possible to obtain point clouds of real-world terrains. However, due to limitations in sensor acquisition technology or specific requirements, point clouds often contain defects such as holes with missing data. Inpainting algorithms are widely used to patch these holes. However, existing traditional inpainting algorithms rely on precise hole boundaries, which limits their ability to handle cases where the boundaries are not well-defined. On the other hand, learning-based completion methods often prioritize reconstructing the entire point cloud instead of solely focusing on hole filling. Based on the fact that real-world terrain exhibits both global smoothness and rich local detail, we propose a novel representation for terrain point clouds. This representation can help to repair the holes without clear boundaries. Specifically, it decomposes terrains into low-frequency and high-frequency components, which are represented by B-spline surfaces and relative height maps respectively. In this way, the terrain point cloud inpainting problem is transformed into a B-spline surface fitting and 2D image inpainting problem. By solving the two problems, the highly complex and irregular holes on the terrain point clouds can be well-filled, which not only satisfies the global terrain undulation but also exhibits rich geometric details. The experimental results also demonstrate the effectiveness of our method.</p>
            <div id="pdf-container-2404.03572" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03572" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(20).html"></iframe>
            </div>
            <div id="kimi-container-2404.03572" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03566" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03566" target="_blank" title="22/93"><span class="index">#22</span></a>
                <a id="title-2404.03566" class="title-link" href="https://papers.cool/arxiv/2404.03566" target="_blank">PointInfinity: Resolution-Invariant Point Diffusion Models</a>
                <a id="pdf-2404.03566" class="title-pdf" onclick="togglePdf(&#39;2404.03566&#39;, &#39;https://arxiv.org/pdf/2404.03566.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03566">4</sup>]</a>
                <a id="copy-2404.03566" class="title-copy" onclick="copyToClipboard(&#39;2404.03566&#39;)">[Copy]</a>
                <a id="kimi-2404.03566" class="title-kimi" onclick="chatKimi(&#39;2404.03566&#39;, this)">[Kimi<sup id="kimi-stars-2404.03566"></sup>]</a>
            </h2>
            <p id="authors-2404.03566" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zixuan%20Huang" target="_blank"><span class="author">Zixuan Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Justin%20Johnson" target="_blank"><span class="author">Justin Johnson<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shoubhik%20Debnath" target="_blank"><span class="author">Shoubhik Debnath<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=James%20M.%20Rehg" target="_blank"><span class="author">James M. Rehg<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chao-Yuan%20Wu" target="_blank"><span class="author">Chao-Yuan Wu<span></span></span></a>
            </p>
            <p id="summary-2404.03566" class="summary">We present PointInfinity, an efficient family of point cloud diffusion models. Our core idea is to use a transformer-based architecture with a fixed-size, resolution-invariant latent representation. This enables efficient training with low-resolution point clouds, while allowing high-resolution point clouds to be generated during inference. More importantly, we show that scaling the test-time resolution beyond the training resolution improves the fidelity of generated point clouds and surfaces. We analyze this phenomenon and draw a link to classifier-free guidance commonly used in diffusion models, demonstrating that both allow trading off fidelity and variability during inference. Experiments on CO3D show that PointInfinity can efficiently generate high-resolution point clouds (up to 131k points, 31 times more than Point-E) with state-of-the-art quality.</p>
            <div id="pdf-container-2404.03566" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03566" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(21).html"></iframe>
            </div>
            <div id="kimi-container-2404.03566" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03539" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03539" target="_blank" title="23/93"><span class="index">#23</span></a>
                <a id="title-2404.03539" class="title-link" href="https://papers.cool/arxiv/2404.03539" target="_blank">Is CLIP the main roadblock for fine-grained open-world perception?</a>
                <a id="pdf-2404.03539" class="title-pdf" onclick="togglePdf(&#39;2404.03539&#39;, &#39;https://arxiv.org/pdf/2404.03539.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03539">4</sup>]</a>
                <a id="copy-2404.03539" class="title-copy" onclick="copyToClipboard(&#39;2404.03539&#39;)">[Copy]</a>
                <a id="kimi-2404.03539" class="title-kimi" onclick="chatKimi(&#39;2404.03539&#39;, this)">[Kimi<sup id="kimi-stars-2404.03539">2</sup>]</a>
            </h2>
            <p id="authors-2404.03539" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lorenzo%20Bianchi" target="_blank"><span class="author">Lorenzo Bianchi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fabio%20Carrara" target="_blank"><span class="author">Fabio Carrara<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nicola%20Messina" target="_blank"><span class="author">Nicola Messina<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fabrizio%20Falchi" target="_blank"><span class="author">Fabrizio Falchi<span></span></span></a>
            </p>
            <p id="summary-2404.03539" class="summary">Modern applications increasingly demand flexible computer vision models that adapt to novel concepts not encountered during training. This necessity is pivotal in emerging domains like extended reality, robotics, and autonomous driving, which require the ability to respond to open-world stimuli. A key ingredient is the ability to identify objects based on free-form textual queries defined at inference time - a task known as open-vocabulary object detection. Multimodal backbones like CLIP are the main enabling technology for current open-world perception solutions. Despite performing well on generic queries, recent studies highlighted limitations on the fine-grained recognition capabilities in open-vocabulary settings - i.e., for distinguishing subtle object features like color, shape, and material. In this paper, we perform a detailed examination of these open-vocabulary object recognition limitations to find the root cause. We evaluate the performance of CLIP, the most commonly used vision-language backbone, against a fine-grained object-matching benchmark, revealing interesting analogies between the limitations of open-vocabulary object detectors and their backbones. Experiments suggest that the lack of fine-grained understanding is caused by the poor separability of object characteristics in the CLIP latent space. Therefore, we try to understand whether fine-grained knowledge is present in CLIP embeddings but not exploited at inference time due, for example, to the unsuitability of the cosine similarity matching function, which may discard important object characteristics. Our preliminary experiments show that simple CLIP latent-space re-projections help separate fine-grained concepts, paving the way towards the development of backbones inherently able to process fine-grained details. The code for reproducing these experiments is available at https://github.com/lorebianchi98/FG-CLIP.</p>
            <div id="pdf-container-2404.03539" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03539" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(22).html"></iframe>
            </div>
            <div id="kimi-container-2404.03539" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03537" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03537" target="_blank" title="24/93"><span class="index">#24</span></a>
                <a id="title-2404.03537" class="title-link" href="https://papers.cool/arxiv/2404.03537" target="_blank">If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces</a>
                <a id="pdf-2404.03537" class="title-pdf" onclick="togglePdf(&#39;2404.03537&#39;, &#39;https://arxiv.org/pdf/2404.03537.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03537">1</sup>]</a>
                <a id="copy-2404.03537" class="title-copy" onclick="copyToClipboard(&#39;2404.03537&#39;)">[Copy]</a>
                <a id="kimi-2404.03537" class="title-kimi" onclick="chatKimi(&#39;2404.03537&#39;, this)">[Kimi<sup id="kimi-stars-2404.03537"></sup>]</a>
            </h2>
            <p id="authors-2404.03537" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andrea%20Atzori" target="_blank"><span class="author">Andrea Atzori<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fadi%20Boutros" target="_blank"><span class="author">Fadi Boutros<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Naser%20Damer" target="_blank"><span class="author">Naser Damer<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gianni%20Fenu" target="_blank"><span class="author">Gianni Fenu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mirko%20Marras" target="_blank"><span class="author">Mirko Marras<span></span></span></a>
            </p>
            <p id="summary-2404.03537" class="summary">Recent advances in deep face recognition have spurred a growing demand for large, diverse, and manually annotated face datasets. Acquiring authentic, high-quality data for face recognition has proven to be a challenge, primarily due to privacy concerns. Large face datasets are primarily sourced from web-based images, lacking explicit user consent. In this paper, we examine whether and how synthetic face data can be used to train effective face recognition models with reduced reliance on authentic images, thereby mitigating data collection concerns. First, we explored the performance gap among recent state-of-the-art face recognition models, trained with synthetic data only and authentic (scarce) data only. Then, we deepened our analysis by training a state-of-the-art backbone with various combinations of synthetic and authentic data, gaining insights into optimizing the limited use of the latter for verification accuracy. Finally, we assessed the effectiveness of data augmentation approaches on synthetic and authentic data, with the same goal in mind. Our results highlighted the effectiveness of FR trained on combined datasets, particularly when combined with appropriate augmentation techniques.</p>
            <div id="pdf-container-2404.03537" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03537" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(23).html"></iframe>
            </div>
            <div id="kimi-container-2404.03537" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03531" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03531" target="_blank" title="25/93"><span class="index">#25</span></a>
                <a id="title-2404.03531" class="title-link" href="https://papers.cool/arxiv/2404.03531" target="_blank">COMO: Compact Mapping and Odometry</a>
                <a id="pdf-2404.03531" class="title-pdf" onclick="togglePdf(&#39;2404.03531&#39;, &#39;https://arxiv.org/pdf/2404.03531.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03531">1</sup>]</a>
                <a id="copy-2404.03531" class="title-copy" onclick="copyToClipboard(&#39;2404.03531&#39;)">[Copy]</a>
                <a id="kimi-2404.03531" class="title-kimi" onclick="chatKimi(&#39;2404.03531&#39;, this)">[Kimi<sup id="kimi-stars-2404.03531">2</sup>]</a>
            </h2>
            <p id="authors-2404.03531" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Eric%20Dexheimer" target="_blank"><span class="author">Eric Dexheimer<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andrew%20J.%20Davison" target="_blank"><span class="author">Andrew J. Davison<span></span></span></a>
            </p>
            <p id="summary-2404.03531" class="summary">We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points. Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points. The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference. To maintain a compact yet expressive map, we introduce a frontend that leverages the covariance function for tracking and initializing potentially visually indistinct 3D points across frames. Altogether, we introduce a real-time system capable of estimating accurate poses and consistent geometry.</p>
            <div id="pdf-container-2404.03531" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03531" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(24).html"></iframe>
            </div>
            <div id="kimi-container-2404.03531" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03527" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03527" target="_blank" title="26/93"><span class="index">#26</span></a>
                <a id="title-2404.03527" class="title-link" href="https://papers.cool/arxiv/2404.03527" target="_blank">HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion</a>
                <a id="pdf-2404.03527" class="title-pdf" onclick="togglePdf(&#39;2404.03527&#39;, &#39;https://arxiv.org/pdf/2404.03527.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03527">1</sup>]</a>
                <a id="copy-2404.03527" class="title-copy" onclick="copyToClipboard(&#39;2404.03527&#39;)">[Copy]</a>
                <a id="kimi-2404.03527" class="title-kimi" onclick="chatKimi(&#39;2404.03527&#39;, this)">[Kimi<sup id="kimi-stars-2404.03527"></sup>]</a>
            </h2>
            <p id="authors-2404.03527" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiahang%20Li" target="_blank"><span class="author">Jiahang Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Peng%20Yun" target="_blank"><span class="author">Peng Yun<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qijun%20Chen" target="_blank"><span class="author">Qijun Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rui%20Fan" target="_blank"><span class="author">Rui Fan<span></span></span></a>
            </p>
            <p id="summary-2404.03527" class="summary">Data-fusion networks have shown significant promise for RGB-thermal scene parsing. However, the majority of existing studies have relied on symmetric duplex encoders for heterogeneous feature extraction and fusion, paying inadequate attention to the inherent differences between RGB and thermal modalities. Recent progress in vision foundation models (VFMs) trained through self-supervision on vast amounts of unlabeled data has proven their ability to extract informative, general-purpose features. However, this potential has yet to be fully leveraged in the domain. In this study, we take one step toward this new research area by exploring a feasible strategy to fully exploit VFM features for RGB-thermal scene parsing. Specifically, we delve deeper into the unique characteristics of RGB and thermal modalities, thereby designing a hybrid, asymmetric encoder that incorporates both a VFM and a convolutional neural network. This design allows for more effective extraction of complementary heterogeneous features, which are subsequently fused in a dual-path, progressive manner. Moreover, we introduce an auxiliary task to further enrich the local semantics of the fused features, thereby improving the overall performance of RGB-thermal scene parsing. Our proposed HAPNet, equipped with all these components, demonstrates superior performance compared to all other state-of-the-art RGB-thermal scene parsing networks, achieving top ranks across three widely used public RGB-thermal scene parsing datasets. We believe this new paradigm has opened up new opportunities for future developments in data-fusion scene parsing approaches.</p>
            <div id="pdf-container-2404.03527" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03527" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(25).html"></iframe>
            </div>
            <div id="kimi-container-2404.03527" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03518" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03518" target="_blank" title="27/93"><span class="index">#27</span></a>
                <a id="title-2404.03518" class="title-link" href="https://papers.cool/arxiv/2404.03518" target="_blank">SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation</a>
                <a id="pdf-2404.03518" class="title-pdf" onclick="togglePdf(&#39;2404.03518&#39;, &#39;https://arxiv.org/pdf/2404.03518.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03518">2</sup>]</a>
                <a id="copy-2404.03518" class="title-copy" onclick="copyToClipboard(&#39;2404.03518&#39;)">[Copy]</a>
                <a id="kimi-2404.03518" class="title-kimi" onclick="chatKimi(&#39;2404.03518&#39;, this)">[Kimi<sup id="kimi-stars-2404.03518">2</sup>]</a>
            </h2>
            <p id="authors-2404.03518" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sichen%20Chen" target="_blank"><span class="author">Sichen Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yingyi%20Zhang" target="_blank"><span class="author">Yingyi Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Siming%20Huang" target="_blank"><span class="author">Siming Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ran%20Yi" target="_blank"><span class="author">Ran Yi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ke%20Fan" target="_blank"><span class="author">Ke Fan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruixin%20Zhang" target="_blank"><span class="author">Ruixin Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Peixian%20Chen" target="_blank"><span class="author">Peixian Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun%20Wang" target="_blank"><span class="author">Jun Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shouhong%20Ding" target="_blank"><span class="author">Shouhong Ding<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lizhuang%20Ma" target="_blank"><span class="author">Lizhuang Ma<span></span></span></a>
            </p>
            <p id="summary-2404.03518" class="summary">Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models. To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods. Our code is available at https://github.com/MartyrPenink/SDPose.</p>
            <div id="pdf-container-2404.03518" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03518" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(26).html"></iframe>
            </div>
            <div id="kimi-container-2404.03518" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03507" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03507" target="_blank" title="28/93"><span class="index">#28</span></a>
                <a id="title-2404.03507" class="title-link" href="https://papers.cool/arxiv/2404.03507" target="_blank">DQ-DETR: DETR with Dynamic Query for Tiny Object Detection</a>
                <a id="pdf-2404.03507" class="title-pdf" onclick="togglePdf(&#39;2404.03507&#39;, &#39;https://arxiv.org/pdf/2404.03507.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03507">4</sup>]</a>
                <a id="copy-2404.03507" class="title-copy" onclick="copyToClipboard(&#39;2404.03507&#39;)">[Copy]</a>
                <a id="kimi-2404.03507" class="title-kimi" onclick="chatKimi(&#39;2404.03507&#39;, this)">[Kimi<sup id="kimi-stars-2404.03507">4</sup>]</a>
            </h2>
            <p id="authors-2404.03507" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yi-Xin%20Huang" target="_blank"><span class="author">Yi-Xin Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hou-I%20Liu" target="_blank"><span class="author">Hou-I Liu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hong-Han%20Shuai" target="_blank"><span class="author">Hong-Han Shuai<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wen-Huang%20Cheng" target="_blank"><span class="author">Wen-Huang Cheng<span></span></span></a>
            </p>
            <p id="summary-2404.03507" class="summary">Despite previous DETR-like methods having performed successfully in generic object detection, tiny object detection is still a challenging task for them since the positional information of object queries is not customized for detecting tiny objects, whose scale is extraordinarily smaller than general objects. Also, DETR-like methods using a fixed number of queries make them unsuitable for aerial datasets, which only contain tiny objects, and the numbers of instances are imbalanced between different images. Thus, we present a simple yet effective model, named DQ-DETR, which consists of three different components: categorical counting module, counting-guided feature enhancement, and dynamic query selection to solve the above-mentioned problems. DQ-DETR uses the prediction and density maps from the categorical counting module to dynamically adjust the number of object queries and improve the positional information of queries. Our model DQ-DETR outperforms previous CNN-based and DETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2 dataset, which mostly consists of tiny objects.</p>
            <div id="pdf-container-2404.03507" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03507" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(27).html"></iframe>
            </div>
            <div id="kimi-container-2404.03507" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03482" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03482" target="_blank" title="29/93"><span class="index">#29</span></a>
                <a id="title-2404.03482" class="title-link" href="https://papers.cool/arxiv/2404.03482" target="_blank">AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale</a>
                <a id="pdf-2404.03482" class="title-pdf" onclick="togglePdf(&#39;2404.03482&#39;, &#39;https://arxiv.org/pdf/2404.03482.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03482">1</sup>]</a>
                <a id="copy-2404.03482" class="title-copy" onclick="copyToClipboard(&#39;2404.03482&#39;)">[Copy]</a>
                <a id="kimi-2404.03482" class="title-kimi" onclick="chatKimi(&#39;2404.03482&#39;, this)">[Kimi<sup id="kimi-stars-2404.03482">2</sup>]</a>
            </h2>
            <p id="authors-2404.03482" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Adam%20Pardyl" target="_blank"><span class="author">Adam Pardyl<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Micha%C5%82%20Wronka" target="_blank"><span class="author">Michał Wronka<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Maciej%20Wo%C5%82czyk" target="_blank"><span class="author">Maciej Wołczyk<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kamil%20Adamczewski" target="_blank"><span class="author">Kamil Adamczewski<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tomasz%20Trzci%C5%84ski" target="_blank"><span class="author">Tomasz Trzciński<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bartosz%20Zieli%C5%84ski" target="_blank"><span class="author">Bartosz Zieliński<span></span></span></a>
            </p>
            <p id="summary-2404.03482" class="summary">Active Visual Exploration (AVE) is a task that involves dynamically selecting observations (glimpses), which is critical to facilitate comprehension and navigation within an environment. While modern AVE methods have demonstrated impressive performance, they are constrained to fixed-scale glimpses from rigid grids. In contrast, existing mobile platforms equipped with optical zoom capabilities can capture glimpses of arbitrary positions and scales. To address this gap between software and hardware capabilities, we introduce AdaGlimpse. It uses Soft Actor-Critic, a reinforcement learning algorithm tailored for exploration tasks, to select glimpses of arbitrary position and scale. This approach enables our model to rapidly establish a general awareness of the environment before zooming in for detailed analysis. Experimental results demonstrate that AdaGlimpse surpasses previous methods across various visual tasks while maintaining greater applicability in realistic AVE scenarios.</p>
            <div id="pdf-container-2404.03482" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03482" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(28).html"></iframe>
            </div>
            <div id="kimi-container-2404.03482" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03477" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03477" target="_blank" title="30/93"><span class="index">#30</span></a>
                <a id="title-2404.03477" class="title-link" href="https://papers.cool/arxiv/2404.03477" target="_blank">Towards Automated Movie Trailer Generation</a>
                <a id="pdf-2404.03477" class="title-pdf" onclick="togglePdf(&#39;2404.03477&#39;, &#39;https://arxiv.org/pdf/2404.03477.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03477">2</sup>]</a>
                <a id="copy-2404.03477" class="title-copy" onclick="copyToClipboard(&#39;2404.03477&#39;)">[Copy]</a>
                <a id="kimi-2404.03477" class="title-kimi" onclick="chatKimi(&#39;2404.03477&#39;, this)">[Kimi<sup id="kimi-stars-2404.03477">2</sup>]</a>
            </h2>
            <p id="authors-2404.03477" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dawit%20Mureja%20Argaw" target="_blank"><span class="author">Dawit Mureja Argaw<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mattia%20Soldan" target="_blank"><span class="author">Mattia Soldan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alejandro%20Pardo" target="_blank"><span class="author">Alejandro Pardo<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chen%20Zhao" target="_blank"><span class="author">Chen Zhao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fabian%20Caba%20Heilbron" target="_blank"><span class="author">Fabian Caba Heilbron<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Joon%20Son%20Chung" target="_blank"><span class="author">Joon Son Chung<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bernard%20Ghanem" target="_blank"><span class="author">Bernard Ghanem<span></span></span></a>
            </p>
            <p id="summary-2404.03477" class="summary">Movie trailers are an essential tool for promoting films and attracting audiences. However, the process of creating trailers can be time-consuming and expensive. To streamline this process, we propose an automatic trailer generation framework that generates plausible trailers from a full movie by automating shot selection and composition. Our approach draws inspiration from machine translation techniques and models the movies and trailers as sequences of shots, thus formulating the trailer generation problem as a sequence-to-sequence task. We introduce Trailer Generation Transformer (TGT), a deep-learning framework utilizing an encoder-decoder architecture. TGT movie encoder is tasked with contextualizing each movie shot representation via self-attention, while the autoregressive trailer decoder predicts the feature representation of the next trailer shot, accounting for the relevance of shots' temporal order in trailers. Our TGT significantly outperforms previous methods on a comprehensive suite of metrics.</p>
            <div id="pdf-container-2404.03477" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03477" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(29).html"></iframe>
            </div>
            <div id="kimi-container-2404.03477" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03474" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03474" target="_blank" title="31/93"><span class="index">#31</span></a>
                <a id="title-2404.03474" class="title-link" href="https://papers.cool/arxiv/2404.03474" target="_blank">Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images</a>
                <a id="pdf-2404.03474" class="title-pdf" onclick="togglePdf(&#39;2404.03474&#39;, &#39;https://arxiv.org/pdf/2404.03474.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03474"></sup>]</a>
                <a id="copy-2404.03474" class="title-copy" onclick="copyToClipboard(&#39;2404.03474&#39;)">[Copy]</a>
                <a id="kimi-2404.03474" class="title-kimi" onclick="chatKimi(&#39;2404.03474&#39;, this)">[Kimi<sup id="kimi-stars-2404.03474">1</sup>]</a>
            </h2>
            <p id="authors-2404.03474" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rita%20Pucci" target="_blank"><span class="author">Rita Pucci<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Vincent%20J.%20Kalkman" target="_blank"><span class="author">Vincent J. Kalkman<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dan%20Stowell" target="_blank"><span class="author">Dan Stowell<span></span></span></a>
            </p>
            <p id="summary-2404.03474" class="summary">With fine-grained classification, we identify unique characteristics to distinguish among classes of the same super-class. We are focusing on species recognition in Insecta, as they are critical for biodiversity monitoring and at the base of many ecosystems. With citizen science campaigns, billions of images are collected in the wild. Once these are labelled, experts can use them to create distribution maps. However, the labelling process is time-consuming, which is where computer vision comes in. The field of computer vision offers a wide range of algorithms, each with its strengths and weaknesses; how do we identify the algorithm that is in line with our application? To answer this question, we provide a full and detailed evaluation of nine algorithms among deep convolutional networks (CNN), vision transformers (ViT), and locality-based vision transformers (LBVT) on 4 different aspects: classification performance, embedding quality, computational cost, and gradient activity. We offer insights that we haven't yet had in this domain proving to which extent these algorithms solve the fine-grained tasks in Insecta. We found that the ViT performs the best on inference speed and computational cost while the LBVT outperforms the others on performance and embedding quality; the CNN provide a trade-off among the metrics.</p>
            <div id="pdf-container-2404.03474" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03474" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(30).html"></iframe>
            </div>
            <div id="kimi-container-2404.03474" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03462" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03462" target="_blank" title="32/93"><span class="index">#32</span></a>
                <a id="title-2404.03462" class="title-link" href="https://papers.cool/arxiv/2404.03462" target="_blank">You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects</a>
                <a id="pdf-2404.03462" class="title-pdf" onclick="togglePdf(&#39;2404.03462&#39;, &#39;https://arxiv.org/pdf/2404.03462.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03462">2</sup>]</a>
                <a id="copy-2404.03462" class="title-copy" onclick="copyToClipboard(&#39;2404.03462&#39;)">[Copy]</a>
                <a id="kimi-2404.03462" class="title-kimi" onclick="chatKimi(&#39;2404.03462&#39;, this)">[Kimi<sup id="kimi-stars-2404.03462">1</sup>]</a>
            </h2>
            <p id="authors-2404.03462" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lei%20Zhou" target="_blank"><span class="author">Lei Zhou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haozhe%20Wang" target="_blank"><span class="author">Haozhe Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhengshen%20Zhang" target="_blank"><span class="author">Zhengshen Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhiyang%20Liu" target="_blank"><span class="author">Zhiyang Liu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Francis%20EH%20Tay" target="_blank"><span class="author">Francis EH Tay<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=adn%20Marcelo%20H.%20Ang.%20Jr" target="_blank"><span class="author">adn Marcelo H. Ang. Jr<span></span></span></a>
            </p>
            <p id="summary-2404.03462" class="summary">In the realm of robotic grasping, achieving accurate and reliable interactions with the environment is a pivotal challenge. Traditional methods of grasp planning methods utilizing partial point clouds derived from depth image often suffer from reduced scene understanding due to occlusion, ultimately impeding their grasping accuracy. Furthermore, scene reconstruction methods have primarily relied upon static techniques, which are susceptible to environment change during manipulation process limits their efficacy in real-time grasping tasks. To address these limitations, this paper introduces a novel two-stage pipeline for dynamic scene reconstruction. In the first stage, our approach takes scene scanning as input to register each target object with mesh reconstruction and novel object pose tracking. In the second stage, pose tracking is still performed to provide object poses in real-time, enabling our approach to transform the reconstructed object point clouds back into the scene. Unlike conventional methodologies, which rely on static scene snapshots, our method continuously captures the evolving scene geometry, resulting in a comprehensive and up-to-date point cloud representation. By circumventing the constraints posed by occlusion, our method enhances the overall grasp planning process and empowers state-of-the-art 6-DoF robotic grasping algorithms to exhibit markedly improved accuracy.</p>
            <div id="pdf-container-2404.03462" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03462" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(31).html"></iframe>
            </div>
            <div id="kimi-container-2404.03462" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03451" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03451" target="_blank" title="33/93"><span class="index">#33</span></a>
                <a id="title-2404.03451" class="title-link" href="https://papers.cool/arxiv/2404.03451" target="_blank">How Much Data are Enough? Investigating Dataset Requirements for Patch-Based Brain MRI Segmentation Tasks</a>
                <a id="pdf-2404.03451" class="title-pdf" onclick="togglePdf(&#39;2404.03451&#39;, &#39;https://arxiv.org/pdf/2404.03451.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03451"></sup>]</a>
                <a id="copy-2404.03451" class="title-copy" onclick="copyToClipboard(&#39;2404.03451&#39;)">[Copy]</a>
                <a id="kimi-2404.03451" class="title-kimi" onclick="chatKimi(&#39;2404.03451&#39;, this)">[Kimi<sup id="kimi-stars-2404.03451"></sup>]</a>
            </h2>
            <p id="authors-2404.03451" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dongang%20Wang" target="_blank"><span class="author">Dongang Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Peilin%20Liu" target="_blank"><span class="author">Peilin Liu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hengrui%20Wang" target="_blank"><span class="author">Hengrui Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Heidi%20Beadnall" target="_blank"><span class="author">Heidi Beadnall<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kain%20Kyle" target="_blank"><span class="author">Kain Kyle<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Linda%20Ly" target="_blank"><span class="author">Linda Ly<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mariano%20Cabezas" target="_blank"><span class="author">Mariano Cabezas<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Geng%20Zhan" target="_blank"><span class="author">Geng Zhan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ryan%20Sullivan" target="_blank"><span class="author">Ryan Sullivan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Weidong%20Cai" target="_blank"><span class="author">Weidong Cai<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wanli%20Ouyang" target="_blank"><span class="author">Wanli Ouyang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fernando%20Calamante" target="_blank"><span class="author">Fernando Calamante<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michael%20Barnett" target="_blank"><span class="author">Michael Barnett<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chenyu%20Wang" target="_blank"><span class="author">Chenyu Wang<span></span></span></a>
            </p>
            <p id="summary-2404.03451" class="summary">Training deep neural networks reliably requires access to large-scale datasets. However, obtaining such datasets can be challenging, especially in the context of neuroimaging analysis tasks, where the cost associated with image acquisition and annotation can be prohibitive. To mitigate both the time and financial costs associated with model development, a clear understanding of the amount of data required to train a satisfactory model is crucial. This paper focuses on an early stage phase of deep learning research, prior to model development, and proposes a strategic framework for estimating the amount of annotated data required to train patch-based segmentation networks. This framework includes the establishment of performance expectations using a novel Minor Boundary Adjustment for Threshold (MinBAT) method, and standardizing patch selection through the ROI-based Expanded Patch Selection (REPS) method. Our experiments demonstrate that tasks involving regions of interest (ROIs) with different sizes or shapes may yield variably acceptable Dice Similarity Coefficient (DSC) scores. By setting an acceptable DSC as the target, the required amount of training data can be estimated and even predicted as data accumulates. This approach could assist researchers and engineers in estimating the cost associated with data collection and annotation when defining a new segmentation task based on deep neural networks, ultimately contributing to their efficient translation to real-world applications.</p>
            <div id="pdf-container-2404.03451" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03451" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(32).html"></iframe>
            </div>
            <div id="kimi-container-2404.03451" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03446" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03446" target="_blank" title="34/93"><span class="index">#34</span></a>
                <a id="title-2404.03446" class="title-link" href="https://papers.cool/arxiv/2404.03446" target="_blank">SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering</a>
                <a id="pdf-2404.03446" class="title-pdf" onclick="togglePdf(&#39;2404.03446&#39;, &#39;https://arxiv.org/pdf/2404.03446.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03446">1</sup>]</a>
                <a id="copy-2404.03446" class="title-copy" onclick="copyToClipboard(&#39;2404.03446&#39;)">[Copy]</a>
                <a id="kimi-2404.03446" class="title-kimi" onclick="chatKimi(&#39;2404.03446&#39;, this)">[Kimi<sup id="kimi-stars-2404.03446">2</sup>]</a>
            </h2>
            <p id="authors-2404.03446" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chuyu%20Zhang" target="_blank"><span class="author">Chuyu Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hui%20Ren" target="_blank"><span class="author">Hui Ren<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xuming%20He" target="_blank"><span class="author">Xuming He<span></span></span></a>
            </p>
            <p id="summary-2404.03446" class="summary">Deep clustering, which learns representation and semantic clustering without labels information, poses a great challenge for deep learning-based approaches. Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the practical applicability of their methods. In this paper, we propose a more practical problem setting named deep imbalanced clustering, where the underlying classes exhibit an imbalance distribution. To address this challenge, we introduce a novel optimal transport-based pseudo-label learning framework. Our framework formulates pseudo-label generation as a Semantic-regularized Progressive Partial Optimal Transport (SP$^2$OT) problem, which progressively transports each sample to imbalanced clusters under several prior distribution and semantic relation constraints, thus generating high-quality and imbalance-aware pseudo-labels. To solve SP$^2$OT, we develop a Majorization-Minimization-based optimization algorithm. To be more precise, we employ the strategy of majorization to reformulate the SP$^2$OT problem into a Progressive Partial Optimal Transport problem, which can be transformed into an unbalanced optimal transport problem with augmented constraints and can be solved efficiently by a fast matrix scaling algorithm. Experiments on various datasets, including a human-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority of our method.</p>
            <div id="pdf-container-2404.03446" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03446" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(33).html"></iframe>
            </div>
            <div id="kimi-container-2404.03446" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03443" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03443" target="_blank" title="35/93"><span class="index">#35</span></a>
                <a id="title-2404.03443" class="title-link" href="https://papers.cool/arxiv/2404.03443" target="_blank">Part-Attention Based Model Make Occluded Person Re-Identification Stronger</a>
                <a id="pdf-2404.03443" class="title-pdf" onclick="togglePdf(&#39;2404.03443&#39;, &#39;https://arxiv.org/pdf/2404.03443.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03443">1</sup>]</a>
                <a id="copy-2404.03443" class="title-copy" onclick="copyToClipboard(&#39;2404.03443&#39;)">[Copy]</a>
                <a id="kimi-2404.03443" class="title-kimi" onclick="chatKimi(&#39;2404.03443&#39;, this)">[Kimi<sup id="kimi-stars-2404.03443"></sup>]</a>
            </h2>
            <p id="authors-2404.03443" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhihao%20Chen" target="_blank"><span class="author">Zhihao Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yiyuan%20Ge" target="_blank"><span class="author">Yiyuan Ge<span></span></span></a>
            </p>
            <p id="summary-2404.03443" class="summary">The goal of occluded person re-identification (ReID) is to retrieve specific pedestrians in occluded situations. However, occluded person ReID still suffers from background clutter and low-quality local feature representations, which limits model performance. In our research, we introduce a new framework called PAB-ReID, which is a novel ReID model incorporating part-attention mechanisms to tackle the aforementioned issues effectively. Firstly, we introduce the human parsing label to guide the generation of more accurate human part attention maps. In addition, we propose a fine-grained feature focuser for generating fine-grained human local feature representations while suppressing background interference. Moreover, We also design a part triplet loss to supervise the learning of human local features, which optimizes intra/inter-class distance. We conducted extensive experiments on specialized occlusion and regular ReID datasets, showcasing that our approach outperforms the existing state-of-the-art methods.</p>
            <div id="pdf-container-2404.03443" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03443" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(34).html"></iframe>
            </div>
            <div id="kimi-container-2404.03443" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03421" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03421" target="_blank" title="36/93"><span class="index">#36</span></a>
                <a id="title-2404.03421" class="title-link" href="https://papers.cool/arxiv/2404.03421" target="_blank">Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View</a>
                <a id="pdf-2404.03421" class="title-pdf" onclick="togglePdf(&#39;2404.03421&#39;, &#39;https://arxiv.org/pdf/2404.03421.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03421">2</sup>]</a>
                <a id="copy-2404.03421" class="title-copy" onclick="copyToClipboard(&#39;2404.03421&#39;)">[Copy]</a>
                <a id="kimi-2404.03421" class="title-kimi" onclick="chatKimi(&#39;2404.03421&#39;, this)">[Kimi<sup id="kimi-stars-2404.03421">1</sup>]</a>
            </h2>
            <p id="authors-2404.03421" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andreea%20Dogaru" target="_blank"><span class="author">Andreea Dogaru<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mert%20%C3%96zer" target="_blank"><span class="author">Mert Özer<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bernhard%20Egger" target="_blank"><span class="author">Bernhard Egger<span></span></span></a>
            </p>
            <p id="summary-2404.03421" class="summary">Single-view 3D reconstruction is currently approached from two dominant perspectives: reconstruction of scenes with limited diversity using 3D data supervision or reconstruction of diverse singular objects using large image priors. However, real-world scenarios are far more complex and exceed the capabilities of these methods. We therefore propose a hybrid method following a divide-and-conquer strategy. We first process the scene holistically, extracting depth and semantic information, and then leverage a single-shot object-level method for the detailed reconstruction of individual components. By following a compositional processing approach, the overall framework achieves full reconstruction of complex 3D scenes from a single image. We purposely design our pipeline to be highly modular by carefully integrating specific procedures for each processing step, without requiring an end-to-end training of the whole system. This enables the pipeline to naturally improve as future methods can replace the individual modules. We demonstrate the reconstruction performance of our approach on both synthetic and real-world scenes, comparing favorable against prior works. Project page: https://andreeadogaru.github.io/Gen3DSR.</p>
            <div id="pdf-container-2404.03421" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03421" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(35).html"></iframe>
            </div>
            <div id="kimi-container-2404.03421" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03417" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03417" target="_blank" title="37/93"><span class="index">#37</span></a>
                <a id="title-2404.03417" class="title-link" href="https://papers.cool/arxiv/2404.03417" target="_blank">NMF-Based Analysis of Mobile Eye-Tracking Data</a>
                <a id="pdf-2404.03417" class="title-pdf" onclick="togglePdf(&#39;2404.03417&#39;, &#39;https://arxiv.org/pdf/2404.03417.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03417">1</sup>]</a>
                <a id="copy-2404.03417" class="title-copy" onclick="copyToClipboard(&#39;2404.03417&#39;)">[Copy]</a>
                <a id="kimi-2404.03417" class="title-kimi" onclick="chatKimi(&#39;2404.03417&#39;, this)">[Kimi<sup id="kimi-stars-2404.03417"></sup>]</a>
            </h2>
            <p id="authors-2404.03417" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daniel%20Kl%C3%B6tzl" target="_blank"><span class="author">Daniel Klötzl<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tim%20Krake" target="_blank"><span class="author">Tim Krake<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Frank%20Heyen" target="_blank"><span class="author">Frank Heyen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michael%20Becher" target="_blank"><span class="author">Michael Becher<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Maurice%20Koch" target="_blank"><span class="author">Maurice Koch<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daniel%20Weiskopf" target="_blank"><span class="author">Daniel Weiskopf<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kuno%20Kurzhals" target="_blank"><span class="author">Kuno Kurzhals<span></span></span></a>
            </p>
            <p id="summary-2404.03417" class="summary">The depiction of scanpaths from mobile eye-tracking recordings by thumbnails from the stimulus allows the application of visual computing to detect areas of interest in an unsupervised way. We suggest using nonnegative matrix factorization (NMF) to identify such areas in stimuli. For a user-defined integer k, NMF produces an explainable decomposition into k components, each consisting of a spatial representation associated with a temporal indicator. In the context of multiple eye-tracking recordings, this leads to k spatial representations, where the temporal indicator highlights the appearance within recordings. The choice of k provides an opportunity to control the refinement of the decomposition, i.e., the number of areas to detect. We combine our NMF-based approach with visualization techniques to enable an exploratory analysis of multiple recordings. Finally, we demonstrate the usefulness of our approach with mobile eye-tracking data of an art gallery.</p>
            <div id="pdf-container-2404.03417" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03417" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(36).html"></iframe>
            </div>
            <div id="kimi-container-2404.03417" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03413" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03413" target="_blank" title="38/93"><span class="index">#38</span></a>
                <a id="title-2404.03413" class="title-link" href="https://papers.cool/arxiv/2404.03413" target="_blank">MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens</a>
                <a id="pdf-2404.03413" class="title-pdf" onclick="togglePdf(&#39;2404.03413&#39;, &#39;https://arxiv.org/pdf/2404.03413.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03413">5</sup>]</a>
                <a id="copy-2404.03413" class="title-copy" onclick="copyToClipboard(&#39;2404.03413&#39;)">[Copy]</a>
                <a id="kimi-2404.03413" class="title-kimi" onclick="chatKimi(&#39;2404.03413&#39;, this)">[Kimi<sup id="kimi-stars-2404.03413">5</sup>]</a>
            </h2>
            <p id="authors-2404.03413" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kirolos%20Ataallah" target="_blank"><span class="author">Kirolos Ataallah<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaoqian%20Shen" target="_blank"><span class="author">Xiaoqian Shen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Eslam%20Abdelrahman" target="_blank"><span class="author">Eslam Abdelrahman<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Essam%20Sleiman" target="_blank"><span class="author">Essam Sleiman<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Deyao%20Zhu" target="_blank"><span class="author">Deyao Zhu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jian%20Ding" target="_blank"><span class="author">Jian Ding<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mohamed%20Elhoseiny" target="_blank"><span class="author">Mohamed Elhoseiny<span></span></span></a>
            </p>
            <p id="summary-2404.03413" class="summary">This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/</p>
            <div id="pdf-container-2404.03413" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03413" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(37).html"></iframe>
            </div>
            <div id="kimi-container-2404.03413" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03407" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03407" target="_blank" title="39/93"><span class="index">#39</span></a>
                <a id="title-2404.03407" class="title-link" href="https://papers.cool/arxiv/2404.03407" target="_blank">AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment</a>
                <a id="pdf-2404.03407" class="title-pdf" onclick="togglePdf(&#39;2404.03407&#39;, &#39;https://arxiv.org/pdf/2404.03407.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03407">1</sup>]</a>
                <a id="copy-2404.03407" class="title-copy" onclick="copyToClipboard(&#39;2404.03407&#39;)">[Copy]</a>
                <a id="kimi-2404.03407" class="title-kimi" onclick="chatKimi(&#39;2404.03407&#39;, this)">[Kimi<sup id="kimi-stars-2404.03407">2</sup>]</a>
            </h2>
            <p id="authors-2404.03407" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chunyi%20Li" target="_blank"><span class="author">Chunyi Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tengchuan%20Kou" target="_blank"><span class="author">Tengchuan Kou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yixuan%20Gao" target="_blank"><span class="author">Yixuan Gao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuqin%20Cao" target="_blank"><span class="author">Yuqin Cao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wei%20Sun" target="_blank"><span class="author">Wei Sun<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zicheng%20Zhang" target="_blank"><span class="author">Zicheng Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yingjie%20Zhou" target="_blank"><span class="author">Yingjie Zhou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhichao%20Zhang" target="_blank"><span class="author">Zhichao Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Weixia%20Zhang" target="_blank"><span class="author">Weixia Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haoning%20Wu" target="_blank"><span class="author">Haoning Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaohong%20Liu" target="_blank"><span class="author">Xiaohong Liu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiongkuo%20Min" target="_blank"><span class="author">Xiongkuo Min<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guangtao%20Zhai" target="_blank"><span class="author">Guangtao Zhai<span></span></span></a>
            </p>
            <p id="summary-2404.03407" class="summary">With the rapid advancements in AI-Generated Content (AIGC), AI-Generated Images (AIGIs) have been widely applied in entertainment, education, and social media. However, due to the significant variance in quality among different AIGIs, there is an urgent need for models that consistently match human subjective ratings. To address this issue, we organized a challenge towards AIGC quality assessment on NTIRE 2024 that extensively considers 15 popular generative models, utilizing dynamic hyper-parameters (including classifier-free guidance, iteration epochs, and output image resolution), and gather subjective scores that consider perceptual quality and text-to-image alignment altogether comprehensively involving 21 subjects. This approach culminates in the creation of the largest fine-grained AIGI subjective quality database to date with 20,000 AIGIs and 420,000 subjective ratings, known as AIGIQA-20K. Furthermore, we conduct benchmark experiments on this database to assess the correspondence between 16 mainstream AIGI quality models and human perception. We anticipate that this large-scale quality database will inspire robust quality indicators for AIGIs and propel the evolution of AIGC for vision. The database is released on https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image.</p>
            <div id="pdf-container-2404.03407" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03407" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(38).html"></iframe>
            </div>
            <div id="kimi-container-2404.03407" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03398" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03398" target="_blank" title="40/93"><span class="index">#40</span></a>
                <a id="title-2404.03398" class="title-link" href="https://papers.cool/arxiv/2404.03398" target="_blank">Scaling Up Video Summarization Pretraining with Large Language Models</a>
                <a id="pdf-2404.03398" class="title-pdf" onclick="togglePdf(&#39;2404.03398&#39;, &#39;https://arxiv.org/pdf/2404.03398.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03398">1</sup>]</a>
                <a id="copy-2404.03398" class="title-copy" onclick="copyToClipboard(&#39;2404.03398&#39;)">[Copy]</a>
                <a id="kimi-2404.03398" class="title-kimi" onclick="chatKimi(&#39;2404.03398&#39;, this)">[Kimi<sup id="kimi-stars-2404.03398">2</sup>]</a>
            </h2>
            <p id="authors-2404.03398" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dawit%20Mureja%20Argaw" target="_blank"><span class="author">Dawit Mureja Argaw<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Seunghyun%20Yoon" target="_blank"><span class="author">Seunghyun Yoon<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fabian%20Caba%20Heilbron" target="_blank"><span class="author">Fabian Caba Heilbron<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hanieh%20Deilamsalehy" target="_blank"><span class="author">Hanieh Deilamsalehy<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Trung%20Bui" target="_blank"><span class="author">Trung Bui<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhaowen%20Wang" target="_blank"><span class="author">Zhaowen Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Franck%20Dernoncourt" target="_blank"><span class="author">Franck Dernoncourt<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Joon%20Son%20Chung" target="_blank"><span class="author">Joon Son Chung<span></span></span></a>
            </p>
            <p id="summary-2404.03398" class="summary">Long-form video content constitutes a significant portion of internet traffic, making automated video summarization an essential research problem. However, existing video summarization datasets are notably limited in their size, constraining the effectiveness of state-of-the-art methods for generalization. Our work aims to overcome this limitation by capitalizing on the abundance of long-form videos with dense speech-to-video alignment and the remarkable capabilities of recent large language models (LLMs) in summarizing long text. We introduce an automated and scalable pipeline for generating a large-scale video summarization dataset using LLMs as Oracle summarizers. By leveraging the generated dataset, we analyze the limitations of existing approaches and propose a new video summarization model that effectively addresses them. To facilitate further research in the field, our work also presents a new benchmark dataset that contains 1200 long videos each with high-quality summaries annotated by professionals. Extensive experiments clearly indicate that our proposed approach sets a new state-of-the-art in video summarization across several benchmarks.</p>
            <div id="pdf-container-2404.03398" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03398" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(39).html"></iframe>
            </div>
            <div id="kimi-container-2404.03398" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03394" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03394" target="_blank" title="41/93"><span class="index">#41</span></a>
                <a id="title-2404.03394" class="title-link" href="https://papers.cool/arxiv/2404.03394" target="_blank">Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation</a>
                <a id="pdf-2404.03394" class="title-pdf" onclick="togglePdf(&#39;2404.03394&#39;, &#39;https://arxiv.org/pdf/2404.03394.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03394">1</sup>]</a>
                <a id="copy-2404.03394" class="title-copy" onclick="copyToClipboard(&#39;2404.03394&#39;)">[Copy]</a>
                <a id="kimi-2404.03394" class="title-kimi" onclick="chatKimi(&#39;2404.03394&#39;, this)">[Kimi<sup id="kimi-stars-2404.03394"></sup>]</a>
            </h2>
            <p id="authors-2404.03394" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Izumi%20Fujimori" target="_blank"><span class="author">Izumi Fujimori<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Masaki%20Oono" target="_blank"><span class="author">Masaki Oono<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Masami%20Shishibori" target="_blank"><span class="author">Masami Shishibori<span></span></span></a>
            </p>
            <p id="summary-2404.03394" class="summary">In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.</p>
            <div id="pdf-container-2404.03394" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03394" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(40).html"></iframe>
            </div>
            <div id="kimi-container-2404.03394" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03392" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03392" target="_blank" title="42/93"><span class="index">#42</span></a>
                <a id="title-2404.03392" class="title-link" href="https://papers.cool/arxiv/2404.03392" target="_blank">Two Tricks to Improve Unsupervised Segmentation Learning</a>
                <a id="pdf-2404.03392" class="title-pdf" onclick="togglePdf(&#39;2404.03392&#39;, &#39;https://arxiv.org/pdf/2404.03392.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03392">4</sup>]</a>
                <a id="copy-2404.03392" class="title-copy" onclick="copyToClipboard(&#39;2404.03392&#39;)">[Copy]</a>
                <a id="kimi-2404.03392" class="title-kimi" onclick="chatKimi(&#39;2404.03392&#39;, this)">[Kimi<sup id="kimi-stars-2404.03392">5</sup>]</a>
            </h2>
            <p id="authors-2404.03392" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alp%20Eren%20Sari" target="_blank"><span class="author">Alp Eren Sari<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Francesco%20Locatello" target="_blank"><span class="author">Francesco Locatello<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Paolo%20Favar" target="_blank"><span class="author">Paolo Favar<span></span></span></a>
            </p>
            <p id="summary-2404.03392" class="summary">We present two practical improvement techniques for unsupervised segmentation learning. These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods. Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs. Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme. This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other. Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques.</p>
            <div id="pdf-container-2404.03392" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03392" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(41).html"></iframe>
            </div>
            <div id="kimi-container-2404.03392" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03384" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03384" target="_blank" title="43/93"><span class="index">#43</span></a>
                <a id="title-2404.03384" class="title-link" href="https://papers.cool/arxiv/2404.03384" target="_blank">LongVLM: Efficient Long Video Understanding via Large Language Models</a>
                <a id="pdf-2404.03384" class="title-pdf" onclick="togglePdf(&#39;2404.03384&#39;, &#39;https://arxiv.org/pdf/2404.03384.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03384">2</sup>]</a>
                <a id="copy-2404.03384" class="title-copy" onclick="copyToClipboard(&#39;2404.03384&#39;)">[Copy]</a>
                <a id="kimi-2404.03384" class="title-kimi" onclick="chatKimi(&#39;2404.03384&#39;, this)">[Kimi<sup id="kimi-stars-2404.03384">2</sup>]</a>
            </h2>
            <p id="authors-2404.03384" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuetian%20Weng" target="_blank"><span class="author">Yuetian Weng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mingfei%20Han" target="_blank"><span class="author">Mingfei Han<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haoyu%20He" target="_blank"><span class="author">Haoyu He<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaojun%20Chang" target="_blank"><span class="author">Xiaojun Chang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bohan%20Zhuang" target="_blank"><span class="author">Bohan Zhuang<span></span></span></a>
            </p>
            <p id="summary-2404.03384" class="summary">Empowered by Large Language Models (LLMs), recent advancements in VideoLLMs have driven progress in various video understanding tasks. These models encode video representations through pooling or query aggregation over a vast number of visual tokens, making computational and memory costs affordable. Despite successfully providing an overall comprehension of video content, existing VideoLLMs still face challenges in achieving detailed understanding in videos due to overlooking local information in long-term videos. To tackle this challenge, we introduce LongVLM, a straightforward yet powerful VideoLLM for long video understanding, building upon the observation that long videos often consist of sequential key events, complex actions, and camera movements. Our approach proposes to decompose long videos into multiple short-term segments and encode local features for each local segment via a hierarchical token merging module. These features are concatenated in temporal order to maintain the storyline across sequential short-term segments. Additionally, we propose to integrate global semantics into each local feature to enhance context understanding. In this way, we encode video representations that incorporate both local and global information, enabling the LLM to generate comprehensive responses for long-term videos. Experimental results on the VideoChatGPT benchmark and zero-shot video question-answering datasets demonstrate the superior capabilities of our model over the previous state-of-the-art methods. Qualitative examples demonstrate that our model produces more precise responses for long videos understanding. Code is available at \url{https://github.com/ziplab/LongVLM}.</p>
            <div id="pdf-container-2404.03384" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03384" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(42).html"></iframe>
            </div>
            <div id="kimi-container-2404.03384" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03349" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03349" target="_blank" title="44/93"><span class="index">#44</span></a>
                <a id="title-2404.03349" class="title-link" href="https://papers.cool/arxiv/2404.03349" target="_blank">VF-NeRF: Viewshed Fields for Rigid NeRF Registration</a>
                <a id="pdf-2404.03349" class="title-pdf" onclick="togglePdf(&#39;2404.03349&#39;, &#39;https://arxiv.org/pdf/2404.03349.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03349"></sup>]</a>
                <a id="copy-2404.03349" class="title-copy" onclick="copyToClipboard(&#39;2404.03349&#39;)">[Copy]</a>
                <a id="kimi-2404.03349" class="title-kimi" onclick="chatKimi(&#39;2404.03349&#39;, this)">[Kimi<sup id="kimi-stars-2404.03349">1</sup>]</a>
            </h2>
            <p id="authors-2404.03349" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Leo%20Segre" target="_blank"><span class="author">Leo Segre<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shai%20Avidan" target="_blank"><span class="author">Shai Avidan<span></span></span></a>
            </p>
            <p id="summary-2404.03349" class="summary">3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese.</p>
            <div id="pdf-container-2404.03349" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03349" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(43).html"></iframe>
            </div>
            <div id="kimi-container-2404.03349" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03340" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03340" target="_blank" title="45/93"><span class="index">#45</span></a>
                <a id="title-2404.03340" class="title-link" href="https://papers.cool/arxiv/2404.03340" target="_blank">Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks</a>
                <a id="pdf-2404.03340" class="title-pdf" onclick="togglePdf(&#39;2404.03340&#39;, &#39;https://arxiv.org/pdf/2404.03340.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03340">1</sup>]</a>
                <a id="copy-2404.03340" class="title-copy" onclick="copyToClipboard(&#39;2404.03340&#39;)">[Copy]</a>
                <a id="kimi-2404.03340" class="title-kimi" onclick="chatKimi(&#39;2404.03340&#39;, this)">[Kimi<sup id="kimi-stars-2404.03340"></sup>]</a>
            </h2>
            <p id="authors-2404.03340" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lei%20Zhang" target="_blank"><span class="author">Lei Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuhang%20Zhou" target="_blank"><span class="author">Yuhang Zhou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yi%20Yang" target="_blank"><span class="author">Yi Yang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xinbo%20Gao" target="_blank"><span class="author">Xinbo Gao<span></span></span></a>
            </p>
            <p id="summary-2404.03340" class="summary">Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks. Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked. Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed. Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID). Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle. The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance. 2) The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration. Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks.</p>
            <div id="pdf-container-2404.03340" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03340" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(44).html"></iframe>
            </div>
            <div id="kimi-container-2404.03340" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03327" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03327" target="_blank" title="46/93"><span class="index">#46</span></a>
                <a id="title-2404.03327" class="title-link" href="https://papers.cool/arxiv/2404.03327" target="_blank">DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement</a>
                <a id="pdf-2404.03327" class="title-pdf" onclick="togglePdf(&#39;2404.03327&#39;, &#39;https://arxiv.org/pdf/2404.03327.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03327">1</sup>]</a>
                <a id="copy-2404.03327" class="title-copy" onclick="copyToClipboard(&#39;2404.03327&#39;)">[Copy]</a>
                <a id="kimi-2404.03327" class="title-kimi" onclick="chatKimi(&#39;2404.03327&#39;, this)">[Kimi<sup id="kimi-stars-2404.03327"></sup>]</a>
            </h2>
            <p id="authors-2404.03327" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shangquan%20Sun" target="_blank"><span class="author">Shangquan Sun<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenqi%20Ren" target="_blank"><span class="author">Wenqi Ren<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingyang%20Peng" target="_blank"><span class="author">Jingyang Peng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fenglong%20Song" target="_blank"><span class="author">Fenglong Song<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiaochun%20Cao" target="_blank"><span class="author">Xiaochun Cao<span></span></span></a>
            </p>
            <p id="summary-2404.03327" class="summary">Many existing methods for low-light image enhancement (LLIE) based on Retinex theory ignore important factors that affect the validity of this theory in digital imaging, such as noise, quantization error, non-linearity, and dynamic range overflow. In this paper, we propose a new expression called Digital-Imaging Retinex theory (DI-Retinex) through theoretical and experimental analysis of Retinex theory in digital imaging. Our new expression includes an offset term in the enhancement model, which allows for pixel-wise brightness contrast adjustment with a non-linear mapping function. In addition, to solve the lowlight enhancement problem in an unsupervised manner, we propose an image-adaptive masked reverse degradation loss in Gamma space. We also design a variance suppression loss for regulating the additional offset term. Extensive experiments show that our proposed method outperforms all existing unsupervised methods in terms of visual quality, model size, and speed. Our algorithm can also assist downstream face detectors in low-light, as it shows the most performance gain after the low-light enhancement compared to other methods.</p>
            <div id="pdf-container-2404.03327" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03327" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(45).html"></iframe>
            </div>
            <div id="kimi-container-2404.03327" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03323" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03323" target="_blank" title="47/93"><span class="index">#47</span></a>
                <a id="title-2404.03323" class="title-link" href="https://papers.cool/arxiv/2404.03323" target="_blank">Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning</a>
                <a id="pdf-2404.03323" class="title-pdf" onclick="togglePdf(&#39;2404.03323&#39;, &#39;https://arxiv.org/pdf/2404.03323.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03323">3</sup>]</a>
                <a id="copy-2404.03323" class="title-copy" onclick="copyToClipboard(&#39;2404.03323&#39;)">[Copy]</a>
                <a id="kimi-2404.03323" class="title-kimi" onclick="chatKimi(&#39;2404.03323&#39;, this)">[Kimi<sup id="kimi-stars-2404.03323">8</sup>]</a>
            </h2>
            <p id="authors-2404.03323" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andrei%20Semenov" target="_blank"><span class="author">Andrei Semenov<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Vladimir%20Ivanov" target="_blank"><span class="author">Vladimir Ivanov<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aleksandr%20Beznosikov" target="_blank"><span class="author">Aleksandr Beznosikov<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alexander%20Gasnikov" target="_blank"><span class="author">Alexander Gasnikov<span></span></span></a>
            </p>
            <p id="summary-2404.03323" class="summary">We propose a novel architecture and method of explainable classification with Concept Bottleneck Models (CBMs). While SOTA approaches to Image Classification task work as a black box, there is a growing demand for models that would provide interpreted results. Such a models often learn to predict the distribution over class labels using additional description of this target instances, called concepts. However, existing Bottleneck methods have a number of limitations: their accuracy is lower than that of a standard model and CBMs require an additional set of concepts to leverage. We provide a framework for creating Concept Bottleneck Model from pre-trained multi-modal encoder and new CLIP-like architectures. By introducing a new type of layers known as Concept Bottleneck Layers, we outline three methods for training them: with $\ell_1$-loss, contrastive loss and loss function based on Gumbel-Softmax distribution (Sparse-CBM), while final FC layer is still trained with Cross-Entropy. We show a significant increase in accuracy using sparse hidden layers in CLIP-based bottleneck models. Which means that sparse representation of concepts activation vector is meaningful in Concept Bottleneck Models. Moreover, with our Concept Matrix Search algorithm we can improve CLIP predictions on complex datasets without any additional training or fine-tuning. The code is available at: https://github.com/Andron00e/SparseCBM.</p>
            <div id="pdf-container-2404.03323" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03323" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(46).html"></iframe>
            </div>
            <div id="kimi-container-2404.03323" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03296" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03296" target="_blank" title="48/93"><span class="index">#48</span></a>
                <a id="title-2404.03296" class="title-link" href="https://papers.cool/arxiv/2404.03296" target="_blank">AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution</a>
                <a id="pdf-2404.03296" class="title-pdf" onclick="togglePdf(&#39;2404.03296&#39;, &#39;https://arxiv.org/pdf/2404.03296.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03296">1</sup>]</a>
                <a id="copy-2404.03296" class="title-copy" onclick="copyToClipboard(&#39;2404.03296&#39;)">[Copy]</a>
                <a id="kimi-2404.03296" class="title-kimi" onclick="chatKimi(&#39;2404.03296&#39;, this)">[Kimi<sup id="kimi-stars-2404.03296"></sup>]</a>
            </h2>
            <p id="authors-2404.03296" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Cheeun%20Hong" target="_blank"><span class="author">Cheeun Hong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kyoung%20Mu%20Lee" target="_blank"><span class="author">Kyoung Mu Lee<span></span></span></a>
            </p>
            <p id="summary-2404.03296" class="summary">Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computational costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adapting the quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant adaptive network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs to learn the appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time from hours to seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map the input image to the image-wise bit adaptation factor and one to obtain the layer-wise adaptation factors. These bit mappings are calibrated and fine-tuned using only a small number of calibration images. We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2000. Codes are available at https://github.com/Cheeun/AdaBM.</p>
            <div id="pdf-container-2404.03296" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03296" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(47).html"></iframe>
            </div>
            <div id="kimi-container-2404.03296" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03277" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03277" target="_blank" title="49/93"><span class="index">#49</span></a>
                <a id="title-2404.03277" class="title-link" href="https://papers.cool/arxiv/2404.03277" target="_blank">Design and Development of a Framework For Stroke-Based Handwritten Gujarati Font Generation</a>
                <a id="pdf-2404.03277" class="title-pdf" onclick="togglePdf(&#39;2404.03277&#39;, &#39;https://arxiv.org/pdf/2404.03277.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03277">1</sup>]</a>
                <a id="copy-2404.03277" class="title-copy" onclick="copyToClipboard(&#39;2404.03277&#39;)">[Copy]</a>
                <a id="kimi-2404.03277" class="title-kimi" onclick="chatKimi(&#39;2404.03277&#39;, this)">[Kimi<sup id="kimi-stars-2404.03277"></sup>]</a>
            </h2>
            <p id="authors-2404.03277" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Preeti%20P.%20Bhatt" target="_blank"><span class="author">Preeti P. Bhatt<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jitendra%20V.%20Nasriwala" target="_blank"><span class="author">Jitendra V. Nasriwala<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rakesh%20R.%20Savant" target="_blank"><span class="author">Rakesh R. Savant<span></span></span></a>
            </p>
            <p id="summary-2404.03277" class="summary">Handwritten font generation is important for preserving cultural heritage and creating personalized designs. It adds an authentic and expressive touch to printed materials, making them visually appealing and establishing a stronger connection with the audience. This paper aims to design a framework for generating handwritten fonts in the Gujarati script, mimicking the variation of human handwriting. The proposed font generation model consists of a learning phase and a generation phase. In the learning phase, Gujarati scripts are analyzed, and rules for designing each character are formulated. This ruleset involves the concatenation of strokes in a stroke-based manner, ensuring visual consistency in the resulting glyphs. The generation phase involves the user providing a small subset of characters, and the system automatically generates the remaining character glyphs based on extracted strokes and learned rules, resulting in handwritten Gujarati fonts. The resulting character glyphs are converted into an open-type font using the FontForge tool, making them compatible with any Gujarati editor. Both subjective and objective evaluations are conducted to assess the synthesized images and fonts. Subjective evaluation through user studies provides feedback on quality and visual appeal, achieving an overall accuracy of 84.84%. Notably, eleven characters demonstrated a success ratio above 90%. Objective evaluation using an existing recognition system achieves an overall accuracy of 84.28% in OCR evaluation. Notably, fifteen characters had a success ratio of 80% or higher.</p>
            <div id="pdf-container-2404.03277" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03277" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(48).html"></iframe>
            </div>
            <div id="kimi-container-2404.03277" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03256" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03256" target="_blank" title="50/93"><span class="index">#50</span></a>
                <a id="title-2404.03256" class="title-link" href="https://papers.cool/arxiv/2404.03256" target="_blank">Multi Positive Contrastive Learning with Pose-Consistent Generated Images</a>
                <a id="pdf-2404.03256" class="title-pdf" onclick="togglePdf(&#39;2404.03256&#39;, &#39;https://arxiv.org/pdf/2404.03256.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03256">2</sup>]</a>
                <a id="copy-2404.03256" class="title-copy" onclick="copyToClipboard(&#39;2404.03256&#39;)">[Copy]</a>
                <a id="kimi-2404.03256" class="title-kimi" onclick="chatKimi(&#39;2404.03256&#39;, this)">[Kimi<sup id="kimi-stars-2404.03256">2</sup>]</a>
            </h2>
            <p id="authors-2404.03256" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sho%20Inayoshi" target="_blank"><span class="author">Sho Inayoshi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aji%20Resindra%20Widya" target="_blank"><span class="author">Aji Resindra Widya<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Satoshi%20Ozaki" target="_blank"><span class="author">Satoshi Ozaki<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Junji%20Otsuka" target="_blank"><span class="author">Junji Otsuka<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Takeshi%20Ohashi" target="_blank"><span class="author">Takeshi Ohashi<span></span></span></a>
            </p>
            <p id="summary-2404.03256" class="summary">Model pre-training has become essential in various recognition tasks. Meanwhile, with the remarkable advancements in image generation models, pre-training methods utilizing generated images have also emerged given their ability to produce unlimited training data. However, while existing methods utilizing generated images excel in classification, they fall short in more practical tasks, such as human pose estimation. In this paper, we have experimentally demonstrated it and propose the generation of visually distinct images with identical human poses. We then propose a novel multi-positive contrastive learning, which optimally utilize the previously generated images to learn structural features of the human body. We term the entire learning pipeline as GenPoCCL. Despite using only less than 1% amount of data compared to current state-of-the-art method, GenPoCCL captures structural features of the human body more effectively, surpassing existing methods in a variety of human-centric perception tasks.</p>
            <div id="pdf-container-2404.03256" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03256" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(49).html"></iframe>
            </div>
            <div id="kimi-container-2404.03256" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03251" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03251" target="_blank" title="51/93"><span class="index">#51</span></a>
                <a id="title-2404.03251" class="title-link" href="https://papers.cool/arxiv/2404.03251" target="_blank">Real-time Noise Source Estimation of a Camera System from an Image and Metadata</a>
                <a id="pdf-2404.03251" class="title-pdf" onclick="togglePdf(&#39;2404.03251&#39;, &#39;https://arxiv.org/pdf/2404.03251.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03251">1</sup>]</a>
                <a id="copy-2404.03251" class="title-copy" onclick="copyToClipboard(&#39;2404.03251&#39;)">[Copy]</a>
                <a id="kimi-2404.03251" class="title-kimi" onclick="chatKimi(&#39;2404.03251&#39;, this)">[Kimi<sup id="kimi-stars-2404.03251"></sup>]</a>
            </h2>
            <p id="authors-2404.03251" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Maik%20Wischow" target="_blank"><span class="author">Maik Wischow<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Patrick%20Irmisch" target="_blank"><span class="author">Patrick Irmisch<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anko%20Boerner" target="_blank"><span class="author">Anko Boerner<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guillermo%20Gallego" target="_blank"><span class="author">Guillermo Gallego<span></span></span></a>
            </p>
            <p id="summary-2404.03251" class="summary">Autonomous machines must self-maintain proper functionality to ensure the safety of humans and themselves. This pertains particularly to its cameras as predominant sensors to perceive the environment and support actions. A fundamental camera problem addressed in this study is noise. Solutions often focus on denoising images a posteriori, that is, fighting symptoms rather than root causes. However, tackling root causes requires identifying the noise sources, considering the limitations of mobile platforms. This work investigates a real-time, memory-efficient and reliable noise source estimator that combines data- and physically-based models. To this end, a DNN that examines an image with camera metadata for major camera noise sources is built and trained. In addition, it quantifies unexpected factors that impact image noise or metadata. This study investigates seven different estimators on six datasets that include synthetic noise, real-world noise from two camera systems, and real field campaigns. For these, only the model with most metadata is capable to accurately and robustly quantify all individual noise contributions. This method outperforms total image noise estimators and can be plug-and-play deployed. It also serves as a basis to include more advanced noise sources, or as part of an automatic countermeasure feedback-loop to approach fully reliable machines.</p>
            <div id="pdf-container-2404.03251" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03251" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(50).html"></iframe>
            </div>
            <div id="kimi-container-2404.03251" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03248" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03248" target="_blank" title="52/93"><span class="index">#52</span></a>
                <a id="title-2404.03248" class="title-link" href="https://papers.cool/arxiv/2404.03248" target="_blank">Learning Transferable Negative Prompts for Out-of-Distribution Detection</a>
                <a id="pdf-2404.03248" class="title-pdf" onclick="togglePdf(&#39;2404.03248&#39;, &#39;https://arxiv.org/pdf/2404.03248.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03248">5</sup>]</a>
                <a id="copy-2404.03248" class="title-copy" onclick="copyToClipboard(&#39;2404.03248&#39;)">[Copy]</a>
                <a id="kimi-2404.03248" class="title-kimi" onclick="chatKimi(&#39;2404.03248&#39;, this)">[Kimi<sup id="kimi-stars-2404.03248">5</sup>]</a>
            </h2>
            <p id="authors-2404.03248" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tianqi%20Li" target="_blank"><span class="author">Tianqi Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guansong%20Pang" target="_blank"><span class="author">Guansong Pang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiao%20Bai" target="_blank"><span class="author">Xiao Bai<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wenjun%20Miao" target="_blank"><span class="author">Wenjun Miao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jin%20Zheng" target="_blank"><span class="author">Jin Zheng<span></span></span></a>
            </p>
            <p id="summary-2404.03248" class="summary">Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate. To address this issue, we introduce a novel OOD detection method, named 'NegPrompt', to learn a set of negative prompts, each representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images. It learns such negative prompts with ID data only, without any reliance on external outlier data. Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training. In contrast, our learned negative prompts are transferable to novel class labels. Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios. Code is available at https://github.com/mala-lab/negprompt.</p>
            <div id="pdf-container-2404.03248" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03248" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(51).html"></iframe>
            </div>
            <div id="kimi-container-2404.03248" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03242" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03242" target="_blank" title="53/93"><span class="index">#53</span></a>
                <a id="title-2404.03242" class="title-link" href="https://papers.cool/arxiv/2404.03242" target="_blank">Would Deep Generative Models Amplify Bias in Future Models?</a>
                <a id="pdf-2404.03242" class="title-pdf" onclick="togglePdf(&#39;2404.03242&#39;, &#39;https://arxiv.org/pdf/2404.03242.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03242">2</sup>]</a>
                <a id="copy-2404.03242" class="title-copy" onclick="copyToClipboard(&#39;2404.03242&#39;)">[Copy]</a>
                <a id="kimi-2404.03242" class="title-kimi" onclick="chatKimi(&#39;2404.03242&#39;, this)">[Kimi<sup id="kimi-stars-2404.03242">3</sup>]</a>
            </h2>
            <p id="authors-2404.03242" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tianwei%20Chen" target="_blank"><span class="author">Tianwei Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yusuke%20Hirota" target="_blank"><span class="author">Yusuke Hirota<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mayu%20Otani" target="_blank"><span class="author">Mayu Otani<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Noa%20Garcia" target="_blank"><span class="author">Noa Garcia<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuta%20Nakashima" target="_blank"><span class="author">Yuta Nakashima<span></span></span></a>
            </p>
            <p id="summary-2404.03242" class="summary">We investigate the impact of deep generative models on potential social biases in upcoming computer vision models. As the internet witnesses an increasing influx of AI-generated images, concerns arise regarding inherent biases that may accompany them, potentially leading to the dissemination of harmful content. This paper explores whether a detrimental feedback loop, resulting in bias amplification, would occur if generated images were used as the training data for future models. We conduct simulations by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion. The modified datasets are used to train OpenCLIP and image captioning models, which we evaluate in terms of quality and bias. Contrary to expectations, our findings indicate that introducing generated images during training does not uniformly amplify bias. Instead, instances of bias mitigation across specific tasks are observed. We further explore the factors that may influence these phenomena, such as artifacts in image generation (e.g., blurry faces) or pre-existing biases in the original datasets.</p>
            <div id="pdf-container-2404.03242" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03242" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(52).html"></iframe>
            </div>
            <div id="kimi-container-2404.03242" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03225" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03225" target="_blank" title="54/93"><span class="index">#54</span></a>
                <a id="title-2404.03225" class="title-link" href="https://papers.cool/arxiv/2404.03225" target="_blank">FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification</a>
                <a id="pdf-2404.03225" class="title-pdf" onclick="togglePdf(&#39;2404.03225&#39;, &#39;https://arxiv.org/pdf/2404.03225.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03225">1</sup>]</a>
                <a id="copy-2404.03225" class="title-copy" onclick="copyToClipboard(&#39;2404.03225&#39;)">[Copy]</a>
                <a id="kimi-2404.03225" class="title-kimi" onclick="chatKimi(&#39;2404.03225&#39;, this)">[Kimi<sup id="kimi-stars-2404.03225">1</sup>]</a>
            </h2>
            <p id="authors-2404.03225" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xu%20Wang" target="_blank"><span class="author">Xu Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tian%20Ye" target="_blank"><span class="author">Tian Ye<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rajgopal%20Kannan" target="_blank"><span class="author">Rajgopal Kannan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Viktor%20Prasanna" target="_blank"><span class="author">Viktor Prasanna<span></span></span></a>
            </p>
            <p id="summary-2404.03225" class="summary">Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR), while delivering improved performance, have been shown to be quite vulnerable to adversarial attacks. Existing works improve robustness by training models on adversarial samples. However, by focusing mostly on attacks that manipulate images randomly, they neglect the real-world feasibility of such attacks. In this paper, we propose FACTUAL, a novel Contrastive Learning framework for Adversarial Training and robust SAR classification. FACTUAL consists of two components: (1) Differing from existing works, a novel perturbation scheme that incorporates realistic physical adversarial attacks (such as OTSA) to build a supervised adversarial pre-training network. This network utilizes class labels for clustering clean and perturbed images together into a more informative feature space. (2) A linear classifier cascaded after the encoder to use the computed representations to predict the target labels. By pre-training and fine-tuning our model on both clean and adversarial samples, we show that our model achieves high prediction accuracy on both cases. Our model achieves 99.7% accuracy on clean samples, and 89.6% on perturbed samples, both outperforming previous state-of-the-art methods.</p>
            <div id="pdf-container-2404.03225" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03225" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(53).html"></iframe>
            </div>
            <div id="kimi-container-2404.03225" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03219" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03219" target="_blank" title="55/93"><span class="index">#55</span></a>
                <a id="title-2404.03219" class="title-link" href="https://papers.cool/arxiv/2404.03219" target="_blank">iSeg: Interactive 3D Segmentation via Interactive Attention</a>
                <a id="pdf-2404.03219" class="title-pdf" onclick="togglePdf(&#39;2404.03219&#39;, &#39;https://arxiv.org/pdf/2404.03219.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03219"></sup>]</a>
                <a id="copy-2404.03219" class="title-copy" onclick="copyToClipboard(&#39;2404.03219&#39;)">[Copy]</a>
                <a id="kimi-2404.03219" class="title-kimi" onclick="chatKimi(&#39;2404.03219&#39;, this)">[Kimi<sup id="kimi-stars-2404.03219">1</sup>]</a>
            </h2>
            <p id="authors-2404.03219" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Itai%20Lang" target="_blank"><span class="author">Itai Lang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fei%20Xu" target="_blank"><span class="author">Fei Xu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dale%20Decatur" target="_blank"><span class="author">Dale Decatur<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sudarshan%20Babu" target="_blank"><span class="author">Sudarshan Babu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rana%20Hanocka" target="_blank"><span class="author">Rana Hanocka<span></span></span></a>
            </p>
            <p id="summary-2404.03219" class="summary">We present iSeg, a new interactive technique for segmenting 3D shapes. Previous works have focused mainly on leveraging pre-trained 2D foundation models for 3D segmentation based on text. However, text may be insufficient for accurately describing fine-grained spatial segmentations. Moreover, achieving a consistent 3D segmentation using a 2D model is challenging since occluded areas of the same semantic region may not be visible together from any 2D view. Thus, we design a segmentation method conditioned on fine user clicks, which operates entirely in 3D. Our system accepts user clicks directly on the shape's surface, indicating the inclusion or exclusion of regions from the desired shape partition. To accommodate various click settings, we propose a novel interactive attention module capable of processing different numbers and types of clicks, enabling the training of a single unified interactive segmentation model. We apply iSeg to a myriad of shapes from different domains, demonstrating its versatility and faithfulness to the user's specifications. Our project page is at https://threedle.github.io/iSeg/.</p>
            <div id="pdf-container-2404.03219" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03219" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(54).html"></iframe>
            </div>
            <div id="kimi-container-2404.03219" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03214" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03214" target="_blank" title="56/93"><span class="index">#56</span></a>
                <a id="title-2404.03214" class="title-link" href="https://papers.cool/arxiv/2404.03214" target="_blank">LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity</a>
                <a id="pdf-2404.03214" class="title-pdf" onclick="togglePdf(&#39;2404.03214&#39;, &#39;https://arxiv.org/pdf/2404.03214.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03214">1</sup>]</a>
                <a id="copy-2404.03214" class="title-copy" onclick="copyToClipboard(&#39;2404.03214&#39;)">[Copy]</a>
                <a id="kimi-2404.03214" class="title-kimi" onclick="chatKimi(&#39;2404.03214&#39;, this)">[Kimi<sup id="kimi-stars-2404.03214">3</sup>]</a>
            </h2>
            <p id="authors-2404.03214" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Walid%20Bousselham" target="_blank"><span class="author">Walid Bousselham<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Angie%20Boggust" target="_blank"><span class="author">Angie Boggust<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sofian%20Chaybouti" target="_blank"><span class="author">Sofian Chaybouti<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hendrik%20Strobelt" target="_blank"><span class="author">Hendrik Strobelt<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hilde%20Kuehne" target="_blank"><span class="author">Hilde Kuehne<span></span></span></a>
            </p>
            <p id="summary-2404.03214" class="summary">Vision Transformers (ViTs), with their ability to model long-range dependencies through self-attention mechanisms, have become a standard architecture in computer vision. However, the interpretability of these models remains a challenge. To address this, we propose LeGrad, an explainability method specifically designed for ViTs. LeGrad computes the gradient with respect to the attention maps of ViT layers, considering the gradient itself as the explainability signal. We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map. This makes LeGrad a conceptually simple and an easy-to-implement tool for enhancing the transparency of ViTs. We evaluate LeGrad in challenging segmentation, perturbation, and open-vocabulary settings, showcasing its versatility compared to other SotA explainability methods demonstrating its superior spatial fidelity and robustness to perturbations. A demo and the code is available at https://github.com/WalBouss/LeGrad.</p>
            <div id="pdf-container-2404.03214" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03214" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(55).html"></iframe>
            </div>
            <div id="kimi-container-2404.03214" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03210" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03210" target="_blank" title="57/93"><span class="index">#57</span></a>
                <a id="title-2404.03210" class="title-link" href="https://papers.cool/arxiv/2404.03210" target="_blank">HDR Imaging for Dynamic Scenes with Events</a>
                <a id="pdf-2404.03210" class="title-pdf" onclick="togglePdf(&#39;2404.03210&#39;, &#39;https://arxiv.org/pdf/2404.03210.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03210">2</sup>]</a>
                <a id="copy-2404.03210" class="title-copy" onclick="copyToClipboard(&#39;2404.03210&#39;)">[Copy]</a>
                <a id="kimi-2404.03210" class="title-kimi" onclick="chatKimi(&#39;2404.03210&#39;, this)">[Kimi<sup id="kimi-stars-2404.03210">2</sup>]</a>
            </h2>
            <p id="authors-2404.03210" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Li%20Xiaopeng" target="_blank"><span class="author">Li Xiaopeng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zeng%20Zhaoyuan" target="_blank"><span class="author">Zeng Zhaoyuan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fan%20Cien" target="_blank"><span class="author">Fan Cien<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhao%20Chen" target="_blank"><span class="author">Zhao Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Deng%20Lei" target="_blank"><span class="author">Deng Lei<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu%20Lei" target="_blank"><span class="author">Yu Lei<span></span></span></a>
            </p>
            <p id="summary-2404.03210" class="summary">High dynamic range imaging (HDRI) for real-world dynamic scenes is challenging because moving objects may lead to hybrid degradation of low dynamic range and motion blur. Existing event-based approaches only focus on a separate task, while cascading HDRI and motion deblurring would lead to sub-optimal solutions, and unavailable ground-truth sharp HDR images aggravate the predicament. To address these challenges, we propose an Event-based HDRI framework within a Self-supervised learning paradigm, i.e., Self-EHDRI, which generalizes HDRI performance in real-world dynamic scenarios. Specifically, a self-supervised learning strategy is carried out by learning cross-domain conversions from blurry LDR images to sharp LDR images, which enables sharp HDR images to be accessible in the intermediate process even though ground-truth sharp HDR images are missing. Then, we formulate the event-based HDRI and motion deblurring model and conduct a unified network to recover the intermediate sharp HDR results, where both the high dynamic range and high temporal resolution of events are leveraged simultaneously for compensation. We construct large-scale synthetic and real-world datasets to evaluate the effectiveness of our method. Comprehensive experiments demonstrate that the proposed Self-EHDRI outperforms state-of-the-art approaches by a large margin. The codes, datasets, and results are available at https://lxp-whu.github.io/Self-EHDRI.</p>
            <div id="pdf-container-2404.03210" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03210" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(56).html"></iframe>
            </div>
            <div id="kimi-container-2404.03210" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03202" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03202" target="_blank" title="58/93"><span class="index">#58</span></a>
                <a id="title-2404.03202" class="title-link" href="https://papers.cool/arxiv/2404.03202" target="_blank">OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images</a>
                <a id="pdf-2404.03202" class="title-pdf" onclick="togglePdf(&#39;2404.03202&#39;, &#39;https://arxiv.org/pdf/2404.03202.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03202"></sup>]</a>
                <a id="copy-2404.03202" class="title-copy" onclick="copyToClipboard(&#39;2404.03202&#39;)">[Copy]</a>
                <a id="kimi-2404.03202" class="title-kimi" onclick="chatKimi(&#39;2404.03202&#39;, this)">[Kimi<sup id="kimi-stars-2404.03202">1</sup>]</a>
            </h2>
            <p id="authors-2404.03202" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Longwei%20Li" target="_blank"><span class="author">Longwei Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huajian%20Huang" target="_blank"><span class="author">Huajian Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sai-Kit%20Yeung" target="_blank"><span class="author">Sai-Kit Yeung<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hui%20Cheng" target="_blank"><span class="author">Hui Cheng<span></span></span></a>
            </p>
            <p id="summary-2404.03202" class="summary">Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published.</p>
            <div id="pdf-container-2404.03202" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03202" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(57).html"></iframe>
            </div>
            <div id="kimi-container-2404.03202" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03191" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03191" target="_blank" title="59/93"><span class="index">#59</span></a>
                <a id="title-2404.03191" class="title-link" href="https://papers.cool/arxiv/2404.03191" target="_blank">CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception Tasks</a>
                <a id="pdf-2404.03191" class="title-pdf" onclick="togglePdf(&#39;2404.03191&#39;, &#39;https://arxiv.org/pdf/2404.03191.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03191">1</sup>]</a>
                <a id="copy-2404.03191" class="title-copy" onclick="copyToClipboard(&#39;2404.03191&#39;)">[Copy]</a>
                <a id="kimi-2404.03191" class="title-kimi" onclick="chatKimi(&#39;2404.03191&#39;, this)">[Kimi<sup id="kimi-stars-2404.03191">2</sup>]</a>
            </h2>
            <p id="authors-2404.03191" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Beibei%20Wang" target="_blank"><span class="author">Beibei Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lu%20Zhang" target="_blank"><span class="author">Lu Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shuang%20Meng" target="_blank"><span class="author">Shuang Meng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chenjie%20Wang" target="_blank"><span class="author">Chenjie Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jingjing%20Huang" target="_blank"><span class="author">Jingjing Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yao%20Li" target="_blank"><span class="author">Yao Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Haojie%20Ren" target="_blank"><span class="author">Haojie Ren<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuxuan%20Xiao" target="_blank"><span class="author">Yuxuan Xiao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuru%20Peng" target="_blank"><span class="author">Yuru Peng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jianmin%20Ji" target="_blank"><span class="author">Jianmin Ji<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu%20Zhang" target="_blank"><span class="author">Yu Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yanyong%20Zhang" target="_blank"><span class="author">Yanyong Zhang<span></span></span></a>
            </p>
            <p id="summary-2404.03191" class="summary">Numerous roadside perception datasets have been introduced to propel advancements in autonomous driving and intelligent transportation systems research and development. However, it has been observed that the majority of their concentrates is on urban arterial roads, inadvertently overlooking residential areas such as parks and campuses that exhibit entirely distinct characteristics. In light of this gap, we propose CORP, which stands as the first public benchmark dataset tailored for multi-modal roadside perception tasks under campus scenarios. Collected in a university campus, CORP consists of over 205k images plus 102k point clouds captured from 18 cameras and 9 LiDAR sensors. These sensors with different configurations are mounted on roadside utility poles to provide diverse viewpoints within the campus region. The annotations of CORP encompass multi-dimensional information beyond 2D and 3D bounding boxes, providing extra support for 3D seamless tracking and instance segmentation with unique IDs and pixel masks for identifying targets, to enhance the understanding of objects and their behaviors distributed across the campus premises. Unlike other roadside datasets about urban traffic, CORP extends the spectrum to highlight the challenges for multi-modal perception in campuses and other residential areas.</p>
            <div id="pdf-container-2404.03191" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03191" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(58).html"></iframe>
            </div>
            <div id="kimi-container-2404.03191" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03190" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03190" target="_blank" title="60/93"><span class="index">#60</span></a>
                <a id="title-2404.03190" class="title-link" href="https://papers.cool/arxiv/2404.03190" target="_blank">Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation</a>
                <a id="pdf-2404.03190" class="title-pdf" onclick="togglePdf(&#39;2404.03190&#39;, &#39;https://arxiv.org/pdf/2404.03190.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03190">2</sup>]</a>
                <a id="copy-2404.03190" class="title-copy" onclick="copyToClipboard(&#39;2404.03190&#39;)">[Copy]</a>
                <a id="kimi-2404.03190" class="title-kimi" onclick="chatKimi(&#39;2404.03190&#39;, this)">[Kimi<sup id="kimi-stars-2404.03190">1</sup>]</a>
            </h2>
            <p id="authors-2404.03190" class="authors a-dynamic"><strong>Author</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jianwei%20Ren" target="_blank"><span class="author">Jianwei Ren<span></span></span></a>
            </p>
            <p id="summary-2404.03190" class="summary">In self-supervised monocular depth estimation tasks, discrete disparity prediction has been proven to attain higher quality depth maps than common continuous methods. However, current discretization strategies often divide depth ranges of scenes into bins in a handcrafted and rigid manner, limiting model performance. In this paper, we propose a learnable module, Adaptive Discrete Disparity Volume (ADDV), which is capable of dynamically sensing depth distributions in different RGB images and generating adaptive bins for them. Without any extra supervision, this module can be integrated into existing CNN architectures, allowing networks to produce representative values for bins and a probability volume over them. Furthermore, we introduce novel training strategies - uniformizing and sharpening - through a loss term and temperature parameter, respectively, to provide regularizations under self-supervised conditions, preventing model degradation or collapse. Empirical results demonstrate that ADDV effectively processes global information, generating appropriate bins for various scenes and producing higher quality depth maps compared to handcrafted methods.</p>
            <div id="pdf-container-2404.03190" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03190" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(59).html"></iframe>
            </div>
            <div id="kimi-container-2404.03190" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03187" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03187" target="_blank" title="61/93"><span class="index">#61</span></a>
                <a id="title-2404.03187" class="title-link" href="https://papers.cool/arxiv/2404.03187" target="_blank">AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales</a>
                <a id="pdf-2404.03187" class="title-pdf" onclick="togglePdf(&#39;2404.03187&#39;, &#39;https://arxiv.org/pdf/2404.03187.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03187">1</sup>]</a>
                <a id="copy-2404.03187" class="title-copy" onclick="copyToClipboard(&#39;2404.03187&#39;)">[Copy]</a>
                <a id="kimi-2404.03187" class="title-kimi" onclick="chatKimi(&#39;2404.03187&#39;, this)">[Kimi<sup id="kimi-stars-2404.03187">1</sup>]</a>
            </h2>
            <p id="authors-2404.03187" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tianrui%20Guan" target="_blank"><span class="author">Tianrui Guan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruiqi%20Xian" target="_blank"><span class="author">Ruiqi Xian<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xijun%20Wang" target="_blank"><span class="author">Xijun Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiyang%20Wu" target="_blank"><span class="author">Xiyang Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mohamed%20Elnoor" target="_blank"><span class="author">Mohamed Elnoor<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Daeun%20Song" target="_blank"><span class="author">Daeun Song<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Dinesh%20Manocha" target="_blank"><span class="author">Dinesh Manocha<span></span></span></a>
            </p>
            <p id="summary-2404.03187" class="summary">We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment. The code and dataset will be made publicly available.</p>
            <div id="pdf-container-2404.03187" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03187" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(60).html"></iframe>
            </div>
            <div id="kimi-container-2404.03187" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03183" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03183" target="_blank" title="62/93"><span class="index">#62</span></a>
                <a id="title-2404.03183" class="title-link" href="https://papers.cool/arxiv/2404.03183" target="_blank">BodyMAP - Jointly Predicting Body Mesh and 3D Applied Pressure Map for People in Bed</a>
                <a id="pdf-2404.03183" class="title-pdf" onclick="togglePdf(&#39;2404.03183&#39;, &#39;https://arxiv.org/pdf/2404.03183.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03183"></sup>]</a>
                <a id="copy-2404.03183" class="title-copy" onclick="copyToClipboard(&#39;2404.03183&#39;)">[Copy]</a>
                <a id="kimi-2404.03183" class="title-kimi" onclick="chatKimi(&#39;2404.03183&#39;, this)">[Kimi<sup id="kimi-stars-2404.03183">1</sup>]</a>
            </h2>
            <p id="authors-2404.03183" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Abhishek%20Tandon" target="_blank"><span class="author">Abhishek Tandon<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anujraaj%20Goyal" target="_blank"><span class="author">Anujraaj Goyal<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Henry%20M.%20Clever" target="_blank"><span class="author">Henry M. Clever<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zackory%20Erickson" target="_blank"><span class="author">Zackory Erickson<span></span></span></a>
            </p>
            <p id="summary-2404.03183" class="summary">Accurately predicting the 3D human posture and the pressure exerted on the body for people resting in bed, visualized as a body mesh (3D pose &amp; shape) with a 3D pressure map, holds significant promise for healthcare applications, particularly, in the prevention of pressure ulcers. Current methods focus on singular facets of the problem -- predicting only 2D/3D poses, generating 2D pressure images, predicting pressure only for certain body regions instead of the full body, or forming indirect approximations to the 3D pressure map. In contrast, we introduce BodyMAP, which jointly predicts the human body mesh and 3D applied pressure map across the entire human body. Our network leverages multiple visual modalities, incorporating both a depth image of a person in bed and its corresponding 2D pressure image acquired from a pressure-sensing mattress. The 3D pressure map is represented as a pressure value at each mesh vertex and thus allows for precise localization of high-pressure regions on the body. Additionally, we present BodyMAP-WS, a new formulation of pressure prediction in which we implicitly learn pressure in 3D by aligning sensed 2D pressure images with a differentiable 2D projection of the predicted 3D pressure maps. In evaluations with real-world human data, our method outperforms the current state-of-the-art technique by 25% on both body mesh and 3D applied pressure map prediction tasks for people in bed.</p>
            <div id="pdf-container-2404.03183" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03183" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(61).html"></iframe>
            </div>
            <div id="kimi-container-2404.03183" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03181" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03181" target="_blank" title="63/93"><span class="index">#63</span></a>
                <a id="title-2404.03181" class="title-link" href="https://papers.cool/arxiv/2404.03181" target="_blank">MonoCD: Monocular 3D Object Detection with Complementary Depths</a>
                <a id="pdf-2404.03181" class="title-pdf" onclick="togglePdf(&#39;2404.03181&#39;, &#39;https://arxiv.org/pdf/2404.03181.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03181"></sup>]</a>
                <a id="copy-2404.03181" class="title-copy" onclick="copyToClipboard(&#39;2404.03181&#39;)">[Copy]</a>
                <a id="kimi-2404.03181" class="title-kimi" onclick="chatKimi(&#39;2404.03181&#39;, this)">[Kimi<sup id="kimi-stars-2404.03181">2</sup>]</a>
            </h2>
            <p id="authors-2404.03181" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Longfei%20Yan" target="_blank"><span class="author">Longfei Yan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Pei%20Yan" target="_blank"><span class="author">Pei Yan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shengzhou%20Xiong" target="_blank"><span class="author">Shengzhou Xiong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xuanyu%20Xiang" target="_blank"><span class="author">Xuanyu Xiang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yihua%20Tan" target="_blank"><span class="author">Yihua Tan<span></span></span></a>
            </p>
            <p id="summary-2404.03181" class="summary">Monocular 3D object detection has attracted widespread attention due to its potential to accurately obtain object 3D localization from a single image at a low cost. Depth estimation is an essential but challenging subtask of monocular 3D object detection due to the ill-posedness of 2D to 3D mapping. Many methods explore multiple local depth clues such as object heights and keypoints and then formulate the object depth estimation as an ensemble of multiple depth predictions to mitigate the insufficiency of single-depth information. However, the errors of existing multiple depths tend to have the same sign, which hinders them from neutralizing each other and limits the overall accuracy of combined depth. To alleviate this problem, we propose to increase the complementarity of depths with two novel designs. First, we add a new depth prediction branch named complementary depth that utilizes global and efficient depth clues from the entire image rather than the local clues to reduce the correlation of depth predictions. Second, we propose to fully exploit the geometric relations between multiple depth clues to achieve complementarity in form. Benefiting from these designs, our method achieves higher complementarity. Experiments on the KITTI benchmark demonstrate that our method achieves state-of-the-art performance without introducing extra data. In addition, complementary depth can also be a lightweight and plug-and-play module to boost multiple existing monocular 3d object detectors. Code is available at https://github.com/elvintanhust/MonoCD.</p>
            <div id="pdf-container-2404.03181" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03181" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(62).html"></iframe>
            </div>
            <div id="kimi-container-2404.03181" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03179" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03179" target="_blank" title="64/93"><span class="index">#64</span></a>
                <a id="title-2404.03179" class="title-link" href="https://papers.cool/arxiv/2404.03179" target="_blank">UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization</a>
                <a id="pdf-2404.03179" class="title-pdf" onclick="togglePdf(&#39;2404.03179&#39;, &#39;https://arxiv.org/pdf/2404.03179.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03179"></sup>]</a>
                <a id="copy-2404.03179" class="title-copy" onclick="copyToClipboard(&#39;2404.03179&#39;)">[Copy]</a>
                <a id="kimi-2404.03179" class="title-kimi" onclick="chatKimi(&#39;2404.03179&#39;, this)">[Kimi<sup id="kimi-stars-2404.03179">1</sup>]</a>
            </h2>
            <p id="authors-2404.03179" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tiantian%20Geng" target="_blank"><span class="author">Tiantian Geng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Teng%20Wang" target="_blank"><span class="author">Teng Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yanfu%20Zhang" target="_blank"><span class="author">Yanfu Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinming%20Duan" target="_blank"><span class="author">Jinming Duan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Weili%20Guan" target="_blank"><span class="author">Weili Guan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Feng%20Zheng" target="_blank"><span class="author">Feng Zheng<span></span></span></a>
            </p>
            <p id="summary-2404.03179" class="summary">Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content. In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage diverse data available in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities. To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task. Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing prompts during inference. UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks.</p>
            <div id="pdf-container-2404.03179" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03179" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(63).html"></iframe>
            </div>
            <div id="kimi-container-2404.03179" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03161" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03161" target="_blank" title="65/93"><span class="index">#65</span></a>
                <a id="title-2404.03161" class="title-link" href="https://papers.cool/arxiv/2404.03161" target="_blank">BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro QR Codes</a>
                <a id="pdf-2404.03161" class="title-pdf" onclick="togglePdf(&#39;2404.03161&#39;, &#39;https://arxiv.org/pdf/2404.03161.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03161"></sup>]</a>
                <a id="copy-2404.03161" class="title-copy" onclick="copyToClipboard(&#39;2404.03161&#39;)">[Copy]</a>
                <a id="kimi-2404.03161" class="title-kimi" onclick="chatKimi(&#39;2404.03161&#39;, this)">[Kimi<sup id="kimi-stars-2404.03161">1</sup>]</a>
            </h2>
            <p id="authors-2404.03161" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Taichi%20Nishimura" target="_blank"><span class="author">Taichi Nishimura<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Koki%20Yamamoto" target="_blank"><span class="author">Koki Yamamoto<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuto%20Haneji" target="_blank"><span class="author">Yuto Haneji<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Keiya%20Kajimura" target="_blank"><span class="author">Keiya Kajimura<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chihiro%20Nishiwaki" target="_blank"><span class="author">Chihiro Nishiwaki<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Eriko%20Daikoku" target="_blank"><span class="author">Eriko Daikoku<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Natsuko%20Okuda" target="_blank"><span class="author">Natsuko Okuda<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fumihito%20Ono" target="_blank"><span class="author">Fumihito Ono<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hirotaka%20Kameko" target="_blank"><span class="author">Hirotaka Kameko<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shinsuke%20Mori" target="_blank"><span class="author">Shinsuke Mori<span></span></span></a>
            </p>
            <p id="summary-2404.03161" class="summary">This paper introduces a biochemical vision-and-language dataset, which consists of 24 egocentric experiment videos, corresponding protocols, and video-and-language alignments. The key challenge in the wet-lab domain is detecting equipment, reagents, and containers is difficult because the lab environment is scattered by filling objects on the table and some objects are indistinguishable. Therefore, previous studies assume that objects are manually annotated and given for downstream tasks, but this is costly and time-consuming. To address this issue, this study focuses on Micro QR Codes to detect objects automatically. From our preliminary study, we found that detecting objects only using Micro QR Codes is still difficult because the researchers manipulate objects, causing blur and occlusion frequently. To address this, we also propose a novel object labeling method by combining a Micro QR Code detector and an off-the-shelf hand object detector. As one of the applications of our dataset, we conduct the task of generating protocols from experiment videos and find that our approach can generate accurate protocols.</p>
            <div id="pdf-container-2404.03161" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03161" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(64).html"></iframe>
            </div>
            <div id="kimi-container-2404.03161" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03159" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03159" target="_blank" title="66/93"><span class="index">#66</span></a>
                <a id="title-2404.03159" class="title-link" href="https://papers.cool/arxiv/2404.03159" target="_blank">HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</a>
                <a id="pdf-2404.03159" class="title-pdf" onclick="togglePdf(&#39;2404.03159&#39;, &#39;https://arxiv.org/pdf/2404.03159.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03159"></sup>]</a>
                <a id="copy-2404.03159" class="title-copy" onclick="copyToClipboard(&#39;2404.03159&#39;)">[Copy]</a>
                <a id="kimi-2404.03159" class="title-kimi" onclick="chatKimi(&#39;2404.03159&#39;, this)">[Kimi<sup id="kimi-stars-2404.03159">1</sup>]</a>
            </h2>
            <p id="authors-2404.03159" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wencan%20Cheng" target="_blank"><span class="author">Wencan Cheng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hao%20Tang" target="_blank"><span class="author">Hao Tang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Luc%20Van%20Gool" target="_blank"><span class="author">Luc Van Gool<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jong%20Hwan%20Ko" target="_blank"><span class="author">Jong Hwan Ko<span></span></span></a>
            </p>
            <p id="summary-2404.03159" class="summary">Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDiff.</p>
            <div id="pdf-container-2404.03159" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03159" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(65).html"></iframe>
            </div>
            <div id="kimi-container-2404.03159" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03145" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03145" target="_blank" title="67/93"><span class="index">#67</span></a>
                <a id="title-2404.03145" class="title-link" href="https://papers.cool/arxiv/2404.03145" target="_blank">DreamWalk: Style Space Exploration using Diffusion Guidance</a>
                <a id="pdf-2404.03145" class="title-pdf" onclick="togglePdf(&#39;2404.03145&#39;, &#39;https://arxiv.org/pdf/2404.03145.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03145">2</sup>]</a>
                <a id="copy-2404.03145" class="title-copy" onclick="copyToClipboard(&#39;2404.03145&#39;)">[Copy]</a>
                <a id="kimi-2404.03145" class="title-kimi" onclick="chatKimi(&#39;2404.03145&#39;, this)">[Kimi<sup id="kimi-stars-2404.03145">4</sup>]</a>
            </h2>
            <p id="authors-2404.03145" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michelle%20Shu" target="_blank"><span class="author">Michelle Shu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Charles%20Herrmann" target="_blank"><span class="author">Charles Herrmann<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Richard%20Strong%20Bowen" target="_blank"><span class="author">Richard Strong Bowen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Forrester%20Cole" target="_blank"><span class="author">Forrester Cole<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ramin%20Zabih" target="_blank"><span class="author">Ramin Zabih<span></span></span></a>
            </p>
            <p id="summary-2404.03145" class="summary">Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform "prompt engineering," constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model's neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: https://mshu1.github.io/dreamwalk.github.io/</p>
            <div id="pdf-container-2404.03145" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03145" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(66).html"></iframe>
            </div>
            <div id="kimi-container-2404.03145" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03144" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03144" target="_blank" title="68/93"><span class="index">#68</span></a>
                <a id="title-2404.03144" class="title-link" href="https://papers.cool/arxiv/2404.03144" target="_blank">Diverse and Tailored Image Generation for Zero-shot Multi-label Classification</a>
                <a id="pdf-2404.03144" class="title-pdf" onclick="togglePdf(&#39;2404.03144&#39;, &#39;https://arxiv.org/pdf/2404.03144.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03144">1</sup>]</a>
                <a id="copy-2404.03144" class="title-copy" onclick="copyToClipboard(&#39;2404.03144&#39;)">[Copy]</a>
                <a id="kimi-2404.03144" class="title-kimi" onclick="chatKimi(&#39;2404.03144&#39;, this)">[Kimi<sup id="kimi-stars-2404.03144">1</sup>]</a>
            </h2>
            <p id="authors-2404.03144" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kaixin%20Zhang" target="_blank"><span class="author">Kaixin Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhixiang%20Yuan" target="_blank"><span class="author">Zhixiang Yuan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tao%20Huang" target="_blank"><span class="author">Tao Huang<span></span></span></a>
            </p>
            <p id="summary-2404.03144" class="summary">Recently, zero-shot multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of text-to-image generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained large language model to generate diverse prompts. Employing a pre-trained multi-modal CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text prompts for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to fine-tune the text encoder in the diffusion model. Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from fine-tuning the entire visual encoder, we propose a feature fusion module inspired by transformer attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods.</p>
            <div id="pdf-container-2404.03144" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03144" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(67).html"></iframe>
            </div>
            <div id="kimi-container-2404.03144" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03138" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03138" target="_blank" title="69/93"><span class="index">#69</span></a>
                <a id="title-2404.03138" class="title-link" href="https://papers.cool/arxiv/2404.03138" target="_blank">Discontinuity-preserving Normal Integration with Auxiliary Edges</a>
                <a id="pdf-2404.03138" class="title-pdf" onclick="togglePdf(&#39;2404.03138&#39;, &#39;https://arxiv.org/pdf/2404.03138.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03138"></sup>]</a>
                <a id="copy-2404.03138" class="title-copy" onclick="copyToClipboard(&#39;2404.03138&#39;)">[Copy]</a>
                <a id="kimi-2404.03138" class="title-kimi" onclick="chatKimi(&#39;2404.03138&#39;, this)">[Kimi<sup id="kimi-stars-2404.03138"></sup>]</a>
            </h2>
            <p id="authors-2404.03138" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hyomin%20Kim" target="_blank"><span class="author">Hyomin Kim<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yucheol%20Jung" target="_blank"><span class="author">Yucheol Jung<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Seungyong%20Lee" target="_blank"><span class="author">Seungyong Lee<span></span></span></a>
            </p>
            <p id="summary-2404.03138" class="summary">Many surface reconstruction methods incorporate normal integration, which is a process to obtain a depth map from surface gradients. In this process, the input may represent a surface with discontinuities, e.g., due to self-occlusion. To reconstruct an accurate depth map from the input normal map, hidden surface gradients occurring from the jumps must be handled. To model these jumps correctly, we design a novel discretization scheme for the domain of normal integration. Our key idea is to introduce auxiliary edges, which bridge between piecewise-smooth patches in the domain so that the magnitude of hidden jumps can be explicitly expressed. Using the auxiliary edges, we design a novel algorithm to optimize the discontinuity and the depth map from the input normal map. Our method optimizes discontinuities by using a combination of iterative re-weighted least squares and iterative filtering of the jump magnitudes on auxiliary edges to provide strong sparsity regularization. Compared to previous discontinuity-preserving normal integration methods, which model the magnitudes of jumps only implicitly, our method reconstructs subtle discontinuities accurately thanks to our explicit representation of jumps allowing for strong sparsity regularization.</p>
            <div id="pdf-container-2404.03138" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03138" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(68).html"></iframe>
            </div>
            <div id="kimi-container-2404.03138" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03121" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03121" target="_blank" title="70/93"><span class="index">#70</span></a>
                <a id="title-2404.03121" class="title-link" href="https://papers.cool/arxiv/2404.03121" target="_blank">Utilizing Computer Vision for Continuous Monitoring of Vaccine Side Effects in Experimental Mice</a>
                <a id="pdf-2404.03121" class="title-pdf" onclick="togglePdf(&#39;2404.03121&#39;, &#39;https://arxiv.org/pdf/2404.03121.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03121"></sup>]</a>
                <a id="copy-2404.03121" class="title-copy" onclick="copyToClipboard(&#39;2404.03121&#39;)">[Copy]</a>
                <a id="kimi-2404.03121" class="title-kimi" onclick="chatKimi(&#39;2404.03121&#39;, this)">[Kimi<sup id="kimi-stars-2404.03121"></sup>]</a>
            </h2>
            <p id="authors-2404.03121" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chuang%20Li" target="_blank"><span class="author">Chuang Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shuai%20Shao" target="_blank"><span class="author">Shuai Shao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Willian%20Mikason" target="_blank"><span class="author">Willian Mikason<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Rubing%20Lin" target="_blank"><span class="author">Rubing Lin<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yantong%20Liu" target="_blank"><span class="author">Yantong Liu<span></span></span></a>
            </p>
            <p id="summary-2404.03121" class="summary">The demand for improved efficiency and accuracy in vaccine safety assessments is increasing. Here, we explore the application of computer vision technologies to automate the monitoring of experimental mice for potential side effects after vaccine administration. Traditional observation methods are labor-intensive and lack the capability for continuous monitoring. By deploying a computer vision system, our research aims to improve the efficiency and accuracy of vaccine safety assessments. The methodology involves training machine learning models on annotated video data of mice behaviors pre- and post-vaccination. Preliminary results indicate that computer vision effectively identify subtle changes, signaling possible side effects. Therefore, our approach has the potential to significantly enhance the monitoring process in vaccine trials in animals, providing a practical solution to the limitations of human observation.</p>
            <div id="pdf-container-2404.03121" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03121" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(69).html"></iframe>
            </div>
            <div id="kimi-container-2404.03121" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03118" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03118" target="_blank" title="71/93"><span class="index">#71</span></a>
                <a id="title-2404.03118" class="title-link" href="https://papers.cool/arxiv/2404.03118" target="_blank">LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models</a>
                <a id="pdf-2404.03118" class="title-pdf" onclick="togglePdf(&#39;2404.03118&#39;, &#39;https://arxiv.org/pdf/2404.03118.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03118">2</sup>]</a>
                <a id="copy-2404.03118" class="title-copy" onclick="copyToClipboard(&#39;2404.03118&#39;)">[Copy]</a>
                <a id="kimi-2404.03118" class="title-kimi" onclick="chatKimi(&#39;2404.03118&#39;, this)">[Kimi<sup id="kimi-stars-2404.03118">5</sup>]</a>
            </h2>
            <p id="authors-2404.03118" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Gabriela%20Ben%20Melech%20Stan" target="_blank"><span class="author">Gabriela Ben Melech Stan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Raanan%20Yehezkel%20Rohekar" target="_blank"><span class="author">Raanan Yehezkel Rohekar<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yaniv%20Gurwicz" target="_blank"><span class="author">Yaniv Gurwicz<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Matthew%20Lyle%20Olson" target="_blank"><span class="author">Matthew Lyle Olson<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anahita%20Bhiwandiwalla" target="_blank"><span class="author">Anahita Bhiwandiwalla<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Estelle%20Aflalo" target="_blank"><span class="author">Estelle Aflalo<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chenfei%20Wu" target="_blank"><span class="author">Chenfei Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nan%20Duan" target="_blank"><span class="author">Nan Duan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shao-Yen%20Tseng" target="_blank"><span class="author">Shao-Yen Tseng<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Vasudev%20Lal" target="_blank"><span class="author">Vasudev Lal<span></span></span></a>
            </p>
            <p id="summary-2404.03118" class="summary">In the rapidly evolving landscape of artificial intelligence, multi-modal large language models are emerging as a significant area of interest. These models, which combine various forms of data input, are becoming increasingly popular. However, understanding their internal mechanisms remains a complex task. Numerous advancements have been made in the field of explainability tools and mechanisms, yet there is still much to explore. In this work, we present a novel interactive application aimed towards understanding the internal mechanisms of large vision-language models. Our interface is designed to enhance the interpretability of the image patches, which are instrumental in generating an answer, and assess the efficacy of the language model in grounding its output in the image. With our application, a user can systematically investigate the model and uncover system limitations, paving the way for enhancements in system capabilities. Finally, we present a case study of how our application can aid in understanding failure mechanisms in a popular large multi-modal model: LLaVA.</p>
            <div id="pdf-container-2404.03118" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03118" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(70).html"></iframe>
            </div>
            <div id="kimi-container-2404.03118" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03110" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03110" target="_blank" title="72/93"><span class="index">#72</span></a>
                <a id="title-2404.03110" class="title-link" href="https://papers.cool/arxiv/2404.03110" target="_blank">Ego-Motion Aware Target Prediction Module for Robust Multi-Object Tracking</a>
                <a id="pdf-2404.03110" class="title-pdf" onclick="togglePdf(&#39;2404.03110&#39;, &#39;https://arxiv.org/pdf/2404.03110.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03110">1</sup>]</a>
                <a id="copy-2404.03110" class="title-copy" onclick="copyToClipboard(&#39;2404.03110&#39;)">[Copy]</a>
                <a id="kimi-2404.03110" class="title-kimi" onclick="chatKimi(&#39;2404.03110&#39;, this)">[Kimi<sup id="kimi-stars-2404.03110">2</sup>]</a>
            </h2>
            <p id="authors-2404.03110" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Navid%20Mahdian" target="_blank"><span class="author">Navid Mahdian<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mohammad%20Jani" target="_blank"><span class="author">Mohammad Jani<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Amir%20M.%20Soufi%20Enayati" target="_blank"><span class="author">Amir M. Soufi Enayati<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Homayoun%20Najjaran" target="_blank"><span class="author">Homayoun Najjaran<span></span></span></a>
            </p>
            <p id="summary-2404.03110" class="summary">Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories. Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target. Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model. These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections. Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories. In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models. Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter. This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model. We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively. At the same time, it elevates other performance metrics such as HOTA by more than 5%. Our source code is available at https://github.com/noyzzz/EMAP.</p>
            <div id="pdf-container-2404.03110" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03110" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(71).html"></iframe>
            </div>
            <div id="kimi-container-2404.03110" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03109" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03109" target="_blank" title="73/93"><span class="index">#73</span></a>
                <a id="title-2404.03109" class="title-link" href="https://papers.cool/arxiv/2404.03109" target="_blank">Many-to-many Image Generation with Auto-regressive Diffusion Models</a>
                <a id="pdf-2404.03109" class="title-pdf" onclick="togglePdf(&#39;2404.03109&#39;, &#39;https://arxiv.org/pdf/2404.03109.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03109">4</sup>]</a>
                <a id="copy-2404.03109" class="title-copy" onclick="copyToClipboard(&#39;2404.03109&#39;)">[Copy]</a>
                <a id="kimi-2404.03109" class="title-kimi" onclick="chatKimi(&#39;2404.03109&#39;, this)">[Kimi<sup id="kimi-stars-2404.03109">1</sup>]</a>
            </h2>
            <p id="authors-2404.03109" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ying%20Shen" target="_blank"><span class="author">Ying Shen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yizhe%20Zhang" target="_blank"><span class="author">Yizhe Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shuangfei%20Zhai" target="_blank"><span class="author">Shuangfei Zhai<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lifu%20Huang" target="_blank"><span class="author">Lifu Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Joshua%20M.%20Susskind" target="_blank"><span class="author">Joshua M. Susskind<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jiatao%20Gu" target="_blank"><span class="author">Jiatao Gu<span></span></span></a>
            </p>
            <p id="summary-2404.03109" class="summary">Recent advancements in image generation have made significant progress, yet existing models present limitations in perceiving and generating an arbitrary number of interrelated images within a broad context. This limitation becomes increasingly critical as the demand for multi-image scenarios, such as multi-view images and visual narratives, grows with the expansion of multimedia platforms. This paper introduces a domain-general framework for many-to-many image generation, capable of producing interrelated image series from a given set of images, offering a scalable solution that obviates the need for task-specific solutions across different multi-image scenarios. To facilitate this, we present MIS, a novel large-scale multi-image dataset, containing 12M synthetic multi-image samples, each with 25 interconnected images. Utilizing Stable Diffusion with varied latent noises, our method produces a set of interconnected images from a single caption. Leveraging MIS, we learn M2M, an autoregressive model for many-to-many generation, where each image is modeled within a diffusion framework. Throughout training on the synthetic MIS, the model excels in capturing style and content from preceding images - synthetic or real - and generates novel images following the captured patterns. Furthermore, through task-specific fine-tuning, our model demonstrates its adaptability to various multi-image generation tasks, including Novel View Synthesis and Visual Procedure Generation.</p>
            <div id="pdf-container-2404.03109" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03109" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(72).html"></iframe>
            </div>
            <div id="kimi-container-2404.03109" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03097" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03097" target="_blank" title="74/93"><span class="index">#74</span></a>
                <a id="title-2404.03097" class="title-link" href="https://papers.cool/arxiv/2404.03097" target="_blank">SalFoM: Dynamic Saliency Prediction with Video Foundation Models</a>
                <a id="pdf-2404.03097" class="title-pdf" onclick="togglePdf(&#39;2404.03097&#39;, &#39;https://arxiv.org/pdf/2404.03097.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03097">2</sup>]</a>
                <a id="copy-2404.03097" class="title-copy" onclick="copyToClipboard(&#39;2404.03097&#39;)">[Copy]</a>
                <a id="kimi-2404.03097" class="title-kimi" onclick="chatKimi(&#39;2404.03097&#39;, this)">[Kimi<sup id="kimi-stars-2404.03097">1</sup>]</a>
            </h2>
            <p id="authors-2404.03097" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Morteza%20Moradi" target="_blank"><span class="author">Morteza Moradi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mohammad%20Moradi" target="_blank"><span class="author">Mohammad Moradi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Francesco%20Rundo" target="_blank"><span class="author">Francesco Rundo<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Concetto%20Spampinato" target="_blank"><span class="author">Concetto Spampinato<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ali%20Borji" target="_blank"><span class="author">Ali Borji<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Simone%20Palazzo" target="_blank"><span class="author">Simone Palazzo<span></span></span></a>
            </p>
            <p id="summary-2404.03097" class="summary">Recent advancements in video saliency prediction (VSP) have shown promising performance compared to the human visual system, whose emulation is the primary goal of VSP. However, current state-of-the-art models employ spatio-temporal transformers trained on limited amounts of data, hindering generalizability adaptation to downstream tasks. The benefits of vision foundation models present a potential solution to improve the VSP process. However, adapting image foundation models to the video domain presents significant challenges in modeling scene dynamics and capturing temporal information. To address these challenges, and as the first initiative to design a VSP model based on video foundation models, we introduce SalFoM, a novel encoder-decoder video transformer architecture. Our model employs UnMasked Teacher (UMT) as feature extractor and presents a heterogeneous decoder which features a locality-aware spatio-temporal transformer and integrates local and global spatio-temporal information from various perspectives to produce the final saliency map. Our qualitative and quantitative experiments on the challenging VSP benchmark datasets of DHF1K, Hollywood-2 and UCF-Sports demonstrate the superiority of our proposed model in comparison with the state-of-the-art methods.</p>
            <div id="pdf-container-2404.03097" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03097" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(73).html"></iframe>
            </div>
            <div id="kimi-container-2404.03097" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03070" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03070" target="_blank" title="75/93"><span class="index">#75</span></a>
                <a id="title-2404.03070" class="title-link" href="https://papers.cool/arxiv/2404.03070" target="_blank">Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion</a>
                <a id="pdf-2404.03070" class="title-pdf" onclick="togglePdf(&#39;2404.03070&#39;, &#39;https://arxiv.org/pdf/2404.03070.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03070"></sup>]</a>
                <a id="copy-2404.03070" class="title-copy" onclick="copyToClipboard(&#39;2404.03070&#39;)">[Copy]</a>
                <a id="kimi-2404.03070" class="title-kimi" onclick="chatKimi(&#39;2404.03070&#39;, this)">[Kimi<sup id="kimi-stars-2404.03070"></sup>]</a>
            </h2>
            <p id="authors-2404.03070" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Su%20Sun" target="_blank"><span class="author">Su Sun<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Cheng%20Zhao" target="_blank"><span class="author">Cheng Zhao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuliang%20Guo" target="_blank"><span class="author">Yuliang Guo<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruoyu%20Wang" target="_blank"><span class="author">Ruoyu Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xinyu%20Huang" target="_blank"><span class="author">Xinyu Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yingjie%20Victor%20Chen" target="_blank"><span class="author">Yingjie Victor Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liu%20Ren" target="_blank"><span class="author">Liu Ren<span></span></span></a>
            </p>
            <p id="summary-2404.03070" class="summary">In this paper, we present a novel indoor 3D reconstruction method with occluded surface completion, given a sequence of depth readings. Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene, neglecting the invisible areas due to the occlusions, e.g., the contact surface between furniture, occluded wall and floor. Our method tackles the task of completing the occluded scene surfaces, resulting in a complete 3D scene mesh. The core idea of our method is learning 3D geometry prior from various complete scenes to infer the occluded geometry of an unseen scene from solely depth measurements. We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture, i.e., Geo-decoder and 3D Inpainter, which jointly reconstructs the complete 3D scene geometry. The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces. The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces. As a result, while the Geo-decoder is specialized for an individual scene, the 3D Inpainter can be generally applied across different scenes. We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets, significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction. 3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage.</p>
            <div id="pdf-container-2404.03070" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03070" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(74).html"></iframe>
            </div>
            <div id="kimi-container-2404.03070" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03043" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03043" target="_blank" title="76/93"><span class="index">#76</span></a>
                <a id="title-2404.03043" class="title-link" href="https://papers.cool/arxiv/2404.03043" target="_blank">Linear Anchored Gaussian Mixture Model for Location and Width Computation of Objects in Thick Line Shape</a>
                <a id="pdf-2404.03043" class="title-pdf" onclick="togglePdf(&#39;2404.03043&#39;, &#39;https://arxiv.org/pdf/2404.03043.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03043">1</sup>]</a>
                <a id="copy-2404.03043" class="title-copy" onclick="copyToClipboard(&#39;2404.03043&#39;)">[Copy]</a>
                <a id="kimi-2404.03043" class="title-kimi" onclick="chatKimi(&#39;2404.03043&#39;, this)">[Kimi<sup id="kimi-stars-2404.03043"></sup>]</a>
            </h2>
            <p id="authors-2404.03043" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nafaa%20Nacereddine" target="_blank"><span class="author">Nafaa Nacereddine<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Djemel%20Ziou" target="_blank"><span class="author">Djemel Ziou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aicha%20Baya%20Goumeidane" target="_blank"><span class="author">Aicha Baya Goumeidane<span></span></span></a>
            </p>
            <p id="summary-2404.03043" class="summary">An accurate detection of the centerlines of linear objects is a challenging topic in many sensitive real-world applications such X-ray imaging, remote sensing and lane marking detection in road traffic. Model-based approaches using Hough and Radon transforms are often used but, are not recommended for thick line detection, whereas approaches based on image derivatives need further step-by-step processing, making their efficiency dependent on each step outcomes. In this paper, we aim to detect linear structures found in images by considering the 3D representation of the image gray levels as a finite mixture model of statistical distribution. The latter, which we named linear anchored Gaussian distribution could be parametrized by a scale value {\sigma} describing the linear structure thickness and a line equation, parametrized, in turn, by a radius \r{ho} and an orientation angle {\theta}, describing the linear structure centerline location. Expectation-Maximization (EM) algorithm is used for the mixture model parameter estimation, where a new paradigm, using the background subtraction for the likelihood function computation, is proposed. For the EM algorithm, two {\theta} parameter initialization schemes are used: the first one is based on a random choice of the first component of {\theta} vector, whereas the second is based on the image Hessian with a simultaneous computation of the mixture model components number. Experiments on real world images and synthetic images corrupted by blur and additive noise show the good performance of the proposed methods, where the algorithm using background subtraction and Hessian-based {\theta} initialization provides an outstanding accuracy of the linear structure detection despite irregular image background and presence of blur and noise.</p>
            <div id="pdf-container-2404.03043" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03043" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(75).html"></iframe>
            </div>
            <div id="kimi-container-2404.03043" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03042" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03042" target="_blank" title="77/93"><span class="index">#77</span></a>
                <a id="title-2404.03042" class="title-link" href="https://papers.cool/arxiv/2404.03042" target="_blank">AWOL: Analysis WithOut synthesis using Language</a>
                <a id="pdf-2404.03042" class="title-pdf" onclick="togglePdf(&#39;2404.03042&#39;, &#39;https://arxiv.org/pdf/2404.03042.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03042">1</sup>]</a>
                <a id="copy-2404.03042" class="title-copy" onclick="copyToClipboard(&#39;2404.03042&#39;)">[Copy]</a>
                <a id="kimi-2404.03042" class="title-kimi" onclick="chatKimi(&#39;2404.03042&#39;, this)">[Kimi<sup id="kimi-stars-2404.03042">1</sup>]</a>
            </h2>
            <p id="authors-2404.03042" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Silvia%20Zuffi" target="_blank"><span class="author">Silvia Zuffi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Michael%20J.%20Black" target="_blank"><span class="author">Michael J. Black<span></span></span></a>
            </p>
            <p id="summary-2404.03042" class="summary">Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters. For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model. Our key idea is to leverage language to control such existing models to produce novel shapes. This involves learning a mapping between the latent space of a vision-language model and the parameter space of the 3D model, which we do using a small set of shape and text pairs. Our hypothesis is that mapping from language to parameters allows us to generate parameters for objects that were never seen during training. If the mapping between language and parameters is sufficiently smooth, then interpolation or generalization in language should translate appropriately into novel 3D shapes. We test our approach with two very different types of parametric shape models (quadrupeds and arboreal trees). We use a learned statistical shape model of quadrupeds and show that we can use text to generate new animals not present during training. In particular, we demonstrate state-of-the-art shape estimation of 3D dogs. This work also constitutes the first language-driven method for generating 3D trees. Finally, embedding images in the CLIP latent space enables us to generate animals and trees directly from images.</p>
            <div id="pdf-container-2404.03042" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03042" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(76).html"></iframe>
            </div>
            <div id="kimi-container-2404.03042" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03015" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03015" target="_blank" title="78/93"><span class="index">#78</span></a>
                <a id="title-2404.03015" class="title-link" href="https://papers.cool/arxiv/2404.03015" target="_blank">DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection</a>
                <a id="pdf-2404.03015" class="title-pdf" onclick="togglePdf(&#39;2404.03015&#39;, &#39;https://arxiv.org/pdf/2404.03015.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03015"></sup>]</a>
                <a id="copy-2404.03015" class="title-copy" onclick="copyToClipboard(&#39;2404.03015&#39;)">[Copy]</a>
                <a id="kimi-2404.03015" class="title-kimi" onclick="chatKimi(&#39;2404.03015&#39;, this)">[Kimi<sup id="kimi-stars-2404.03015"></sup>]</a>
            </h2>
            <p id="authors-2404.03015" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Felix%20Fent" target="_blank"><span class="author">Felix Fent<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andras%20Palffy" target="_blank"><span class="author">Andras Palffy<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Holger%20Caesar" target="_blank"><span class="author">Holger Caesar<span></span></span></a>
            </p>
            <p id="summary-2404.03015" class="summary">The perception of autonomous vehicles has to be efficient, robust, and cost-effective. However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others. Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information. We propose a novel camera-radar fusion approach called Dual Perspective Fusion Transformer (DPFT), designed to overcome these limitations. Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data. As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time. The code is made available as open-source software under https://github.com/TUMFTM/DPFT.</p>
            <div id="pdf-container-2404.03015" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03015" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(77).html"></iframe>
            </div>
            <div id="kimi-container-2404.03015" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.02990" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.02990" target="_blank" title="79/93"><span class="index">#79</span></a>
                <a id="title-2404.02990" class="title-link" href="https://papers.cool/arxiv/2404.02990" target="_blank">ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale</a>
                <a id="pdf-2404.02990" class="title-pdf" onclick="togglePdf(&#39;2404.02990&#39;, &#39;https://arxiv.org/pdf/2404.02990.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.02990">2</sup>]</a>
                <a id="copy-2404.02990" class="title-copy" onclick="copyToClipboard(&#39;2404.02990&#39;)">[Copy]</a>
                <a id="kimi-2404.02990" class="title-kimi" onclick="chatKimi(&#39;2404.02990&#39;, this)">[Kimi<sup id="kimi-stars-2404.02990">4</sup>]</a>
            </h2>
            <p id="authors-2404.02990" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jinbin%20Huang" target="_blank"><span class="author">Jinbin Huang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chen%20Chen" target="_blank"><span class="author">Chen Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Aditi%20Mishra" target="_blank"><span class="author">Aditi Mishra<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Bum%20Chul%20Kwon" target="_blank"><span class="author">Bum Chul Kwon<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhicheng%20Liu" target="_blank"><span class="author">Zhicheng Liu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chris%20Bryan" target="_blank"><span class="author">Chris Bryan<span></span></span></a>
            </p>
            <p id="summary-2404.02990" class="summary">Generative image models have emerged as a promising technology to produce realistic images. Despite potential benefits, concerns grow about its misuse, particularly in generating deceptive images that could raise significant ethical, legal, and societal issues. Consequently, there is growing demand to empower users to effectively discern and comprehend patterns of AI-generated images. To this end, we developed ASAP, an interactive visualization system that automatically extracts distinct patterns of AI-generated images and allows users to interactively explore them via various views. To uncover fake patterns, ASAP introduces a novel image encoder, adapted from CLIP, which transforms images into compact "distilled" representations, enriched with information for differentiating authentic and fake images. These representations generate gradients that propagate back to the attention maps of CLIP's transformer block. This process quantifies the relative importance of each pixel to image authenticity or fakeness, exposing key deceptive patterns. ASAP enables the at scale interactive analysis of these patterns through multiple, coordinated visualizations. This includes a representation overview with innovative cell glyphs to aid in the exploration and qualitative evaluation of fake patterns across a vast array of images, as well as a pattern view that displays authenticity-indicating patterns in images and quantifies their impact. ASAP supports the analysis of cutting-edge generative models with the latest architectures, including GAN-based models like proGAN and diffusion models like the latent diffusion model. We demonstrate ASAP's usefulness through two usage scenarios using multiple fake image detection benchmark datasets, revealing its ability to identify and understand hidden patterns in AI-generated images, especially in detecting fake human faces produced by diffusion-based techniques.</p>
            <div id="pdf-container-2404.02990" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.02990" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(78).html"></iframe>
            </div>
            <div id="kimi-container-2404.02990" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.02973" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.02973" target="_blank" title="80/93"><span class="index">#80</span></a>
                <a id="title-2404.02973" class="title-link" href="https://papers.cool/arxiv/2404.02973" target="_blank">Scaling Laws for Galaxy Images</a>
                <a id="pdf-2404.02973" class="title-pdf" onclick="togglePdf(&#39;2404.02973&#39;, &#39;https://arxiv.org/pdf/2404.02973.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.02973">1</sup>]</a>
                <a id="copy-2404.02973" class="title-copy" onclick="copyToClipboard(&#39;2404.02973&#39;)">[Copy]</a>
                <a id="kimi-2404.02973" class="title-kimi" onclick="chatKimi(&#39;2404.02973&#39;, this)">[Kimi<sup id="kimi-stars-2404.02973">2</sup>]</a>
            </h2>
            <p id="authors-2404.02973" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mike%20Walmsley" target="_blank"><span class="author">Mike Walmsley<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Micah%20Bowles" target="_blank"><span class="author">Micah Bowles<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Anna%20M.%20M.%20Scaife" target="_blank"><span class="author">Anna M. M. Scaife<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jason%20Shingirai%20Makechemu" target="_blank"><span class="author">Jason Shingirai Makechemu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Alexander%20J.%20Gordon" target="_blank"><span class="author">Alexander J. Gordon<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Annette%20M.%20N.%20Ferguson" target="_blank"><span class="author">Annette M. N. Ferguson<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Robert%20G.%20Mann" target="_blank"><span class="author">Robert G. Mann<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=James%20Pearson" target="_blank"><span class="author">James Pearson<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=J%C3%BCrgen%20J.%20Popp" target="_blank"><span class="author">Jürgen J. Popp<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jo%20Bovy" target="_blank"><span class="author">Jo Bovy<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Josh%20Speagle" target="_blank"><span class="author">Josh Speagle<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hugh%20Dickinson" target="_blank"><span class="author">Hugh Dickinson<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lucy%20Fortson" target="_blank"><span class="author">Lucy Fortson<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tobias%20G%C3%A9ron" target="_blank"><span class="author">Tobias Géron<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Sandor%20Kruk" target="_blank"><span class="author">Sandor Kruk<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chris%20J.%20Lintott" target="_blank"><span class="author">Chris J. Lintott<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kameswara%20Mantha" target="_blank"><span class="author">Kameswara Mantha<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Devina%20Mohan" target="_blank"><span class="author">Devina Mohan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=David%20O%27Ryan" target="_blank"><span class="author">David O'Ryan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Inigo%20V.%20Slijepevic" target="_blank"><span class="author">Inigo V. Slijepevic<span></span></span></a>
            </p>
            <p id="summary-2404.02973" class="summary">We present the first systematic investigation of supervised scaling laws outside of an ImageNet-like context - on images of galaxies. We use 840k galaxy images and over 100M annotations by Galaxy Zoo volunteers, comparable in scale to Imagenet-1K. We find that adding annotated galaxy images provides a power law improvement in performance across all architectures and all tasks, while adding trainable parameters is effective only for some (typically more subjectively challenging) tasks. We then compare the downstream performance of finetuned models pretrained on either ImageNet-12k alone vs. additionally pretrained on our galaxy images. We achieve an average relative error rate reduction of 31% across 5 downstream tasks of scientific interest. Our finetuned models are more label-efficient and, unlike their ImageNet-12k-pretrained equivalents, often achieve linear transfer performance equal to that of end-to-end finetuning. We find relatively modest additional downstream benefits from scaling model size, implying that scaling alone is not sufficient to address our domain gap, and suggest that practitioners with qualitatively different images might benefit more from in-domain adaption followed by targeted downstream labelling.</p>
            <div id="pdf-container-2404.02973" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.02973" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(79).html"></iframe>
            </div>
            <div id="kimi-container-2404.02973" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03634" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03634" target="_blank" title="81/93"><span class="index">#81</span></a>
                <a id="title-2404.03634" class="title-link" href="https://papers.cool/arxiv/2404.03634" target="_blank">PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments</a>
                <a id="pdf-2404.03634" class="title-pdf" onclick="togglePdf(&#39;2404.03634&#39;, &#39;https://arxiv.org/pdf/2404.03634.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03634">1</sup>]</a>
                <a id="copy-2404.03634" class="title-copy" onclick="copyToClipboard(&#39;2404.03634&#39;)">[Copy]</a>
                <a id="kimi-2404.03634" class="title-kimi" onclick="chatKimi(&#39;2404.03634&#39;, this)">[Kimi<sup id="kimi-stars-2404.03634">3</sup>]</a>
            </h2>
            <p id="authors-2404.03634" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kairui%20Ding" target="_blank"><span class="author">Kairui Ding<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Boyuan%20Chen" target="_blank"><span class="author">Boyuan Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ruihai%20Wu" target="_blank"><span class="author">Ruihai Wu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yuyang%20Li" target="_blank"><span class="author">Yuyang Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zongzheng%20Zhang" target="_blank"><span class="author">Zongzheng Zhang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Huan-ang%20Gao" target="_blank"><span class="author">Huan-ang Gao<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Siqi%20Li" target="_blank"><span class="author">Siqi Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yixin%20Zhu" target="_blank"><span class="author">Yixin Zhu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Guyue%20Zhou" target="_blank"><span class="author">Guyue Zhou<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hao%20Dong" target="_blank"><span class="author">Hao Dong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hao%20Zhao" target="_blank"><span class="author">Hao Zhao<span></span></span></a>
            </p>
            <p id="summary-2404.03634" class="summary">Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes. Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen. Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments. This work offers a robust and adaptable solution for manipulating ungraspable objects.</p>
            <div id="pdf-container-2404.03634" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03634" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(80).html"></iframe>
            </div>
            <div id="kimi-container-2404.03634" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03617" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03617" target="_blank" title="82/93"><span class="index">#82</span></a>
                <a id="title-2404.03617" class="title-link" href="https://papers.cool/arxiv/2404.03617" target="_blank">On the Efficiency of Convolutional Neural Networks</a>
                <a id="pdf-2404.03617" class="title-pdf" onclick="togglePdf(&#39;2404.03617&#39;, &#39;https://arxiv.org/pdf/2404.03617.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03617">1</sup>]</a>
                <a id="copy-2404.03617" class="title-copy" onclick="copyToClipboard(&#39;2404.03617&#39;)">[Copy]</a>
                <a id="kimi-2404.03617" class="title-kimi" onclick="chatKimi(&#39;2404.03617&#39;, this)">[Kimi<sup id="kimi-stars-2404.03617">8</sup>]</a>
            </h2>
            <p id="authors-2404.03617" class="authors a-dynamic"><strong>Author</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andrew%20Lavin" target="_blank"><span class="author">Andrew Lavin<span></span></span></a>
            </p>
            <p id="summary-2404.03617" class="summary">Since the breakthrough performance of AlexNet in 2012, convolutional neural networks (convnets) have grown into extremely powerful vision models. Deep learning researchers have used convnets to produce accurate results that were unachievable a decade ago. Yet computer scientists make computational efficiency their primary objective. Accuracy with exorbitant cost is not acceptable; an algorithm must also minimize its computational requirements. Confronted with the daunting computation that convnets use, deep learning researchers also became interested in efficiency. Researchers applied tremendous effort to find the convnet architectures that have the greatest efficiency. However, skepticism grew among researchers and engineers alike about the relevance of arithmetic complexity. Contrary to the prevailing view that latency and arithmetic complexity are irreconcilable, a simple formula relates both through computational efficiency. This insight enabled us to co-optimize the separate factors that determine latency. We observed that the degenerate conv2d layers that produce the best accuracy-complexity trade-off also have low operational intensity. Therefore, kernels that implement these layers use significant memory resources. We solved this optimization problem with block-fusion kernels that implement all layers of a residual block, thereby creating temporal locality, avoiding communication, and reducing workspace size. Our ConvFirst model with block-fusion kernels ran approximately four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the ImageNet-1K classification task. Our unified approach to convnet efficiency envisions a new era of models and kernels that achieve greater accuracy at lower cost.</p>
            <div id="pdf-container-2404.03617" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03617" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(81).html"></iframe>
            </div>
            <div id="kimi-container-2404.03617" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03541" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03541" target="_blank" title="83/93"><span class="index">#83</span></a>
                <a id="title-2404.03541" class="title-link" href="https://papers.cool/arxiv/2404.03541" target="_blank">Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models</a>
                <a id="pdf-2404.03541" class="title-pdf" onclick="togglePdf(&#39;2404.03541&#39;, &#39;https://arxiv.org/pdf/2404.03541.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03541"></sup>]</a>
                <a id="copy-2404.03541" class="title-copy" onclick="copyToClipboard(&#39;2404.03541&#39;)">[Copy]</a>
                <a id="kimi-2404.03541" class="title-kimi" onclick="chatKimi(&#39;2404.03541&#39;, this)">[Kimi<sup id="kimi-stars-2404.03541">2</sup>]</a>
            </h2>
            <p id="authors-2404.03541" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Siyuan%20Mei" target="_blank"><span class="author">Siyuan Mei<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fuxin%20Fan" target="_blank"><span class="author">Fuxin Fan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fabian%20Wagner" target="_blank"><span class="author">Fabian Wagner<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mareike%20Thies" target="_blank"><span class="author">Mareike Thies<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Mingxuan%20Gu" target="_blank"><span class="author">Mingxuan Gu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yipeng%20Sun" target="_blank"><span class="author">Yipeng Sun<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Andreas%20Maier" target="_blank"><span class="author">Andreas Maier<span></span></span></a>
            </p>
            <p id="summary-2404.03541" class="summary">Deep learning-based medical image processing algorithms require representative data during development. In particular, surgical data might be difficult to obtain, and high-quality public datasets are limited. To overcome this limitation and augment datasets, a widely adopted solution is the generation of synthetic images. In this work, we employ conditional diffusion models to generate knee radiographs from contour and bone segmentations. Remarkably, two distinct strategies are presented by incorporating the segmentation as a condition into the sampling and training process, namely, conditional sampling and conditional training. The results demonstrate that both methods can generate realistic images while adhering to the conditioning segmentation. The conditional training method outperforms the conditional sampling method and the conventional U-Net.</p>
            <div id="pdf-container-2404.03541" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03541" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(82).html"></iframe>
            </div>
            <div id="kimi-container-2404.03541" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03425" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03425" target="_blank" title="84/93"><span class="index">#84</span></a>
                <a id="title-2404.03425" class="title-link" href="https://papers.cool/arxiv/2404.03425" target="_blank">ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model</a>
                <a id="pdf-2404.03425" class="title-pdf" onclick="togglePdf(&#39;2404.03425&#39;, &#39;https://arxiv.org/pdf/2404.03425.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03425">2</sup>]</a>
                <a id="copy-2404.03425" class="title-copy" onclick="copyToClipboard(&#39;2404.03425&#39;)">[Copy]</a>
                <a id="kimi-2404.03425" class="title-kimi" onclick="chatKimi(&#39;2404.03425&#39;, this)">[Kimi<sup id="kimi-stars-2404.03425">2</sup>]</a>
            </h2>
            <p id="authors-2404.03425" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hongruixuan%20Chen" target="_blank"><span class="author">Hongruixuan Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jian%20Song" target="_blank"><span class="author">Jian Song<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Chengxi%20Han" target="_blank"><span class="author">Chengxi Han<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Junshi%20Xia" target="_blank"><span class="author">Junshi Xia<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Naoto%20Yokoya" target="_blank"><span class="author">Naoto Yokoya<span></span></span></a>
            </p>
            <p id="summary-2404.03425" class="summary">Convolutional neural networks (CNN) and Transformers have made impressive progress in the field of remote sensing change detection (CD). However, both architectures have their inherent shortcomings. Recently, the Mamba architecture, based on spatial state models, has shown remarkable performance in a series of natural language processing tasks, which can effectively compensate for the shortcomings of the above two architectures. In this paper, we explore for the first time the potential of the Mamba architecture for remote sensing change detection tasks. We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD), semantic change detection (SCD), and building damage assessment (BDA), respectively. All three frameworks adopt the cutting-edge visual Mamba architecture as the encoder, which allows full learning of global spatial contextual information from the input images. For the change decoder, which is available in all three architectures, we propose three spatio-temporal relationship modeling mechanisms, which can be naturally combined with the Mamba architecture and fully utilize its attribute to achieve spatio-temporal interaction of multi-temporal features and obtain accurate change information. On five benchmark datasets, our proposed frameworks outperform current CNN- and Transformer-based approaches without using any complex strategies or tricks, fully demonstrating the potential of the Mamba architecture. Specifically, we obtained 83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU, LEVIR-CD+, and WHU-CD; on the SCD dataset SECOND, we obtained 24.04% SeK; and on the xBD dataset, we obtained 81.41% overall F1 score. The source code will be available in https://github.com/ChenHongruixuan/MambaCD</p>
            <div id="pdf-container-2404.03425" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03425" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(83).html"></iframe>
            </div>
            <div id="kimi-container-2404.03425" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03415" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03415" target="_blank" title="85/93"><span class="index">#85</span></a>
                <a id="title-2404.03415" class="title-link" href="https://papers.cool/arxiv/2404.03415" target="_blank">Future Predictive Success-or-Failure Classification for Long-Horizon Robotic Tasks</a>
                <a id="pdf-2404.03415" class="title-pdf" onclick="togglePdf(&#39;2404.03415&#39;, &#39;https://arxiv.org/pdf/2404.03415.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03415">1</sup>]</a>
                <a id="copy-2404.03415" class="title-copy" onclick="copyToClipboard(&#39;2404.03415&#39;)">[Copy]</a>
                <a id="kimi-2404.03415" class="title-kimi" onclick="chatKimi(&#39;2404.03415&#39;, this)">[Kimi<sup id="kimi-stars-2404.03415">2</sup>]</a>
            </h2>
            <p id="authors-2404.03415" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Naoya%20Sogi" target="_blank"><span class="author">Naoya Sogi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Hiroyuki%20Oyama" target="_blank"><span class="author">Hiroyuki Oyama<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Takashi%20Shibata" target="_blank"><span class="author">Takashi Shibata<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Makoto%20Terao" target="_blank"><span class="author">Makoto Terao<span></span></span></a>
            </p>
            <p id="summary-2404.03415" class="summary">Automating long-horizon tasks with a robotic arm has been a central research topic in robotics. Optimization-based action planning is an efficient approach for creating an action plan to complete a given task. Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects. The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually. To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically. The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions. The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan. This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution. The regularization term improves future prediction and classification performance. The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments.</p>
            <div id="pdf-container-2404.03415" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03415" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(84).html"></iframe>
            </div>
            <div id="kimi-container-2404.03415" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03253" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03253" target="_blank" title="86/93"><span class="index">#86</span></a>
                <a id="title-2404.03253" class="title-link" href="https://papers.cool/arxiv/2404.03253" target="_blank">A dataset of primary nasopharyngeal carcinoma MRI with multi-modalities segmentation</a>
                <a id="pdf-2404.03253" class="title-pdf" onclick="togglePdf(&#39;2404.03253&#39;, &#39;https://arxiv.org/pdf/2404.03253.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03253"></sup>]</a>
                <a id="copy-2404.03253" class="title-copy" onclick="copyToClipboard(&#39;2404.03253&#39;)">[Copy]</a>
                <a id="kimi-2404.03253" class="title-kimi" onclick="chatKimi(&#39;2404.03253&#39;, this)">[Kimi<sup id="kimi-stars-2404.03253"></sup>]</a>
            </h2>
            <p id="authors-2404.03253" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yin%20Li" target="_blank"><span class="author">Yin Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qi%20Chen" target="_blank"><span class="author">Qi Chen<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kai%20Wang" target="_blank"><span class="author">Kai Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Meige%20Li" target="_blank"><span class="author">Meige Li<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Liping%20Si" target="_blank"><span class="author">Liping Si<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yingwei%20Guo" target="_blank"><span class="author">Yingwei Guo<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yu%20Xiong" target="_blank"><span class="author">Yu Xiong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Qixing%20Wang" target="_blank"><span class="author">Qixing Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yang%20Qin" target="_blank"><span class="author">Yang Qin<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ling%20Xu" target="_blank"><span class="author">Ling Xu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Patrick%20van%20der%20Smagt" target="_blank"><span class="author">Patrick van der Smagt<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jun%20Tang" target="_blank"><span class="author">Jun Tang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nutan%20Chen" target="_blank"><span class="author">Nutan Chen<span></span></span></a>
            </p>
            <p id="summary-2404.03253" class="summary">Multi-modality magnetic resonance imaging data with various sequences facilitate the early diagnosis, tumor segmentation, and disease staging in the management of nasopharyngeal carcinoma (NPC). The lack of publicly available, comprehensive datasets limits advancements in diagnosis, treatment planning, and the development of machine learning algorithms for NPC. Addressing this critical need, we introduce the first comprehensive NPC MRI dataset, encompassing MR axial imaging of 277 primary NPC patients. This dataset includes T1-weighted, T2-weighted, and contrast-enhanced T1-weighted sequences, totaling 831 scans. In addition to the corresponding clinical data, manually annotated and labeled segmentations by experienced radiologists offer high-quality data resources from untreated primary NPC.</p>
            <div id="pdf-container-2404.03253" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03253" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(85).html"></iframe>
            </div>
            <div id="kimi-container-2404.03253" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03200" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03200" target="_blank" title="87/93"><span class="index">#87</span></a>
                <a id="title-2404.03200" class="title-link" href="https://papers.cool/arxiv/2404.03200" target="_blank">Future-Proofing Class Incremental Learning</a>
                <a id="pdf-2404.03200" class="title-pdf" onclick="togglePdf(&#39;2404.03200&#39;, &#39;https://arxiv.org/pdf/2404.03200.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03200">1</sup>]</a>
                <a id="copy-2404.03200" class="title-copy" onclick="copyToClipboard(&#39;2404.03200&#39;)">[Copy]</a>
                <a id="kimi-2404.03200" class="title-kimi" onclick="chatKimi(&#39;2404.03200&#39;, this)">[Kimi<sup id="kimi-stars-2404.03200">2</sup>]</a>
            </h2>
            <p id="authors-2404.03200" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Quentin%20Jodelet" target="_blank"><span class="author">Quentin Jodelet<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xin%20Liu" target="_blank"><span class="author">Xin Liu<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yin%20Jun%20Phua" target="_blank"><span class="author">Yin Jun Phua<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tsuyoshi%20Murata" target="_blank"><span class="author">Tsuyoshi Murata<span></span></span></a>
            </p>
            <p id="summary-2404.03200" class="summary">Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning.</p>
            <div id="pdf-container-2404.03200" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03200" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(86).html"></iframe>
            </div>
            <div id="kimi-container-2404.03200" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03188" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03188" target="_blank" title="88/93"><span class="index">#88</span></a>
                <a id="title-2404.03188" class="title-link" href="https://papers.cool/arxiv/2404.03188" target="_blank">Classification of Nasopharyngeal Cases using DenseNet Deep Learning Architecture</a>
                <a id="pdf-2404.03188" class="title-pdf" onclick="togglePdf(&#39;2404.03188&#39;, &#39;https://arxiv.org/pdf/2404.03188.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03188"></sup>]</a>
                <a id="copy-2404.03188" class="title-copy" onclick="copyToClipboard(&#39;2404.03188&#39;)">[Copy]</a>
                <a id="kimi-2404.03188" class="title-kimi" onclick="chatKimi(&#39;2404.03188&#39;, this)">[Kimi<sup id="kimi-stars-2404.03188"></sup>]</a>
            </h2>
            <p id="authors-2404.03188" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=W.%20S.%20H.%20M.%20W.%20Ahmad" target="_blank"><span class="author">W. S. H. M. W. Ahmad<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=M.%20F.%20A.%20Fauzi" target="_blank"><span class="author">M. F. A. Fauzi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=M.%20K.%20Abdullahi" target="_blank"><span class="author">M. K. Abdullahi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jenny%20T.%20H.%20Lee" target="_blank"><span class="author">Jenny T. H. Lee<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=N.%20S.%20A.%20Basry" target="_blank"><span class="author">N. S. A. Basry<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=A%20Yahaya" target="_blank"><span class="author">A Yahaya<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=A.%20M.%20Ismail" target="_blank"><span class="author">A. M. Ismail<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=A.%20Adam" target="_blank"><span class="author">A. Adam<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Elaine%20W.%20L.%20Chan" target="_blank"><span class="author">Elaine W. L. Chan<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=F.%20S.%20Abas" target="_blank"><span class="author">F. S. Abas<span></span></span></a>
            </p>
            <p id="summary-2404.03188" class="summary">Nasopharyngeal carcinoma (NPC) is one of the understudied yet deadliest cancers in South East Asia. In Malaysia, the prevalence is identified mainly in Sarawak, among the ethnic of Bidayuh. NPC is often late-diagnosed because it is asymptomatic at the early stage. There are several tissue representations from the nasopharynx biopsy, such as nasopharyngeal inflammation (NPI), lymphoid hyperplasia (LHP), nasopharyngeal carcinoma (NPC) and normal tissue. This paper is our first initiative to identify the difference between NPC, NPI and normal cases. Seven whole slide images (WSIs) with gigapixel resolutions from seven different patients and two hospitals were experimented with using two test setups, consisting of a different set of images. The tissue regions are patched into smaller blocks and classified using DenseNet architecture with 21 dense layers. Two tests are carried out, each for proof of concept (Test 1) and real-test scenario (Test 2). The accuracy achieved for NPC class is 94.8% for Test 1 and 67.0% for Test 2.</p>
            <div id="pdf-container-2404.03188" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03188" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(87).html"></iframe>
            </div>
            <div id="kimi-container-2404.03188" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03126" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03126" target="_blank" title="89/93"><span class="index">#89</span></a>
                <a id="title-2404.03126" class="title-link" href="https://papers.cool/arxiv/2404.03126" target="_blank">GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis</a>
                <a id="pdf-2404.03126" class="title-pdf" onclick="togglePdf(&#39;2404.03126&#39;, &#39;https://arxiv.org/pdf/2404.03126.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03126"></sup>]</a>
                <a id="copy-2404.03126" class="title-copy" onclick="copyToClipboard(&#39;2404.03126&#39;)">[Copy]</a>
                <a id="kimi-2404.03126" class="title-kimi" onclick="chatKimi(&#39;2404.03126&#39;, this)">[Kimi<sup id="kimi-stars-2404.03126"></sup>]</a>
            </h2>
            <p id="authors-2404.03126" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Emmanouil%20Nikolakakis" target="_blank"><span class="author">Emmanouil Nikolakakis<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Utkarsh%20Gupta" target="_blank"><span class="author">Utkarsh Gupta<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jonathan%20Vengosh" target="_blank"><span class="author">Jonathan Vengosh<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Justin%20Bui" target="_blank"><span class="author">Justin Bui<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Razvan%20Marinescu" target="_blank"><span class="author">Razvan Marinescu<span></span></span></a>
            </p>
            <p id="summary-2404.03126" class="summary">We present GaSpCT, a novel view synthesis and 3D scene representation method used to generate novel projection views for Computer Tomography (CT) scans. We adapt the Gaussian Splatting framework to enable novel view synthesis in CT based on limited sets of 2D image projections and without the need for Structure from Motion (SfM) methodologies. Therefore, we reduce the total scanning duration and the amount of radiation dose the patient receives during the scan. We adapted the loss function to our use-case by encouraging a stronger background and foreground distinction using two sparsity promoting regularizers: a beta loss and a total variation (TV) loss. Finally, we initialize the Gaussian locations across the 3D space using a uniform prior distribution of where the brain's positioning would be expected to be within the field of view. We evaluate the performance of our model using brain CT scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and demonstrate that the rendered novel views closely match the original projection views of the simulated scan, and have better performance than other implicit 3D scene representations methodologies. Furthermore, we empirically observe reduced training time compared to neural network based image synthesis for sparse-view CT image reconstruction. Finally, the memory requirements of the Gaussian Splatting representations are reduced by 17% compared to the equivalent voxel grid image representations.</p>
            <div id="pdf-container-2404.03126" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03126" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(88).html"></iframe>
            </div>
            <div id="kimi-container-2404.03126" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03067" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03067" target="_blank" title="90/93"><span class="index">#90</span></a>
                <a id="title-2404.03067" class="title-link" href="https://papers.cool/arxiv/2404.03067" target="_blank">Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System</a>
                <a id="pdf-2404.03067" class="title-pdf" onclick="togglePdf(&#39;2404.03067&#39;, &#39;https://arxiv.org/pdf/2404.03067.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03067">1</sup>]</a>
                <a id="copy-2404.03067" class="title-copy" onclick="copyToClipboard(&#39;2404.03067&#39;)">[Copy]</a>
                <a id="kimi-2404.03067" class="title-kimi" onclick="chatKimi(&#39;2404.03067&#39;, this)">[Kimi<sup id="kimi-stars-2404.03067">2</sup>]</a>
            </h2>
            <p id="authors-2404.03067" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xiwen%20Dengxiong" target="_blank"><span class="author">Xiwen Dengxiong<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Xueting%20Wang" target="_blank"><span class="author">Xueting Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Shi%20Bai" target="_blank"><span class="author">Shi Bai<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yunbo%20Zhang" target="_blank"><span class="author">Yunbo Zhang<span></span></span></a>
            </p>
            <p id="summary-2404.03067" class="summary">Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.</p>
            <div id="pdf-container-2404.03067" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03067" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(89).html"></iframe>
            </div>
            <div id="kimi-container-2404.03067" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03022" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03022" target="_blank" title="91/93"><span class="index">#91</span></a>
                <a id="title-2404.03022" class="title-link" href="https://papers.cool/arxiv/2404.03022" target="_blank">BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes</a>
                <a id="pdf-2404.03022" class="title-pdf" onclick="togglePdf(&#39;2404.03022&#39;, &#39;https://arxiv.org/pdf/2404.03022.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03022"></sup>]</a>
                <a id="copy-2404.03022" class="title-copy" onclick="copyToClipboard(&#39;2404.03022&#39;)">[Copy]</a>
                <a id="kimi-2404.03022" class="title-kimi" onclick="chatKimi(&#39;2404.03022&#39;, this)">[Kimi<sup id="kimi-stars-2404.03022">1</sup>]</a>
            </h2>
            <p id="authors-2404.03022" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Amirhossein%20Abaskohi" target="_blank"><span class="author">Amirhossein Abaskohi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Amirhossein%20Dabiriaghdam" target="_blank"><span class="author">Amirhossein Dabiriaghdam<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Lele%20Wang" target="_blank"><span class="author">Lele Wang<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Giuseppe%20Carenini" target="_blank"><span class="author">Giuseppe Carenini<span></span></span></a>
            </p>
            <p id="summary-2404.03022" class="summary">Memes, combining text and images, frequently use metaphors to convey persuasive messages, shaping public opinion. Motivated by this, our team engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task designed to identify rhetorical and psychological persuasion techniques embedded within memes. To tackle this problem, we introduced a caption generation step to assess the modality gap and the impact of additional semantic information from images, which improved our result. Our best model utilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as the text encoder and CLIP as the image encoder. It outperforms the baseline by a large margin in all 12 subtasks. In particular, it ranked in top-3 across all languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively strong performance. The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders. This highlights the potential for improving abstract visual semantics encoding.</p>
            <div id="pdf-container-2404.03022" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03022" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(90).html"></iframe>
            </div>
            <div id="kimi-container-2404.03022" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.03010" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.03010" target="_blank" title="92/93"><span class="index">#92</span></a>
                <a id="title-2404.03010" class="title-link" href="https://papers.cool/arxiv/2404.03010" target="_blank">Skeleton Recall Loss for Connectivity Conserving and Resource Efficient Segmentation of Thin Tubular Structures</a>
                <a id="pdf-2404.03010" class="title-pdf" onclick="togglePdf(&#39;2404.03010&#39;, &#39;https://arxiv.org/pdf/2404.03010.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.03010"></sup>]</a>
                <a id="copy-2404.03010" class="title-copy" onclick="copyToClipboard(&#39;2404.03010&#39;)">[Copy]</a>
                <a id="kimi-2404.03010" class="title-kimi" onclick="chatKimi(&#39;2404.03010&#39;, this)">[Kimi<sup id="kimi-stars-2404.03010"></sup>]</a>
            </h2>
            <p id="authors-2404.03010" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Yannick%20Kirchhoff" target="_blank"><span class="author">Yannick Kirchhoff<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Maximilian%20R.%20Rokuss" target="_blank"><span class="author">Maximilian R. Rokuss<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Saikat%20Roy" target="_blank"><span class="author">Saikat Roy<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Balint%20Kovacs" target="_blank"><span class="author">Balint Kovacs<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Constantin%20Ulrich" target="_blank"><span class="author">Constantin Ulrich<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Tassilo%20Wald" target="_blank"><span class="author">Tassilo Wald<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Maximilian%20Zenk" target="_blank"><span class="author">Maximilian Zenk<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Philipp%20Vollmuth" target="_blank"><span class="author">Philipp Vollmuth<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jens%20Kleesiek" target="_blank"><span class="author">Jens Kleesiek<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Fabian%20Isensee" target="_blank"><span class="author">Fabian Isensee<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Klaus%20Maier-Hein" target="_blank"><span class="author">Klaus Maier-Hein<span></span></span></a>
            </p>
            <p id="summary-2404.03010" class="summary">Accurately segmenting thin tubular structures, such as vessels, nerves, roads or concrete cracks, is a crucial task in computer vision. Standard deep learning-based segmentation loss functions, such as Dice or Cross-Entropy, focus on volumetric overlap, often at the expense of preserving structural connectivity or topology. This can lead to segmentation errors that adversely affect downstream tasks, including flow calculation, navigation, and structural inspection. Although current topology-focused losses mark an improvement, they introduce significant computational and memory overheads. This is particularly relevant for 3D data, rendering these losses infeasible for larger volumes as well as increasingly important multi-class segmentation problems. To mitigate this, we propose a novel Skeleton Recall Loss, which effectively addresses these challenges by circumventing intensive GPU-based calculations with inexpensive CPU operations. It demonstrates overall superior performance to current state-of-the-art approaches on five public datasets for topology-preserving segmentation, while substantially reducing computational overheads by more than 90%. In doing so, we introduce the first multi-class capable loss function for thin structure segmentation, excelling in both efficiency and efficacy for topology-preservation.</p>
            <div id="pdf-container-2404.03010" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.03010" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(91).html"></iframe>
            </div>
            <div id="kimi-container-2404.03010" class="kimi-container" style="display:none"></div>
        </div>
        <div id="2404.02999" class="panel paper">
            <h2 class="title">
                <a href="https://arxiv.org/abs/2404.02999" target="_blank" title="93/93"><span class="index">#93</span></a>
                <a id="title-2404.02999" class="title-link" href="https://papers.cool/arxiv/2404.02999" target="_blank">MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy</a>
                <a id="pdf-2404.02999" class="title-pdf" onclick="togglePdf(&#39;2404.02999&#39;, &#39;https://arxiv.org/pdf/2404.02999.pdf&#39;, this)">[PDF<sup id="pdf-stars-2404.02999">1</sup>]</a>
                <a id="copy-2404.02999" class="title-copy" onclick="copyToClipboard(&#39;2404.02999&#39;)">[Copy]</a>
                <a id="kimi-2404.02999" class="title-kimi" onclick="chatKimi(&#39;2404.02999&#39;, this)">[Kimi<sup id="kimi-stars-2404.02999">1</sup>]</a>
            </h2>
            <p id="authors-2404.02999" class="authors a-dynamic"><strong>Authors</strong>:
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=John%20J.%20Han" target="_blank"><span class="author">John J. Han<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Ayberk%20Acar" target="_blank"><span class="author">Ayberk Acar<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Nicholas%20Kavoussi" target="_blank"><span class="author">Nicholas Kavoussi<span></span></span></a>
                ;
                <a href="https://arxiv.org/search/?searchtype=author&amp;query=Jie%20Ying%20Wu" target="_blank"><span class="author">Jie Ying Wu<span></span></span></a>
            </p>
            <p id="summary-2404.02999" class="summary">Style transfer is a promising approach to close the sim-to-real gap in medical endoscopy. Rendering realistic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate realistic simulations as well as ground truth camera poses and depth maps. Although image-to-image (I2I) translation models such as CycleGAN perform well, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames. We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering. MeshBrush uses the underlying geometry of patient imaging data while leveraging existing I2I methods. With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs. We demonstrate that mesh stylization is a promising approach for creating realistic simulations for downstream tasks such as training and preoperative planning. Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures.</p>
            <div id="pdf-container-2404.02999" class="pdf-container" style="display:none">
                <iframe id="pdf-iframe-2404.02999" class="pdf-iframe" width="100%" height="800px" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/saved_resource(92).html"></iframe>
            </div>
            <div id="kimi-container-2404.02999" class="kimi-container" style="display:none"></div>
        </div>
    </div>
    <div class="footer">
        Designed by <a href="https://kexue.fm/" target="_blank">kexue.fm</a> | Powered by <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>
    </div>
    <div id="app-bar" class="app-bar panel" style="opacity: 0;">
        <div id="app-bar-search" class="app-bar-content" style="display:none">
            <div class="app-search-keywords">
                <div class="keywords-included">
                    <p>Include:</p>
                    <textarea id="keywords-included" class="text-input" placeholder="LLM
Transformer
Attention"></textarea>
                </div>
                <div class="keywords-excluded">
                    <p>Exclude:</p>
                    <textarea id="keywords-excluded" class="text-input"></textarea>
                </div>
            </div>
            <div class="submit">
                <p><button type="button" onclick="appSearch()" class="search-btn">Search</button></p>
                <p><label><input type="checkbox" id="search-filter">Filter</label></p>
                <p><label><input type="checkbox" id="search-highlight" checked="true">Highlight</label></p>
            </div>
        </div>
        <div id="app-bar-star" class="app-bar-content" style="display:none">
            <p>Stared Paper(s):</p>
            <div class="items">
                <p id="app-bar-star-2404.03658" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03658">Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03658&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03658&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03657" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03657">OW-VISCap: Open-World Video Instance Segmentation and Captioning</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03657&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03657&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03656" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03656">MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03656&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03656&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03654" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03654">RaFE: Generative Radiance Fields Restoration</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03654&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03654&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03653" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03653">CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03653&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03653&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03652" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03652">The More You See in 2D, the More You Perceive in 3D</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03652&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03652&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03650" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03650">OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03650&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03650&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03645" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03645">Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03645&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03645&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03642" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03642">DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03642&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03642&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03635" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03635">WorDepth: Variational Language Prior for Monocular Depth Estimation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03635&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03635&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03632" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03632">Reference-Based 3D-Aware Image Editing with Triplane</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03632&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03632&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03631" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03631">Robust Concept Erasure Using Task Vectors</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03631&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03631&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03620" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03620">LCM-Lookahead for Encoder-based Text-to-Image Personalization</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03620&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03620&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03618" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03618">DeViDe: Faceted medical knowledge for improved medical vision-language pre-training</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03618&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03618&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03613" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03613">Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03613&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03613&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03611" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03611">InsectMamba: Insect Pest Classification with State Space Model</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03611&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03611&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03590" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03590">SemGrasp: Semantic Grasp Generation via Language Aligned Discretization</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03590&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03590&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03584" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03584">Towards more realistic human motion prediction with attention to motion coordination</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03584&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03584&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03575" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03575">DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation Pattern Sampling</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03575&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03575&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03574" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03574">TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03574&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03574&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03572" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03572">Terrain Point Cloud Inpainting via Signal Decomposition</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03572&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03572&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03566" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03566">PointInfinity: Resolution-Invariant Point Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03566&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03566&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03539" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03539">Is CLIP the main roadblock for fine-grained open-world perception?</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03539&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03539&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03537" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03537">If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03537&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03537&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03531" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03531">COMO: Compact Mapping and Odometry</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03531&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03531&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03527" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03527">HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid, Asymmetric, and Progressive Heterogeneous Feature Fusion</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03527&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03527&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03518" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03518">SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03518&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03518&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03507" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03507">DQ-DETR: DETR with Dynamic Query for Tiny Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03507&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03507&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03482" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03482">AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03482&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03482&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03477" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03477">Towards Automated Movie Trailer Generation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03477&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03477&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03474" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03474">Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03474&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03474&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03462" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03462">You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03462&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03462&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03451" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03451">How Much Data are Enough? Investigating Dataset Requirements for Patch-Based Brain MRI Segmentation Tasks</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03451&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03451&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03446" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03446">SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03446&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03446&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03443" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03443">Part-Attention Based Model Make Occluded Person Re-Identification Stronger</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03443&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03443&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03421" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03421">Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03421&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03421&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03417" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03417">NMF-Based Analysis of Mobile Eye-Tracking Data</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03417&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03417&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03413" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03413">MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03413&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03413&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03407" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03407">AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03407&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03407&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03398" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03398">Scaling Up Video Summarization Pretraining with Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03398&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03398&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03394" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03394">Background Noise Reduction of Attention Map for Weakly Supervised Semantic Segmentation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03394&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03394&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03392" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03392">Two Tricks to Improve Unsupervised Segmentation Learning</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03392&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03392&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03384" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03384">LongVLM: Efficient Long Video Understanding via Large Language Models</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03384&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03384&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03349" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03349">VF-NeRF: Viewshed Fields for Rigid NeRF Registration</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03349&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03349&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03340" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03340">Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03340&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03340&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03327" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03327">DI-Retinex: Digital-Imaging Retinex Theory for Low-Light Image Enhancement</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03327&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03327&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03323" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03323">Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03323&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03323&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03296" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03296">AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03296&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03296&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03277" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03277">Design and Development of a Framework For Stroke-Based Handwritten Gujarati Font Generation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03277&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03277&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03256" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03256">Multi Positive Contrastive Learning with Pose-Consistent Generated Images</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03256&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03256&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03251" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03251">Real-time Noise Source Estimation of a Camera System from an Image and Metadata</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03251&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03251&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03248" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03248">Learning Transferable Negative Prompts for Out-of-Distribution Detection</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03248&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03248&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03242" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03242">Would Deep Generative Models Amplify Bias in Future Models?</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03242&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03242&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03225" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03225">FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR Image Classification</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03225&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03225&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03219" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03219">iSeg: Interactive 3D Segmentation via Interactive Attention</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03219&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03219&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03214" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03214">LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03214&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03214&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03210" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03210">HDR Imaging for Dynamic Scenes with Events</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03210&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03210&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03202" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03202">OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field Reconstruction using Omnidirectional Images</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03202&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03202&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03191" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03191">CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception Tasks</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03191&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03191&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03190" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03190">Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth Estimation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03190&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03190&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03187" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03187">AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03187&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03187&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03183" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03183">BodyMAP - Jointly Predicting Body Mesh and 3D Applied Pressure Map for People in Bed</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03183&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03183&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03181" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03181">MonoCD: Monocular 3D Object Detection with Complementary Depths</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03181&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03181&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03179" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03179">UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03179&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03179&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03161" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03161">BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro QR Codes</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03161&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03161&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03159" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03159">HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03159&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03159&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03145" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03145">DreamWalk: Style Space Exploration using Diffusion Guidance</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03145&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03145&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03144" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03144">Diverse and Tailored Image Generation for Zero-shot Multi-label Classification</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03144&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03144&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03138" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03138">Discontinuity-preserving Normal Integration with Auxiliary Edges</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03138&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03138&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03121" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03121">Utilizing Computer Vision for Continuous Monitoring of Vaccine Side Effects in Experimental Mice</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03121&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03121&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03118" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03118">LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03118&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03118&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03110" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03110">Ego-Motion Aware Target Prediction Module for Robust Multi-Object Tracking</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03110&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03110&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03109" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03109">Many-to-many Image Generation with Auto-regressive Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03109&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03109&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03097" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03097">SalFoM: Dynamic Saliency Prediction with Video Foundation Models</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03097&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03097&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03070" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03070">Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03070&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03070&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03043" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03043">Linear Anchored Gaussian Mixture Model for Location and Width Computation of Objects in Thick Line Shape</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03043&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03043&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03042" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03042">AWOL: Analysis WithOut synthesis using Language</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03042&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03042&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03015" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03015">DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03015&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03015&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.02990" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.02990">ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.02990&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.02990&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.02973" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.02973">Scaling Laws for Galaxy Images</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.02973&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.02973&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03634" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03634">PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03634&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03634&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03617" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03617">On the Efficiency of Convolutional Neural Networks</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03617&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03617&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03541" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03541">Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03541&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03541&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03425" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03425">ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03425&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03425&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03415" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03415">Future Predictive Success-or-Failure Classification for Long-Horizon Robotic Tasks</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03415&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03415&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03253" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03253">A dataset of primary nasopharyngeal carcinoma MRI with multi-modalities segmentation</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03253&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03253&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03200" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03200">Future-Proofing Class Incremental Learning</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03200&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03200&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03188" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03188">Classification of Nasopharyngeal Cases using DenseNet Deep Learning Architecture</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03188&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03188&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03126" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03126">GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03126&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03126&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03067" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03067">Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03067&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03067&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03022" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03022">BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03022&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03022&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.03010" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.03010">Skeleton Recall Loss for Connectivity Conserving and Resource Efficient Segmentation of Thin Tubular Structures</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.03010&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.03010&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
                <p id="app-bar-star-2404.02999" style="display:none">
                    <span class="i-pdf" title="[PDF]">[P]</span>
                    <span class="i-kimi" title="[Kimi]">[K]</span>
                    <a class="i-title" href="https://papers.cool/arxiv/cs.CV?show=200#2404.02999">MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy</a>
                    <a class="i-star" onclick="toggleAppStar(&#39;2404.02999&#39;)" style="display: block;"><i class="fa fa-star"></i></a>
                    <a class="i-unstar" onclick="toggleAppStar(&#39;2404.02999&#39;)" style="display:none"><i class="fa fa-star-o"></i></a>
                </p>
            </div>
            <div class="submit">
                <p><button type="button" onclick="exportStaredPapers()" class="search-btn">Export</button></p>
                <p id="export-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-config" class="app-bar-content" style="display:none">
            <p>Magic Token:</p>
            <input id="magic-token" class="text-input single-line" type="text" placeholder="If unsure, ignore it.">
            <p>Kimi Language:</p>
            <select id="kimi-lang" name="kimi-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en">English</option>
            </select>
            <p>Desc Language:</p>
            <select id="desc-lang" name="desc-lang" class="text-input single-line">
                <option value="zh">中文</option>
                <option value="en" selected="">English</option>
            </select>
            <div class="submit">
                <p><button type="button" onclick="appConfig()" class="save-btn">Save</button></p>
                <p id="config-message" class="message"></p>
            </div>
        </div>
        <div id="app-bar-bug" class="app-bar-content" style="display:none">
            <p>Bug report? Issue submit? Please visit:</p>
            <p id="github-url"><strong>Github: </strong><a href="https://github.com/bojone/papers.cool" target="_blank">https://github.com/bojone/papers.cool</a></p>
            <p style="padding-top:15px">Hope you will enjoy it. </p>
            <p>More interesting features can be found at <a href="https://kexue.fm/" target="_blank">kexue.fm</a> and <a href="https://kimi.moonshot.cn/?ref=papers.cool" target="_blank">kimi.ai</a>.</p>
        </div>
        <a class="bar-app" href="https://papers.cool/" title="Home Page"><i class="fa fa-home"></i></a>
        <a class="bar-app" title="In-page Search" onclick="toggleApp(&#39;app-bar-search&#39;, this)"><i class="fa fa-search"></i></a>
        <a class="bar-app" title="Stared Papers" onclick="toggleApp(&#39;app-bar-star&#39;, this)"><i class="fa fa-star"></i></a>
        <a class="bar-app" title="Configuration" onclick="toggleApp(&#39;app-bar-config&#39;, this)"><i class="fa fa-cog"></i></a>
        <a class="bar-app" title="Bug Report" onclick="toggleApp(&#39;app-bar-bug&#39;, this)"><i class="fa fa-bug"></i></a>
    </div>
    <script src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/marked.min.js.download"></script>
    <script src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/flatpickr.min.js.download"></script>
    <script src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/translate.js.download"></script>
    <script src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/cool.js.download"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?606b976365dabacb1f69823d8de064ee";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<!-- Google tag (gtag.js) -->
<script async="" src="./Computer Vision and Pattern Recognition _ Cool Papers - Immersive Paper Discovery_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-214H31WLDF');
</script>



<deepl-input-controller><template shadowrootmode="open"><link rel="stylesheet" href="chrome-extension://cofdbpoegempjloogbagkncekinflcnj/build/content.css"><div dir="ltr"><div class="dl-input-translation-container svelte-95aucy"><div></div></div></div></template></deepl-input-controller><div id="yt_article_summary_widget_wrapper" class="yt_article_summary_widget_wrapper" style="display: none;">
        <div id="yt_article_summary_widget" class="yt_article_summary_widget"><svg style="filter: brightness(0.8);" width="32" height="32" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                <mask id="path-1-outside-1_3606_3145" maskUnits="userSpaceOnUse" x="1" y="1" width="22" height="22" fill="black">
                <rect fill="white" x="1" y="1" width="22" height="22"></rect>
                <path d="M20.6816 10.1843C20.9588 9.34066 21.0063 8.4399 20.8192 7.57245C20.6321 6.70499 20.217 5.90134 19.6157 5.24216C19.0143 4.58298 18.2478 4.09146 17.393 3.81692C16.5382 3.54238 15.6253 3.49449 14.7459 3.67805C14.1453 3.01747 13.379 2.52468 12.524 2.24931C11.669 1.97394 10.7555 1.92571 9.87559 2.10947C8.99568 2.29324 8.18039 2.70252 7.51181 3.29608C6.84323 3.88965 6.34499 4.64654 6.06725 5.49055C5.18642 5.67292 4.3699 6.08122 3.70003 6.67426C3.03017 7.26731 2.53064 8.02413 2.25182 8.86842C1.97299 9.71271 1.92474 10.6146 2.11192 11.4832C2.2991 12.3517 2.71509 13.1562 3.31795 13.8155C3.09309 14.4899 3.01633 15.2037 3.09278 15.9095C3.16924 16.6154 3.39716 17.2971 3.76139 17.9093C4.30169 18.8351 5.12567 19.568 6.11483 20.0027C7.104 20.4373 8.20738 20.5512 9.26631 20.328C9.74353 20.8568 10.3291 21.2796 10.9844 21.5684C11.6396 21.8571 12.3495 22.0053 13.0672 22.003C14.1516 22.003 15.2081 21.6635 16.0847 21.0334C16.9612 20.4034 17.6125 19.5152 17.9449 18.4968C18.649 18.3539 19.3141 18.0649 19.8962 17.6489C20.4784 17.233 20.9642 16.6997 21.3214 16.0843C21.8585 15.1598 22.0858 14.0915 21.9709 13.032C21.856 11.9724 21.4048 10.9758 20.6816 10.1843ZM13.0798 20.6968C12.191 20.6968 11.3302 20.3894 10.6473 19.828L10.7677 19.7593L14.8029 17.4593C14.9069 17.4047 14.9935 17.3225 15.0528 17.2221C15.1121 17.1216 15.1418 17.0068 15.1386 16.8905V11.2655L16.8427 12.2405C16.8517 12.2441 16.8594 12.2501 16.865 12.2579C16.8706 12.2656 16.8739 12.2748 16.8744 12.2843V16.9343C16.876 17.4289 16.7785 17.9189 16.5875 18.3761C16.3964 18.8333 16.1156 19.2488 15.7611 19.5985C15.4067 19.9482 14.9856 20.2253 14.5222 20.4138C14.0588 20.6023 13.5621 20.6984 13.0608 20.6968H13.0798ZM4.90165 17.2593C4.46164 16.5029 4.3026 15.6189 4.45188 14.7593L4.57224 14.828L8.60749 17.128C8.70379 17.1829 8.81303 17.2118 8.92423 17.2118C9.03543 17.2118 9.14467 17.1829 9.24097 17.128L14.1758 14.3218V16.253C14.1797 16.2608 14.1817 16.2694 14.1817 16.278C14.1817 16.2867 14.1797 16.2953 14.1758 16.303L10.0962 18.628C9.66403 18.8748 9.18685 19.0352 8.69188 19.0999C8.19692 19.1647 7.69387 19.1326 7.21148 19.0055C6.72909 18.8784 6.27681 18.6587 5.88048 18.3591C5.48415 18.0595 5.15154 17.6858 4.90165 17.2593ZM3.83741 8.5843C4.28764 7.82089 4.99655 7.23878 5.83919 6.94055V11.6718C5.83595 11.7857 5.86434 11.8983 5.92128 11.9975C5.97823 12.0966 6.06156 12.1785 6.16227 12.2343L11.0717 15.028L9.36766 16.003C9.34918 16.0092 9.32914 16.0092 9.31065 16.003L5.23106 13.678C4.36041 13.1812 3.72487 12.3642 3.46364 11.4059C3.20242 10.4476 3.33682 9.42624 3.83741 8.56555V8.5843ZM17.8563 11.7968L12.9278 8.9718L14.6319 8.00305C14.6403 7.99741 14.6502 7.99439 14.6604 7.99439C14.6705 7.99439 14.6805 7.99741 14.6889 8.00305L18.7685 10.328C19.3915 10.684 19.8992 11.2072 20.2325 11.8368C20.5659 12.4664 20.7111 13.1764 20.6514 13.8843C20.5916 14.5921 20.3294 15.2687 19.8951 15.8352C19.4608 16.4017 18.8724 16.8349 18.1983 17.0843V12.353C18.1946 12.2391 18.1612 12.1281 18.1013 12.0306C18.0414 11.9332 17.957 11.8527 17.8563 11.7968ZM19.554 9.2968L19.4336 9.2218L15.4047 6.9093C15.3047 6.84846 15.1896 6.81624 15.0721 6.81624C14.9547 6.81624 14.8395 6.84846 14.7396 6.9093L9.8111 9.71555V7.75305C9.8061 7.7445 9.80346 7.7348 9.80346 7.72492C9.80346 7.71505 9.8061 7.70535 9.8111 7.6968L13.897 5.37805C14.5222 5.02257 15.2371 4.85003 15.958 4.88059C16.6789 4.91115 17.3762 5.14356 17.9682 5.55064C18.5601 5.95772 19.0225 6.52265 19.301 7.17939C19.5796 7.83614 19.663 8.55755 19.5413 9.2593L19.554 9.2968ZM8.87989 12.7218L7.1695 11.753C7.15339 11.7405 7.1422 11.7228 7.13782 11.703V7.06555C7.13785 6.35289 7.34371 5.65499 7.73128 5.0536C8.11885 4.45222 8.67209 3.97224 9.32619 3.6699C9.98029 3.36756 10.7082 3.25537 11.4246 3.34647C12.141 3.43757 12.8162 3.7282 13.3712 4.1843L13.2636 4.25305L9.21563 6.55305C9.11158 6.60765 9.02504 6.68981 8.96573 6.79029C8.90642 6.89076 8.87669 7.00557 8.87989 7.1218V12.7218ZM9.80476 10.753L11.9966 9.50305L14.1948 10.753V13.253L11.9966 14.503L9.79843 13.253L9.80476 10.753Z"></path>
                </mask>
                <path d="M20.6816 10.1843C20.9588 9.34066 21.0063 8.4399 20.8192 7.57245C20.6321 6.70499 20.217 5.90134 19.6157 5.24216C19.0143 4.58298 18.2478 4.09146 17.393 3.81692C16.5382 3.54238 15.6253 3.49449 14.7459 3.67805C14.1453 3.01747 13.379 2.52468 12.524 2.24931C11.669 1.97394 10.7555 1.92571 9.87559 2.10947C8.99568 2.29324 8.18039 2.70252 7.51181 3.29608C6.84323 3.88965 6.34499 4.64654 6.06725 5.49055C5.18642 5.67292 4.3699 6.08122 3.70003 6.67426C3.03017 7.26731 2.53064 8.02413 2.25182 8.86842C1.97299 9.71271 1.92474 10.6146 2.11192 11.4832C2.2991 12.3517 2.71509 13.1562 3.31795 13.8155C3.09309 14.4899 3.01633 15.2037 3.09278 15.9095C3.16924 16.6154 3.39716 17.2971 3.76139 17.9093C4.30169 18.8351 5.12567 19.568 6.11483 20.0027C7.104 20.4373 8.20738 20.5512 9.26631 20.328C9.74353 20.8568 10.3291 21.2796 10.9844 21.5684C11.6396 21.8571 12.3495 22.0053 13.0672 22.003C14.1516 22.003 15.2081 21.6635 16.0847 21.0334C16.9612 20.4034 17.6125 19.5152 17.9449 18.4968C18.649 18.3539 19.3141 18.0649 19.8962 17.6489C20.4784 17.233 20.9642 16.6997 21.3214 16.0843C21.8585 15.1598 22.0858 14.0915 21.9709 13.032C21.856 11.9724 21.4048 10.9758 20.6816 10.1843ZM13.0798 20.6968C12.191 20.6968 11.3302 20.3894 10.6473 19.828L10.7677 19.7593L14.8029 17.4593C14.9069 17.4047 14.9935 17.3225 15.0528 17.2221C15.1121 17.1216 15.1418 17.0068 15.1386 16.8905V11.2655L16.8427 12.2405C16.8517 12.2441 16.8594 12.2501 16.865 12.2579C16.8706 12.2656 16.8739 12.2748 16.8744 12.2843V16.9343C16.876 17.4289 16.7785 17.9189 16.5875 18.3761C16.3964 18.8333 16.1156 19.2488 15.7611 19.5985C15.4067 19.9482 14.9856 20.2253 14.5222 20.4138C14.0588 20.6023 13.5621 20.6984 13.0608 20.6968H13.0798ZM4.90165 17.2593C4.46164 16.5029 4.3026 15.6189 4.45188 14.7593L4.57224 14.828L8.60749 17.128C8.70379 17.1829 8.81303 17.2118 8.92423 17.2118C9.03543 17.2118 9.14467 17.1829 9.24097 17.128L14.1758 14.3218V16.253C14.1797 16.2608 14.1817 16.2694 14.1817 16.278C14.1817 16.2867 14.1797 16.2953 14.1758 16.303L10.0962 18.628C9.66403 18.8748 9.18685 19.0352 8.69188 19.0999C8.19692 19.1647 7.69387 19.1326 7.21148 19.0055C6.72909 18.8784 6.27681 18.6587 5.88048 18.3591C5.48415 18.0595 5.15154 17.6858 4.90165 17.2593ZM3.83741 8.5843C4.28764 7.82089 4.99655 7.23878 5.83919 6.94055V11.6718C5.83595 11.7857 5.86434 11.8983 5.92128 11.9975C5.97823 12.0966 6.06156 12.1785 6.16227 12.2343L11.0717 15.028L9.36766 16.003C9.34918 16.0092 9.32914 16.0092 9.31065 16.003L5.23106 13.678C4.36041 13.1812 3.72487 12.3642 3.46364 11.4059C3.20242 10.4476 3.33682 9.42624 3.83741 8.56555V8.5843ZM17.8563 11.7968L12.9278 8.9718L14.6319 8.00305C14.6403 7.99741 14.6502 7.99439 14.6604 7.99439C14.6705 7.99439 14.6805 7.99741 14.6889 8.00305L18.7685 10.328C19.3915 10.684 19.8992 11.2072 20.2325 11.8368C20.5659 12.4664 20.7111 13.1764 20.6514 13.8843C20.5916 14.5921 20.3294 15.2687 19.8951 15.8352C19.4608 16.4017 18.8724 16.8349 18.1983 17.0843V12.353C18.1946 12.2391 18.1612 12.1281 18.1013 12.0306C18.0414 11.9332 17.957 11.8527 17.8563 11.7968ZM19.554 9.2968L19.4336 9.2218L15.4047 6.9093C15.3047 6.84846 15.1896 6.81624 15.0721 6.81624C14.9547 6.81624 14.8395 6.84846 14.7396 6.9093L9.8111 9.71555V7.75305C9.8061 7.7445 9.80346 7.7348 9.80346 7.72492C9.80346 7.71505 9.8061 7.70535 9.8111 7.6968L13.897 5.37805C14.5222 5.02257 15.2371 4.85003 15.958 4.88059C16.6789 4.91115 17.3762 5.14356 17.9682 5.55064C18.5601 5.95772 19.0225 6.52265 19.301 7.17939C19.5796 7.83614 19.663 8.55755 19.5413 9.2593L19.554 9.2968ZM8.87989 12.7218L7.1695 11.753C7.15339 11.7405 7.1422 11.7228 7.13782 11.703V7.06555C7.13785 6.35289 7.34371 5.65499 7.73128 5.0536C8.11885 4.45222 8.67209 3.97224 9.32619 3.6699C9.98029 3.36756 10.7082 3.25537 11.4246 3.34647C12.141 3.43757 12.8162 3.7282 13.3712 4.1843L13.2636 4.25305L9.21563 6.55305C9.11158 6.60765 9.02504 6.68981 8.96573 6.79029C8.90642 6.89076 8.87669 7.00557 8.87989 7.1218V12.7218ZM9.80476 10.753L11.9966 9.50305L14.1948 10.753V13.253L11.9966 14.503L9.79843 13.253L9.80476 10.753Z" fill="#828282"></path>
                <path d="M20.6816 10.1843C20.9588 9.34066 21.0063 8.4399 20.8192 7.57245C20.6321 6.70499 20.217 5.90134 19.6157 5.24216C19.0143 4.58298 18.2478 4.09146 17.393 3.81692C16.5382 3.54238 15.6253 3.49449 14.7459 3.67805C14.1453 3.01747 13.379 2.52468 12.524 2.24931C11.669 1.97394 10.7555 1.92571 9.87559 2.10947C8.99568 2.29324 8.18039 2.70252 7.51181 3.29608C6.84323 3.88965 6.34499 4.64654 6.06725 5.49055C5.18642 5.67292 4.3699 6.08122 3.70003 6.67426C3.03017 7.26731 2.53064 8.02413 2.25182 8.86842C1.97299 9.71271 1.92474 10.6146 2.11192 11.4832C2.2991 12.3517 2.71509 13.1562 3.31795 13.8155C3.09309 14.4899 3.01633 15.2037 3.09278 15.9095C3.16924 16.6154 3.39716 17.2971 3.76139 17.9093C4.30169 18.8351 5.12567 19.568 6.11483 20.0027C7.104 20.4373 8.20738 20.5512 9.26631 20.328C9.74353 20.8568 10.3291 21.2796 10.9844 21.5684C11.6396 21.8571 12.3495 22.0053 13.0672 22.003C14.1516 22.003 15.2081 21.6635 16.0847 21.0334C16.9612 20.4034 17.6125 19.5152 17.9449 18.4968C18.649 18.3539 19.3141 18.0649 19.8962 17.6489C20.4784 17.233 20.9642 16.6997 21.3214 16.0843C21.8585 15.1598 22.0858 14.0915 21.9709 13.032C21.856 11.9724 21.4048 10.9758 20.6816 10.1843ZM13.0798 20.6968C12.191 20.6968 11.3302 20.3894 10.6473 19.828L10.7677 19.7593L14.8029 17.4593C14.9069 17.4047 14.9935 17.3225 15.0528 17.2221C15.1121 17.1216 15.1418 17.0068 15.1386 16.8905V11.2655L16.8427 12.2405C16.8517 12.2441 16.8594 12.2501 16.865 12.2579C16.8706 12.2656 16.8739 12.2748 16.8744 12.2843V16.9343C16.876 17.4289 16.7785 17.9189 16.5875 18.3761C16.3964 18.8333 16.1156 19.2488 15.7611 19.5985C15.4067 19.9482 14.9856 20.2253 14.5222 20.4138C14.0588 20.6023 13.5621 20.6984 13.0608 20.6968H13.0798ZM4.90165 17.2593C4.46164 16.5029 4.3026 15.6189 4.45188 14.7593L4.57224 14.828L8.60749 17.128C8.70379 17.1829 8.81303 17.2118 8.92423 17.2118C9.03543 17.2118 9.14467 17.1829 9.24097 17.128L14.1758 14.3218V16.253C14.1797 16.2608 14.1817 16.2694 14.1817 16.278C14.1817 16.2867 14.1797 16.2953 14.1758 16.303L10.0962 18.628C9.66403 18.8748 9.18685 19.0352 8.69188 19.0999C8.19692 19.1647 7.69387 19.1326 7.21148 19.0055C6.72909 18.8784 6.27681 18.6587 5.88048 18.3591C5.48415 18.0595 5.15154 17.6858 4.90165 17.2593ZM3.83741 8.5843C4.28764 7.82089 4.99655 7.23878 5.83919 6.94055V11.6718C5.83595 11.7857 5.86434 11.8983 5.92128 11.9975C5.97823 12.0966 6.06156 12.1785 6.16227 12.2343L11.0717 15.028L9.36766 16.003C9.34918 16.0092 9.32914 16.0092 9.31065 16.003L5.23106 13.678C4.36041 13.1812 3.72487 12.3642 3.46364 11.4059C3.20242 10.4476 3.33682 9.42624 3.83741 8.56555V8.5843ZM17.8563 11.7968L12.9278 8.9718L14.6319 8.00305C14.6403 7.99741 14.6502 7.99439 14.6604 7.99439C14.6705 7.99439 14.6805 7.99741 14.6889 8.00305L18.7685 10.328C19.3915 10.684 19.8992 11.2072 20.2325 11.8368C20.5659 12.4664 20.7111 13.1764 20.6514 13.8843C20.5916 14.5921 20.3294 15.2687 19.8951 15.8352C19.4608 16.4017 18.8724 16.8349 18.1983 17.0843V12.353C18.1946 12.2391 18.1612 12.1281 18.1013 12.0306C18.0414 11.9332 17.957 11.8527 17.8563 11.7968ZM19.554 9.2968L19.4336 9.2218L15.4047 6.9093C15.3047 6.84846 15.1896 6.81624 15.0721 6.81624C14.9547 6.81624 14.8395 6.84846 14.7396 6.9093L9.8111 9.71555V7.75305C9.8061 7.7445 9.80346 7.7348 9.80346 7.72492C9.80346 7.71505 9.8061 7.70535 9.8111 7.6968L13.897 5.37805C14.5222 5.02257 15.2371 4.85003 15.958 4.88059C16.6789 4.91115 17.3762 5.14356 17.9682 5.55064C18.5601 5.95772 19.0225 6.52265 19.301 7.17939C19.5796 7.83614 19.663 8.55755 19.5413 9.2593L19.554 9.2968ZM8.87989 12.7218L7.1695 11.753C7.15339 11.7405 7.1422 11.7228 7.13782 11.703V7.06555C7.13785 6.35289 7.34371 5.65499 7.73128 5.0536C8.11885 4.45222 8.67209 3.97224 9.32619 3.6699C9.98029 3.36756 10.7082 3.25537 11.4246 3.34647C12.141 3.43757 12.8162 3.7282 13.3712 4.1843L13.2636 4.25305L9.21563 6.55305C9.11158 6.60765 9.02504 6.68981 8.96573 6.79029C8.90642 6.89076 8.87669 7.00557 8.87989 7.1218V12.7218ZM9.80476 10.753L11.9966 9.50305L14.1948 10.753V13.253L11.9966 14.503L9.79843 13.253L9.80476 10.753Z" stroke="#828282" stroke-width="0.2" mask="url(#path-1-outside-1_3606_3145)"></path>
            </svg></div>
        <div id="yt_article_summary_close_button" class="yt_article_summary_close_button">×</div>
    </div></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>@charset "UTF-8";
/*!
 * Pico.css v1.5.6 (https://picocss.com)
 * Copyright 2019-2022 - Licensed under MIT
 */
/**
 * Theme: default
 */
#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 0.25rem;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 1rem;
  --typography-spacing-vertical: 1.5rem;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 0.75rem;
  --form-element-spacing-horizontal: 1rem;
  --nav-element-spacing-vertical: 1rem;
  --nav-element-spacing-horizontal: 0.5rem;
  --nav-link-spacing-vertical: 0.5rem;
  --nav-link-spacing-horizontal: 0.5rem;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(0.25rem);
}
@media (min-width: 576px) {
  #mount {
    --font-size: 17px;
  }
}
@media (min-width: 768px) {
  #mount {
    --font-size: 18px;
  }
}
@media (min-width: 992px) {
  #mount {
    --font-size: 19px;
  }
}
@media (min-width: 1200px) {
  #mount {
    --font-size: 20px;
  }
}

@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3);
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3.5);
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 4);
  }
}

@media (min-width: 576px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}
@media (min-width: 992px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.75);
  }
}
@media (min-width: 1200px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 2);
  }
}

dialog > article {
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
}
@media (min-width: 576px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 3);
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}

a {
  --text-decoration: none;
}
a.secondary,
a.contrast {
  --text-decoration: underline;
}

small {
  --font-size: 0.875em;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  --font-weight: 700;
}

h1 {
  --font-size: 2rem;
  --typography-spacing-vertical: 3rem;
}

h2 {
  --font-size: 1.75rem;
  --typography-spacing-vertical: 2.625rem;
}

h3 {
  --font-size: 1.5rem;
  --typography-spacing-vertical: 2.25rem;
}

h4 {
  --font-size: 1.25rem;
  --typography-spacing-vertical: 1.874rem;
}

h5 {
  --font-size: 1.125rem;
  --typography-spacing-vertical: 1.6875rem;
}

[type="checkbox"],
[type="radio"] {
  --border-width: 2px;
}

[type="checkbox"][role="switch"] {
  --border-width: 3px;
}

thead th,
thead td,
tfoot th,
tfoot td {
  --border-width: 3px;
}

:not(thead, tfoot) > * > td {
  --font-size: 0.875em;
}

pre,
code,
kbd,
samp {
  --font-family: "Menlo", "Consolas", "Roboto Mono", "Ubuntu Monospace",
    "Noto Mono", "Oxygen Mono", "Liberation Mono", monospace,
    "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
}

kbd {
  --font-weight: bolder;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --background-color: #fff;
  --background-light-green: #F5F7F9;
  --color: hsl(205deg, 20%, 32%);
  --h1-color: hsl(205deg, 30%, 15%);
  --h2-color: #24333e;
  --h3-color: hsl(205deg, 25%, 23%);
  --h4-color: #374956;
  --h5-color: hsl(205deg, 20%, 32%);
  --h6-color: #4d606d;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: hsl(205deg, 20%, 94%);
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 90%, 32%);
  --primary-focus: rgba(16, 149, 193, 0.125);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 20%, 32%);
  --secondary-focus: rgba(89, 107, 120, 0.125);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 30%, 15%);
  --contrast-hover: #000;
  --contrast-focus: rgba(89, 107, 120, 0.125);
  --contrast-inverse: #fff;
  --mark-background-color: #fff2ca;
  --mark-color: #543a26;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: transparent;
  --form-element-border-color: hsl(205deg, 14%, 68%);
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: transparent;
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 18%, 86%);
  --form-element-disabled-border-color: hsl(205deg, 14%, 68%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #c62828;
  --form-element-invalid-active-border-color: #d32f2f;
  --form-element-invalid-focus-color: rgba(211, 47, 47, 0.125);
  --form-element-valid-border-color: #388e3c;
  --form-element-valid-active-border-color: #43a047;
  --form-element-valid-focus-color: rgba(67, 160, 71, 0.125);
  --switch-background-color: hsl(205deg, 16%, 77%);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: hsl(205deg, 18%, 86%);
  --range-active-border-color: hsl(205deg, 16%, 77%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: #f6f8f9;
  --code-background-color: hsl(205deg, 20%, 94%);
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 40%, 50%);
  --code-property-color: hsl(185deg, 40%, 40%);
  --code-value-color: hsl(40deg, 20%, 50%);
  --code-comment-color: hsl(205deg, 14%, 68%);
  --accordion-border-color: var(--muted-border-color);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: var(--background-color);
  --card-border-color: var(--muted-border-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(27, 40, 50, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(27, 40, 50, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(27, 40, 50, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(27, 40, 50, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(27, 40, 50, 0.04302),
    0.5rem 1rem 6rem rgba(27, 40, 50, 0.06),
    0 0 0 0.0625rem rgba(27, 40, 50, 0.015);
  --card-sectionning-background-color: #fbfbfc;
  --dropdown-background-color: #fbfbfc;
  --dropdown-border-color: #e1e6eb;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: hsl(205deg, 20%, 94%);
  --modal-overlay-background-color: rgba(213, 220, 226, 0.7);
  --progress-background-color: hsl(205deg, 18%, 86%);
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(198, 40, 40)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(56, 142, 60)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  --float-ball-more-button-border-color: #F6F6F6;
  --float-ball-more-button-background-color: #FCFCFC;
  --float-ball-more-button-svg-color: #6C6F73;
  color-scheme: light;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --background-color: #11191f;
    --background-light-green: #141e26;
    --color: hsl(205deg, 16%, 77%);
    --h1-color: hsl(205deg, 20%, 94%);
    --h2-color: #e1e6eb;
    --h3-color: hsl(205deg, 18%, 86%);
    --h4-color: #c8d1d8;
    --h5-color: hsl(205deg, 16%, 77%);
    --h6-color: #afbbc4;
    --muted-color: hsl(205deg, 10%, 50%);
    --muted-border-color: #1f2d38;
    --primary: hsl(195deg, 85%, 41%);
    --primary-hover: hsl(195deg, 80%, 50%);
    --primary-focus: rgba(16, 149, 193, 0.25);
    --primary-inverse: #fff;
    --secondary: hsl(205deg, 15%, 41%);
    --secondary-hover: hsl(205deg, 10%, 50%);
    --secondary-focus: rgba(115, 130, 140, 0.25);
    --secondary-inverse: #fff;
    --contrast: hsl(205deg, 20%, 94%);
    --contrast-hover: #fff;
    --contrast-focus: rgba(115, 130, 140, 0.25);
    --contrast-inverse: #000;
    --mark-background-color: #d1c284;
    --mark-color: #11191f;
    --ins-color: #388e3c;
    --del-color: #c62828;
    --blockquote-border-color: var(--muted-border-color);
    --blockquote-footer-color: var(--muted-color);
    --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --form-element-background-color: #11191f;
    --form-element-border-color: #374956;
    --form-element-color: var(--color);
    --form-element-placeholder-color: var(--muted-color);
    --form-element-active-background-color: var(
      --form-element-background-color
    );
    --form-element-active-border-color: var(--primary);
    --form-element-focus-color: var(--primary-focus);
    --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
    --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
    --form-element-disabled-opacity: 0.5;
    --form-element-invalid-border-color: #b71c1c;
    --form-element-invalid-active-border-color: #c62828;
    --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
    --form-element-valid-border-color: #2e7d32;
    --form-element-valid-active-border-color: #388e3c;
    --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
    --switch-background-color: #374956;
    --switch-color: var(--primary-inverse);
    --switch-checked-background-color: var(--primary);
    --range-border-color: #24333e;
    --range-active-border-color: hsl(205deg, 25%, 23%);
    --range-thumb-border-color: var(--background-color);
    --range-thumb-color: var(--secondary);
    --range-thumb-hover-color: var(--secondary-hover);
    --range-thumb-active-color: var(--primary);
    --table-border-color: var(--muted-border-color);
    --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
    --code-background-color: #18232c;
    --code-color: var(--muted-color);
    --code-kbd-background-color: var(--contrast);
    --code-kbd-color: var(--contrast-inverse);
    --code-tag-color: hsl(330deg, 30%, 50%);
    --code-property-color: hsl(185deg, 30%, 50%);
    --code-value-color: hsl(40deg, 10%, 50%);
    --code-comment-color: #4d606d;
    --accordion-border-color: var(--muted-border-color);
    --accordion-active-summary-color: var(--primary);
    --accordion-close-summary-color: var(--color);
    --accordion-open-summary-color: var(--muted-color);
    --card-background-color: #141e26;
    --card-border-color: var(--card-background-color);
    --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
      0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
      0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
      0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
      0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
      0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
    --card-sectionning-background-color: #18232c;
    --dropdown-background-color: hsl(205deg, 30%, 15%);
    --dropdown-border-color: #24333e;
    --dropdown-box-shadow: var(--card-box-shadow);
    --dropdown-color: var(--color);
    --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
    --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
    --progress-background-color: #24333e;
    --progress-color: var(--primary);
    --loading-spinner-opacity: 0.5;
    --tooltip-background-color: var(--contrast);
    --tooltip-color: var(--contrast-inverse);
    --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
    --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
    --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
    --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
    --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
    --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
    color-scheme: dark;
  }
}
[data-theme="dark"] {
  --background-color: #11191f;
  --background-light-green: #141e26;
  --color: hsl(205deg, 16%, 77%);
  --h1-color: hsl(205deg, 20%, 94%);
  --h2-color: #e1e6eb;
  --h3-color: hsl(205deg, 18%, 86%);
  --h4-color: #c8d1d8;
  --h5-color: hsl(205deg, 16%, 77%);
  --h6-color: #afbbc4;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: #1f2d38;
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 80%, 50%);
  --primary-focus: rgba(16, 149, 193, 0.25);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 10%, 50%);
  --secondary-focus: rgba(115, 130, 140, 0.25);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 20%, 94%);
  --contrast-hover: #fff;
  --contrast-focus: rgba(115, 130, 140, 0.25);
  --contrast-inverse: #000;
  --mark-background-color: #d1c284;
  --mark-color: #11191f;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: #11191f;
  --form-element-border-color: #374956;
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: var(--form-element-background-color);
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
  --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #b71c1c;
  --form-element-invalid-active-border-color: #c62828;
  --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
  --form-element-valid-border-color: #2e7d32;
  --form-element-valid-active-border-color: #388e3c;
  --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
  --switch-background-color: #374956;
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: #24333e;
  --range-active-border-color: hsl(205deg, 25%, 23%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
  --code-background-color: #18232c;
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 30%, 50%);
  --code-property-color: hsl(185deg, 30%, 50%);
  --code-value-color: hsl(40deg, 10%, 50%);
  --code-comment-color: #4d606d;
  --accordion-border-color: var(--muted-border-color);
  --accordion-active-summary-color: var(--primary);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: #141e26;
  --card-border-color: var(--card-background-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
    0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
  --card-sectionning-background-color: #18232c;
  --dropdown-background-color: hsl(205deg, 30%, 15%);
  --dropdown-border-color: #24333e;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
  --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
  --progress-background-color: #24333e;
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,PHN2ZyB3aWR0aD0nMjInIGhlaWdodD0nMjInIHZpZXdCb3g9JzAgMCAyMiAyMicgZmlsbD0nbm9uZScgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJz48cGF0aCBkPSdNMTcuOTM0OCA3LjY0ODQ0QzE5Ljg5NDEgNy42NDg0NCAyMS40ODU1IDYuMDU0NjkgMjEuNDg1NSA0LjA5NzY2QzIxLjQ4NTUgMi4xNDA2MiAxOS44OTE4IDAuNTQ2ODc1IDE3LjkzNDggMC41NDY4NzVDMTUuOTc1NCAwLjU0Njg3NSAxNC4zODQgMi4xNDA2MiAxNC4zODQgNC4wOTc2NkMxNC4zODQgNC4yOTkyMiAxNC40MDA0IDQuNDkzNzUgMTQuNDMzMiA0LjY4NTk0TDYuMzIzODMgOC4zNTM5MUM1LjcwOTc3IDcuODQ1MzEgNC45MjIyNyA3LjU0MDYyIDQuMDY0NDUgNy41NDA2MkMyLjEwNTA4IDcuNTQwNjIgMC41MTM2NzIgOS4xMzQzOCAwLjUxMzY3MiAxMS4wOTE0QzAuNTEzNjcyIDEzLjA0ODQgMi4xMDc0MiAxNC42NDIyIDQuMDY0NDUgMTQuNjQyMkM0LjgzMzIgMTQuNjQyMiA1LjU0NTcgMTQuMzk2MSA2LjEyNjk1IDEzLjk4MTNMMTEuNDk0MSAxNi45OTUzQzExLjQxNjggMTcuMjg1OSAxMS4zNzcgMTcuNTg4MyAxMS4zNzcgMTcuOTAyM0MxMS4zNzcgMTkuODYxNyAxMi45NzA3IDIxLjQ1MzEgMTQuOTI3NyAyMS40NTMxQzE2Ljg4NzEgMjEuNDUzMSAxOC40Nzg1IDE5Ljg1OTQgMTguNDc4NSAxNy45MDIzQzE4LjQ3ODUgMTUuOTQzIDE2Ljg4NDggMTQuMzUxNiAxNC45Mjc3IDE0LjM1MTZDMTMuOTU3NCAxNC4zNTE2IDEzLjA3ODUgMTQuNzQzIDEyLjQzNjMgMTUuMzczNEw3LjMyMjI3IDEyLjUwNDdDNy41MDk3NyAxMi4wNzExIDcuNjE1MjMgMTEuNTk1MyA3LjYxNTIzIDExLjA5MzhDNy42MTUyMyAxMC42ODEyIDcuNTQ0OTIgMTAuMjg3NSA3LjQxNjAyIDkuOTE5NTNMMTUuMjIzIDYuMzg3NUMxNS44NzQ2IDcuMTU2MjUgMTYuODQ5NiA3LjY0ODQ0IDE3LjkzNDggNy42NDg0NFpNNC4wNjQ0NSAxMi43Njk1QzMuMTQxMDIgMTIuNzY5NSAyLjM4ODY3IDEyLjAxNzIgMi4zODg2NyAxMS4wOTM4QzIuMzg4NjcgMTAuMTcwMyAzLjE0MTAyIDkuNDE3OTcgNC4wNjQ0NSA5LjQxNzk3QzQuOTg3ODkgOS40MTc5NyA1Ljc0MDIzIDEwLjE3MDMgNS43NDAyMyAxMS4wOTM4QzUuNzQwMjMgMTIuMDE3MiA0Ljk4Nzg5IDEyLjc2OTUgNC4wNjQ0NSAxMi43Njk1Wk0xNC45Mjc3IDE2LjIyNjZDMTUuODUxMiAxNi4yMjY2IDE2LjYwMzUgMTYuOTc4OSAxNi42MDM1IDE3LjkwMjNDMTYuNjAzNSAxOC44MjU4IDE1Ljg1MTIgMTkuNTc4MSAxNC45Mjc3IDE5LjU3ODFDMTQuMDA0MyAxOS41NzgxIDEzLjI1MiAxOC44MjU4IDEzLjI1MiAxNy45MDIzQzEzLjI1MiAxNi45Nzg5IDE0LjAwMiAxNi4yMjY2IDE0LjkyNzcgMTYuMjI2NlpNMTcuOTM0OCAyLjQxOTUzQzE4Ljg1ODIgMi40MTk1MyAxOS42MTA1IDMuMTcxODcgMTkuNjEwNSA0LjA5NTMxQzE5LjYxMDUgNS4wMTg3NSAxOC44NTgyIDUuNzcxMDkgMTcuOTM0OCA1Ljc3MTA5QzE3LjAxMTMgNS43NzEwOSAxNi4yNTkgNS4wMTg3NSAxNi4yNTkgNC4wOTUzMUMxNi4yNTkgMy4xNzE4NyAxNy4wMTEzIDIuNDE5NTMgMTcuOTM0OCAyLjQxOTUzWicgZmlsbD0nI0I2QjZCNicvPjwvc3ZnPiA=");
  color-scheme: dark;
}

progress,
[type="checkbox"],
[type="radio"],
[type="range"] {
  accent-color: var(--primary);
}

/**
 * Document
 * Content-box & Responsive typography
 */
*,
*::before,
*::after {
  box-sizing: border-box;
  background-repeat: no-repeat;
}

::before,
::after {
  text-decoration: inherit;
  vertical-align: inherit;
}

:where(#mount) {
  -webkit-tap-highlight-color: transparent;
  -webkit-text-size-adjust: 100%;
  -moz-text-size-adjust: 100%;
  text-size-adjust: 100%;
  background-color: var(--background-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  line-height: var(--line-height);
  font-family: var(--font-family);
  text-rendering: optimizeLegibility;
  overflow-wrap: break-word;
  cursor: default;
  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;
}

/**
 * Sectioning
 * Container and responsive spacings for header, main, footer
 */
main {
  display: block;
}

#mount {
  width: 100%;
  margin: 0;
}
#mount > header,
#mount > main,
#mount > footer {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
}
@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    max-width: 1130px;
  }
}

/**
* Container
*/
.container,
.container-fluid {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding-right: var(--spacing);
  padding-left: var(--spacing);
}

@media (min-width: 576px) {
  .container {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  .container {
    max-width: 700px;
  }
}
@media (min-width: 992px) {
  .container {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  .container {
    max-width: 1130px;
  }
}

/**
 * Section
 * Responsive spacings for section
 */
section {
  margin-bottom: var(--block-spacing-vertical);
}

/**
* Grid
* Minimal grid system with auto-layout columns
*/
.grid {
  grid-column-gap: var(--grid-spacing-horizontal);
  grid-row-gap: var(--grid-spacing-vertical);
  display: grid;
  grid-template-columns: 1fr;
  margin: 0;
}
@media (min-width: 992px) {
  .grid {
    grid-template-columns: repeat(auto-fit, minmax(0%, 1fr));
  }
}
.grid > * {
  min-width: 0;
}

/**
 * Horizontal scroller (<figure>)
 */
figure {
  display: block;
  margin: 0;
  padding: 0;
  overflow-x: auto;
}
figure figcaption {
  padding: calc(var(--spacing) * 0.5) 0;
  color: var(--muted-color);
}

/**
 * Typography
 */
b,
strong {
  font-weight: bolder;
}

sub,
sup {
  position: relative;
  font-size: 0.75em;
  line-height: 0;
  vertical-align: baseline;
}

sub {
  bottom: -0.25em;
}

sup {
  top: -0.5em;
}

address,
blockquote,
dl,
figure,
form,
ol,
p,
pre,
table,
ul {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: var(--font-size);
}

a,
[role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
a:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}
a:focus,
[role="link"]:focus {
  --background-color: var(--primary-focus);
}
a.secondary,
[role="link"].secondary {
  --color: var(--secondary);
}
a.secondary:is([aria-current], :hover, :active, :focus),
[role="link"].secondary:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}
a.secondary:focus,
[role="link"].secondary:focus {
  --background-color: var(--secondary-focus);
}
a.contrast,
[role="link"].contrast {
  --color: var(--contrast);
}
a.contrast:is([aria-current], :hover, :active, :focus),
[role="link"].contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}
a.contrast:focus,
[role="link"].contrast:focus {
  --background-color: var(--contrast-focus);
}

h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  font-family: var(--font-family);
}

h1 {
  --color: var(--h1-color);
}

h2 {
  --color: var(--h2-color);
}

h3 {
  --color: var(--h3-color);
}

h4 {
  --color: var(--h4-color);
}

h5 {
  --color: var(--h5-color);
}

h6 {
  --color: var(--h6-color);
}

:where(address, blockquote, dl, figure, form, ol, p, pre, table, ul)
  ~ :is(h1, h2, h3, h4, h5, h6) {
  margin-top: var(--typography-spacing-vertical);
}

hgroup,
.headings {
  margin-bottom: var(--typography-spacing-vertical);
}
hgroup > *,
.headings > * {
  margin-bottom: 0;
}
hgroup > *:last-child,
.headings > *:last-child {
  --color: var(--muted-color);
  --font-weight: unset;
  font-size: 1rem;
  font-family: unset;
}

p {
  margin-bottom: var(--typography-spacing-vertical);
}

small {
  font-size: var(--font-size);
}

:where(dl, ol, ul) {
  padding-right: 0;
  padding-left: var(--spacing);
  -webkit-padding-start: var(--spacing);
  padding-inline-start: var(--spacing);
  -webkit-padding-end: 0;
  padding-inline-end: 0;
}
:where(dl, ol, ul) li {
  margin-bottom: calc(var(--typography-spacing-vertical) * 0.25);
}

:where(dl, ol, ul) :is(dl, ol, ul) {
  margin: 0;
  margin-top: calc(var(--typography-spacing-vertical) * 0.25);
}

ul li {
  list-style: square;
}

mark {
  padding: 0.125rem 0.25rem;
  background-color: var(--mark-background-color);
  color: var(--mark-color);
  vertical-align: baseline;
}

blockquote {
  display: block;
  margin: var(--typography-spacing-vertical) 0;
  padding: var(--spacing);
  border-right: none;
  border-left: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-start: 0.25rem solid var(--blockquote-border-color);
  border-inline-start: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-end: none;
  border-inline-end: none;
}
blockquote footer {
  margin-top: calc(var(--typography-spacing-vertical) * 0.5);
  color: var(--blockquote-footer-color);
}

abbr[title] {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}

ins {
  color: var(--ins-color);
  text-decoration: none;
}

del {
  color: var(--del-color);
}

::-moz-selection {
  background-color: var(--primary-focus);
}

::selection {
  background-color: var(--primary-focus);
}

/**
 * Embedded content
 */
:where(audio, canvas, iframe, img, svg, video) {
  vertical-align: middle;
}

audio,
video {
  display: inline-block;
}

audio:not([controls]) {
  display: none;
  height: 0;
}

:where(iframe) {
  border-style: none;
}

img {
  max-width: 100%;
  height: auto;
  border-style: none;
}

:where(svg:not([fill])) {
  fill: currentColor;
}

svg:not(#mount) {
  overflow: hidden;
}

/**
 * Button
 */
button {
  margin: 0;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

button,
[type="button"],
[type="reset"],
[type="submit"] {
  -webkit-appearance: button;
}

button {
  display: block;
  width: 100%;
  margin-bottom: var(--spacing);
}

[role="button"] {
  display: inline-block;
  text-decoration: none;
}

button,
input[type="submit"],
input[type="button"],
input[type="reset"],
[role="button"] {
  --background-color: var(--primary);
  --border-color: var(--primary);
  --color: var(--primary-inverse);
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
button:is([aria-current], :hover, :active, :focus),
input[type="submit"]:is([aria-current], :hover, :active, :focus),
input[type="button"]:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus),
[role="button"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--primary-hover);
  --border-color: var(--primary-hover);
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  --color: var(--primary-inverse);
}
button:focus,
input[type="submit"]:focus,
input[type="button"]:focus,
input[type="reset"]:focus,
[role="button"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--primary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary,
input[type="reset"] {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  cursor: pointer;
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:focus,
input[type="reset"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--secondary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast {
  --background-color: var(--contrast);
  --border-color: var(--contrast);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--contrast-hover);
  --border-color: var(--contrast-hover);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--contrast-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline,
input[type="reset"].outline {
  --background-color: transparent;
  --color: var(--primary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --background-color: transparent;
  --color: var(--primary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary,
input[type="reset"].outline {
  --color: var(--secondary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast {
  --color: var(--contrast);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}

:where(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  )[disabled],
:where(fieldset[disabled])
  :is(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  ),
a[role="button"]:not([href]) {
  opacity: 0.5;
  pointer-events: none;
}

/**
 * Form elements
 */
input,
optgroup,
select,
textarea {
  margin: 0;
  font-size: 1rem;
  line-height: var(--line-height);
  font-family: inherit;
  letter-spacing: inherit;
}

input {
  overflow: visible;
}

select {
  text-transform: none;
}

legend {
  max-width: 100%;
  padding: 0;
  color: inherit;
  white-space: normal;
}

textarea {
  overflow: auto;
}

[type="checkbox"],
[type="radio"] {
  padding: 0;
}

::-webkit-inner-spin-button,
::-webkit-outer-spin-button {
  height: auto;
}

[type="search"] {
  -webkit-appearance: textfield;
  outline-offset: -2px;
}

[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}

::-webkit-file-upload-button {
  -webkit-appearance: button;
  font: inherit;
}

::-moz-focus-inner {
  padding: 0;
  border-style: none;
}

:-moz-focusring {
  outline: none;
}

:-moz-ui-invalid {
  box-shadow: none;
}

::-ms-expand {
  display: none;
}

[type="file"],
[type="range"] {
  padding: 0;
  border-width: 0;
}

input:not([type="checkbox"], [type="radio"], [type="range"]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
}

fieldset {
  margin: 0;
  margin-bottom: var(--spacing);
  padding: 0;
  border: 0;
}

label,
fieldset legend {
  display: block;
  margin-bottom: calc(var(--spacing) * 0.25);
  font-weight: var(--form-label-font-weight, var(--font-weight));
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  width: 100%;
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]),
select,
textarea {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
}

input,
select,
textarea {
  --background-color: var(--form-element-background-color);
  --border-color: var(--form-element-border-color);
  --color: var(--form-element-color);
  --box-shadow: none;
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="checkbox"],
    [type="radio"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --background-color: var(--form-element-active-background-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="switch"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --border-color: var(--form-element-active-border-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="range"],
    [type="file"],
    [readonly]
  ):focus,
select:focus,
textarea:focus {
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}

input:not([type="submit"], [type="button"], [type="reset"])[disabled],
select[disabled],
textarea[disabled],
:where(fieldset[disabled])
  :is(
    input:not([type="submit"], [type="button"], [type="reset"]),
    select,
    textarea
  ) {
  --background-color: var(--form-element-disabled-background-color);
  --border-color: var(--form-element-disabled-border-color);
  opacity: var(--form-element-disabled-opacity);
  pointer-events: none;
}

:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid] {
  padding-right: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal) !important;
  padding-inline-start: var(--form-element-spacing-horizontal) !important;
  -webkit-padding-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-inline-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="false"] {
  background-image: var(--icon-valid);
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="true"] {
  background-image: var(--icon-invalid);
}
:where(input, select, textarea)[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(input, select, textarea)[aria-invalid="false"]:is(:active, :focus) {
  --border-color: var(--form-element-valid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-valid-focus-color) !important;
}
:where(input, select, textarea)[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}
:where(input, select, textarea)[aria-invalid="true"]:is(:active, :focus) {
  --border-color: var(--form-element-invalid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width)
    var(--form-element-invalid-focus-color) !important;
}

[dir="rtl"]
  :where(input, select, textarea):not([type="checkbox"], [type="radio"]):is(
    [aria-invalid],
    [aria-invalid="true"],
    [aria-invalid="false"]
  ) {
  background-position: center left 0.75rem;
}

input::placeholder,
input::-webkit-input-placeholder,
textarea::placeholder,
textarea::-webkit-input-placeholder,
select:invalid {
  color: var(--form-element-placeholder-color);
  opacity: 1;
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  margin-bottom: var(--spacing);
}

select::-ms-expand {
  border: 0;
  background-color: transparent;
}
select:not([multiple], [size]) {
  padding-right: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal);
  padding-inline-start: var(--form-element-spacing-horizontal);
  -webkit-padding-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-inline-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  background-image: var(--icon-chevron);
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}

[dir="rtl"] select:not([multiple], [size]) {
  background-position: center left 0.75rem;
}

:where(input, select, textarea) + small {
  display: block;
  width: 100%;
  margin-top: calc(var(--spacing) * -0.75);
  margin-bottom: var(--spacing);
  color: var(--muted-color);
}

label > :where(input, select, textarea) {
  margin-top: calc(var(--spacing) * 0.25);
}

/**
 * Form elements
 * Checkboxes & Radios
 */
[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

[type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
[type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
[type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
[type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
[type="checkbox"][role="switch"]:checked {
  background-image: none;
}
[type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

[type="checkbox"][aria-invalid="false"],
[type="checkbox"]:checked[aria-invalid="false"],
[type="radio"][aria-invalid="false"],
[type="radio"]:checked[aria-invalid="false"],
[type="checkbox"][role="switch"][aria-invalid="false"],
[type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
[type="checkbox"][aria-invalid="true"],
[type="checkbox"]:checked[aria-invalid="true"],
[type="radio"][aria-invalid="true"],
[type="radio"]:checked[aria-invalid="true"],
[type="checkbox"][role="switch"][aria-invalid="true"],
[type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

/**
 * Form elements
 * Alternatives input types (Not Checkboxes & Radios)
 */
[type="color"]::-webkit-color-swatch-wrapper {
  padding: 0;
}
[type="color"]::-moz-focus-inner {
  padding: 0;
}
[type="color"]::-webkit-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}
[type="color"]::-moz-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]):is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  --icon-position: 0.75rem;
  --icon-width: 1rem;
  padding-right: calc(var(--icon-width) + var(--icon-position));
  background-image: var(--icon-date);
  background-position: center right var(--icon-position);
  background-size: var(--icon-width) auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="time"] {
  background-image: var(--icon-time);
}

[type="date"]::-webkit-calendar-picker-indicator,
[type="datetime-local"]::-webkit-calendar-picker-indicator,
[type="month"]::-webkit-calendar-picker-indicator,
[type="time"]::-webkit-calendar-picker-indicator,
[type="week"]::-webkit-calendar-picker-indicator {
  width: var(--icon-width);
  margin-right: calc(var(--icon-width) * -1);
  margin-left: var(--icon-position);
  opacity: 0;
}

[dir="rtl"]
  :is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  text-align: right;
}

[type="file"] {
  --color: var(--muted-color);
  padding: calc(var(--form-element-spacing-vertical) * 0.5) 0;
  border: 0;
  border-radius: 0;
  background: none;
}
[type="file"]::file-selector-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::file-selector-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-webkit-file-upload-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-webkit-file-upload-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-ms-browse {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  margin-inline-start: 0;
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-ms-browse:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}

[type="range"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 100%;
  height: 1.25rem;
  background: none;
}
[type="range"]::-webkit-slider-runnable-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -webkit-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-moz-range-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -moz-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-ms-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -ms-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-webkit-slider-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-moz-range-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -moz-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-ms-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]:hover,
[type="range"]:focus {
  --range-border-color: var(--range-active-border-color);
  --range-thumb-color: var(--range-thumb-hover-color);
}
[type="range"]:active {
  --range-thumb-color: var(--range-thumb-active-color);
}
[type="range"]:active::-webkit-slider-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-moz-range-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-ms-thumb {
  transform: scale(1.25);
}

input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  -webkit-padding-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  padding-inline-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  border-radius: 5rem;
  background-image: var(--icon-search);
  background-position: center left 1.125rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  -webkit-padding-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  padding-inline-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  background-position: center left 1.125rem, center right 0.75rem;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="false"] {
  background-image: var(--icon-search), var(--icon-valid);
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="true"] {
  background-image: var(--icon-search), var(--icon-invalid);
}

[type="search"]::-webkit-search-cancel-button {
  -webkit-appearance: none;
  display: none;
}

[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  background-position: center right 1.125rem;
}
[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  background-position: center right 1.125rem, center left 0.75rem;
}

/**
 * Table
 */
:where(table) {
  width: 100%;
  border-collapse: collapse;
  border-spacing: 0;
  text-indent: 0;
}

th,
td {
  padding: calc(var(--spacing) / 2) var(--spacing);
  border-bottom: var(--border-width) solid var(--table-border-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  text-align: left;
  text-align: start;
}

tfoot th,
tfoot td {
  border-top: var(--border-width) solid var(--table-border-color);
  border-bottom: 0;
}

table[role="grid"] tbody tr:nth-child(odd) {
  background-color: var(--table-row-stripped-background-color);
}

/**
 * Code
 */
pre,
code,
kbd,
samp {
  font-size: 0.875em;
  font-family: var(--font-family);
}

pre {
  -ms-overflow-style: scrollbar;
  overflow: auto;
}

pre,
code,
kbd {
  border-radius: var(--border-radius);
  background: var(--code-background-color);
  color: var(--code-color);
  font-weight: var(--font-weight);
  line-height: initial;
}

code,
kbd {
  display: inline-block;
  padding: 0.375rem 0.5rem;
}

pre {
  display: block;
  margin-bottom: var(--spacing);
  overflow-x: auto;
}
pre > code {
  display: block;
  padding: var(--spacing);
  background: none;
  font-size: 14px;
  line-height: var(--line-height);
}

code b {
  color: var(--code-tag-color);
  font-weight: var(--font-weight);
}
code i {
  color: var(--code-property-color);
  font-style: normal;
}
code u {
  color: var(--code-value-color);
  text-decoration: none;
}
code em {
  color: var(--code-comment-color);
  font-style: normal;
}

kbd {
  background-color: var(--code-kbd-background-color);
  color: var(--code-kbd-color);
  vertical-align: baseline;
}

/**
 * Miscs
 */
hr {
  height: 0;
  border: 0;
  border-top: 1px solid var(--muted-border-color);
  color: inherit;
}

[hidden],
template {
  display: none !important;
}

canvas {
  display: inline-block;
}

/**
 * Accordion (<details>)
 */
details {
  display: block;
  margin-bottom: var(--spacing);
  padding-bottom: var(--spacing);
  border-bottom: var(--border-width) solid var(--accordion-border-color);
}
details summary {
  line-height: 1rem;
  list-style-type: none;
  cursor: pointer;
  transition: color var(--transition);
}
details summary:not([role]) {
  color: var(--accordion-close-summary-color);
}
details summary::-webkit-details-marker {
  display: none;
}
details summary::marker {
  display: none;
}
details summary::-moz-list-bullet {
  list-style-type: none;
}
details summary::after {
  display: block;
  width: 1rem;
  height: 1rem;
  -webkit-margin-start: calc(var(--spacing, 1rem) * 0.5);
  margin-inline-start: calc(var(--spacing, 1rem) * 0.5);
  float: right;
  transform: rotate(-90deg);
  background-image: var(--icon-chevron);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
  transition: transform var(--transition);
}
details summary:focus {
  outline: none;
}
details summary:focus:not([role="button"]) {
  color: var(--accordion-active-summary-color);
}
details summary[role="button"] {
  width: 100%;
  text-align: left;
}
details summary[role="button"]::after {
  height: calc(1rem * var(--line-height, 1.5));
  background-image: var(--icon-chevron-button);
}
details summary[role="button"]:not(.outline).contrast::after {
  background-image: var(--icon-chevron-button-inverse);
}
details[open] > summary {
  margin-bottom: calc(var(--spacing));
}
details[open] > summary:not([role]):not(:focus) {
  color: var(--accordion-open-summary-color);
}
details[open] > summary::after {
  transform: rotate(0);
}

[dir="rtl"] details summary {
  text-align: right;
}
[dir="rtl"] details summary::after {
  float: left;
  background-position: left center;
}

/**
 * Card (<article>)
 */
article {
  margin: var(--block-spacing-vertical) 0;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
  border-radius: var(--border-radius);
  background: var(--card-background-color);
  box-shadow: var(--card-box-shadow);
}
article > header,
article > footer {
  margin-right: calc(var(--block-spacing-horizontal) * -1);
  margin-left: calc(var(--block-spacing-horizontal) * -1);
  padding: calc(var(--block-spacing-vertical) * 0.66)
    var(--block-spacing-horizontal);
  background-color: var(--card-sectionning-background-color);
}
article > header {
  margin-top: calc(var(--block-spacing-vertical) * -1);
  margin-bottom: var(--block-spacing-vertical);
  border-bottom: var(--border-width) solid var(--card-border-color);
  border-top-right-radius: var(--border-radius);
  border-top-left-radius: var(--border-radius);
}
article > footer {
  margin-top: var(--block-spacing-vertical);
  margin-bottom: calc(var(--block-spacing-vertical) * -1);
  border-top: var(--border-width) solid var(--card-border-color);
  border-bottom-right-radius: var(--border-radius);
  border-bottom-left-radius: var(--border-radius);
}

/**
 * Modal (<dialog>)
 */
#mount {
  --scrollbar-width: 0px;
}

dialog {
  display: flex;
  z-index: 999;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  align-items: center;
  justify-content: center;
  width: inherit;
  min-width: 100%;
  height: inherit;
  min-height: 100%;
  padding: var(--spacing);
  border: 0;
  -webkit-backdrop-filter: var(--modal-overlay-backdrop-filter);
  backdrop-filter: var(--modal-overlay-backdrop-filter);
  background-color: var(--modal-overlay-background-color);
  color: var(--color);
}
dialog article {
  max-height: calc(100vh - var(--spacing) * 2);
  overflow: auto;
}
@media (min-width: 576px) {
  dialog article {
    max-width: 510px;
  }
}
@media (min-width: 768px) {
  dialog article {
    max-width: 700px;
  }
}
dialog article > header,
dialog article > footer {
  padding: calc(var(--block-spacing-vertical) * 0.5)
    var(--block-spacing-horizontal);
}
dialog article > header .close {
  margin: 0;
  margin-left: var(--spacing);
  float: right;
}
dialog article > footer {
  text-align: right;
}
dialog article > footer [role="button"] {
  margin-bottom: 0;
}
dialog article > footer [role="button"]:not(:first-of-type) {
  margin-left: calc(var(--spacing) * 0.5);
}
dialog article p:last-of-type {
  margin: 0;
}
dialog article .close {
  display: block;
  width: 1rem;
  height: 1rem;
  margin-top: calc(var(--block-spacing-vertical) * -0.5);
  margin-bottom: var(--typography-spacing-vertical);
  margin-left: auto;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}
dialog article .close:is([aria-current], :hover, :active, :focus) {
  opacity: 1;
}
dialog:not([open]),
dialog[open="false"] {
  display: none;
}

.modal-is-open {
  padding-right: var(--scrollbar-width, 0px);
  overflow: hidden;
  pointer-events: none;
}
.modal-is-open dialog {
  pointer-events: auto;
}

:where(.modal-is-opening, .modal-is-closing) dialog,
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-duration: 0.2s;
  animation-timing-function: ease-in-out;
  animation-fill-mode: both;
}
:where(.modal-is-opening, .modal-is-closing) dialog {
  animation-duration: 0.8s;
  animation-name: modal-overlay;
}
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-delay: 0.2s;
  animation-name: modal;
}

.modal-is-closing dialog,
.modal-is-closing dialog > article {
  animation-delay: 0s;
  animation-direction: reverse;
}

@keyframes modal-overlay {
  from {
    -webkit-backdrop-filter: none;
    backdrop-filter: none;
    background-color: transparent;
  }
}
@keyframes modal {
  from {
    transform: translateY(-100%);
    opacity: 0;
  }
}
/**
 * Nav
 */
:where(nav li)::before {
  float: left;
  content: "​";
}

nav,
nav ul {
  display: flex;
}

nav {
  justify-content: space-between;
}
nav ol,
nav ul {
  align-items: center;
  margin-bottom: 0;
  padding: 0;
  list-style: none;
}
nav ol:first-of-type,
nav ul:first-of-type {
  margin-left: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav ol:last-of-type,
nav ul:last-of-type {
  margin-right: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav li {
  display: inline-block;
  margin: 0;
  padding: var(--nav-element-spacing-vertical)
    var(--nav-element-spacing-horizontal);
}
nav li > * {
  --spacing: 0;
}
nav :where(a, [role="link"]) {
  display: inline-block;
  margin: calc(var(--nav-link-spacing-vertical) * -1)
    calc(var(--nav-link-spacing-horizontal) * -1);
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
  border-radius: var(--border-radius);
  text-decoration: none;
}
nav :where(a, [role="link"]):is([aria-current], :hover, :active, :focus) {
  text-decoration: none;
}
nav[aria-label="breadcrumb"] {
  align-items: center;
  justify-content: start;
}
nav[aria-label="breadcrumb"] ul li:not(:first-child) {
  -webkit-margin-start: var(--nav-link-spacing-horizontal);
  margin-inline-start: var(--nav-link-spacing-horizontal);
}
nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  position: absolute;
  width: calc(var(--nav-link-spacing-horizontal) * 2);
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) / 2);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) / 2);
  content: "/";
  color: var(--muted-color);
  text-align: center;
}
nav[aria-label="breadcrumb"] a[aria-current] {
  background-color: transparent;
  color: inherit;
  text-decoration: none;
  pointer-events: none;
}
nav [role="button"] {
  margin-right: inherit;
  margin-left: inherit;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}

aside nav,
aside ol,
aside ul,
aside li {
  display: block;
}
aside li {
  padding: calc(var(--nav-element-spacing-vertical) * 0.5)
    var(--nav-element-spacing-horizontal);
}
aside li a {
  display: block;
}
aside li [role="button"] {
  margin: inherit;
}

[dir="rtl"] nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  content: "\\";
}

/**
 * Progress
 */
progress {
  display: inline-block;
  vertical-align: baseline;
}

progress {
  -webkit-appearance: none;
  -moz-appearance: none;
  display: inline-block;
  appearance: none;
  width: 100%;
  height: 0.5rem;
  margin-bottom: calc(var(--spacing) * 0.5);
  overflow: hidden;
  border: 0;
  border-radius: var(--border-radius);
  background-color: var(--progress-background-color);
  color: var(--progress-color);
}
progress::-webkit-progress-bar {
  border-radius: var(--border-radius);
  background: none;
}
progress[value]::-webkit-progress-value {
  background-color: var(--progress-color);
}
progress::-moz-progress-bar {
  background-color: var(--progress-color);
}
@media (prefers-reduced-motion: no-preference) {
  progress:indeterminate {
    background: var(--progress-background-color)
      linear-gradient(
        to right,
        var(--progress-color) 30%,
        var(--progress-background-color) 30%
      )
      top left/150% 150% no-repeat;
    animation: progress-indeterminate 1s linear infinite;
  }
  progress:indeterminate[value]::-webkit-progress-value {
    background-color: transparent;
  }
  progress:indeterminate::-moz-progress-bar {
    background-color: transparent;
  }
}

@media (prefers-reduced-motion: no-preference) {
  [dir="rtl"] progress:indeterminate {
    animation-direction: reverse;
  }
}

@keyframes progress-indeterminate {
  0% {
    background-position: 200% 0;
  }
  100% {
    background-position: -200% 0;
  }
}
/**
 * Dropdown ([role="list"])
 */
details[role="list"],
li[role="list"] {
  position: relative;
}

details[role="list"] summary + ul,
li[role="list"] > ul {
  display: flex;
  z-index: 99;
  position: absolute;
  top: auto;
  right: 0;
  left: 0;
  flex-direction: column;
  margin: 0;
  padding: 0;
  border: var(--border-width) solid var(--dropdown-border-color);
  border-radius: var(--border-radius);
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  background-color: var(--dropdown-background-color);
  box-shadow: var(--card-box-shadow);
  color: var(--dropdown-color);
  white-space: nowrap;
}
details[role="list"] summary + ul li,
li[role="list"] > ul li {
  width: 100%;
  margin-bottom: 0;
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  list-style: none;
}
details[role="list"] summary + ul li:first-of-type,
li[role="list"] > ul li:first-of-type {
  margin-top: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li:last-of-type,
li[role="list"] > ul li:last-of-type {
  margin-bottom: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li a,
li[role="list"] > ul li a {
  display: block;
  margin: calc(var(--form-element-spacing-vertical) * -0.5)
    calc(var(--form-element-spacing-horizontal) * -1);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  overflow: hidden;
  color: var(--dropdown-color);
  text-decoration: none;
  text-overflow: ellipsis;
}
details[role="list"] summary + ul li a:hover,
li[role="list"] > ul li a:hover {
  background-color: var(--dropdown-hover-background-color);
}

details[role="list"] summary::after,
li[role="list"] > a::after {
  display: block;
  width: 1rem;
  height: calc(1rem * var(--line-height, 1.5));
  -webkit-margin-start: 0.5rem;
  margin-inline-start: 0.5rem;
  float: right;
  transform: rotate(0deg);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
}

details[role="list"] {
  padding: 0;
  border-bottom: none;
}
details[role="list"] summary {
  margin-bottom: 0;
}
details[role="list"] summary:not([role]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--form-element-border-color);
  border-radius: var(--border-radius);
  background-color: var(--form-element-background-color);
  color: var(--form-element-placeholder-color);
  line-height: inherit;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
details[role="list"] summary:not([role]):active,
details[role="list"] summary:not([role]):focus {
  border-color: var(--form-element-active-border-color);
  background-color: var(--form-element-active-background-color);
}
details[role="list"] summary:not([role]):focus {
  box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}
details[role="list"][open] summary {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
details[role="list"][open] summary::before {
  display: block;
  z-index: 1;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  background: none;
  content: "";
  cursor: default;
}

nav details[role="list"] summary,
nav li[role="list"] a {
  display: flex;
  direction: ltr;
}

nav details[role="list"] summary + ul,
nav li[role="list"] > ul {
  min-width: -moz-fit-content;
  min-width: fit-content;
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul li a,
nav li[role="list"] > ul li a {
  border-radius: 0;
}

nav details[role="list"] summary,
nav details[role="list"] summary:not([role]) {
  height: auto;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}
nav details[role="list"][open] summary {
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul {
  margin-top: var(--outline-width);
  -webkit-margin-start: 0;
  margin-inline-start: 0;
}
nav details[role="list"] summary[role="link"] {
  margin-bottom: calc(var(--nav-link-spacing-vertical) * -1);
  line-height: var(--line-height);
}
nav details[role="list"] summary[role="link"] + ul {
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) * -1);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) * -1);
}

li[role="list"]:hover > ul,
li[role="list"] a:active ~ ul,
li[role="list"] a:focus ~ ul {
  display: flex;
}
li[role="list"] > ul {
  display: none;
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
  margin-inline-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
}
li[role="list"] > a::after {
  background-image: var(--icon-chevron);
}

/**
 * Loading ([aria-busy=true])
 */
[aria-busy="true"] {
  cursor: progress;
}

[aria-busy="true"]:not(input, select, textarea)::before {
  display: inline-block;
  width: 1em;
  height: 1em;
  border: 0.1875em solid currentColor;
  border-radius: 1em;
  border-right-color: transparent;
  content: "";
  vertical-align: text-bottom;
  vertical-align: -0.125em;
  animation: spinner 0.75s linear infinite;
  opacity: var(--loading-spinner-opacity);
}
[aria-busy="true"]:not(input, select, textarea):not(:empty)::before {
  margin-right: calc(var(--spacing) * 0.5);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) * 0.5);
  margin-inline-end: calc(var(--spacing) * 0.5);
}
[aria-busy="true"]:not(input, select, textarea):empty {
  text-align: center;
}

button[aria-busy="true"],
input[type="submit"][aria-busy="true"],
input[type="button"][aria-busy="true"],
input[type="reset"][aria-busy="true"],
a[aria-busy="true"] {
  pointer-events: none;
}

@keyframes spinner {
  to {
    transform: rotate(360deg);
  }
}
/**
 * Tooltip ([data-tooltip])
 */
[data-tooltip] {
  position: relative;
}
[data-tooltip]:not(a, button, input) {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}
[data-tooltip][data-placement="top"]::before,
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::before,
[data-tooltip]::after {
  display: block;
  z-index: 99;
  position: absolute;
  bottom: 100%;
  left: 50%;
  padding: 0.25rem 0.5rem;
  overflow: hidden;
  transform: translate(-50%, -0.25rem);
  border-radius: var(--border-radius);
  background: var(--tooltip-background-color);
  content: attr(data-tooltip);
  color: var(--tooltip-color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: 0.875rem;
  text-decoration: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
}
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::after {
  padding: 0;
  transform: translate(-50%, 0rem);
  border-top: 0.3rem solid;
  border-right: 0.3rem solid transparent;
  border-left: 0.3rem solid transparent;
  border-radius: 0;
  background-color: transparent;
  content: "";
  color: var(--tooltip-background-color);
}
[data-tooltip][data-placement="bottom"]::before,
[data-tooltip][data-placement="bottom"]::after {
  top: 100%;
  bottom: auto;
  transform: translate(-50%, 0.25rem);
}
[data-tooltip][data-placement="bottom"]:after {
  transform: translate(-50%, -0.3rem);
  border: 0.3rem solid transparent;
  border-bottom: 0.3rem solid;
}
[data-tooltip][data-placement="left"]::before,
[data-tooltip][data-placement="left"]::after {
  top: 50%;
  right: 100%;
  bottom: auto;
  left: auto;
  transform: translate(-0.25rem, -50%);
}
[data-tooltip][data-placement="left"]:after {
  transform: translate(0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-left: 0.3rem solid;
}
[data-tooltip][data-placement="right"]::before,
[data-tooltip][data-placement="right"]::after {
  top: 50%;
  right: auto;
  bottom: auto;
  left: 100%;
  transform: translate(0.25rem, -50%);
}
[data-tooltip][data-placement="right"]:after {
  transform: translate(-0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-right: 0.3rem solid;
}
[data-tooltip]:focus::before,
[data-tooltip]:focus::after,
[data-tooltip]:hover::before,
[data-tooltip]:hover::after {
  opacity: 1;
}
@media (hover: hover) and (pointer: fine) {
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::before,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::before,
  [data-tooltip]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::after {
    animation-name: tooltip-caret-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::before,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-bottom;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-name: tooltip-caret-slide-bottom;
  }
  [data-tooltip][data-placement="left"]:focus::before,
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::before,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-left;
  }
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-name: tooltip-caret-slide-left;
  }
  [data-tooltip][data-placement="right"]:focus::before,
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::before,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-right;
  }
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-name: tooltip-caret-slide-right;
  }
}
@keyframes tooltip-slide-top {
  from {
    transform: translate(-50%, 0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-top {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.25rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-bottom {
  from {
    transform: translate(-50%, -0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-bottom {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.5rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.3rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-left {
  from {
    transform: translate(0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-left {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.3rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-slide-right {
  from {
    transform: translate(-0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-right {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.3rem, -50%);
    opacity: 1;
  }
}

/**
 * Accessibility & User interaction
 */
[aria-controls] {
  cursor: pointer;
}

[aria-disabled="true"],
[disabled] {
  cursor: not-allowed;
}

[aria-hidden="false"][hidden] {
  display: initial;
}

[aria-hidden="false"][hidden]:not(:focus) {
  clip: rect(0, 0, 0, 0);
  position: absolute;
}

a,
area,
button,
input,
label,
select,
summary,
textarea,
[tabindex] {
  -ms-touch-action: manipulation;
}

[dir="rtl"] {
  direction: rtl;
}

/**
* Reduce Motion Features
*/
@media (prefers-reduced-motion: reduce) {
  *:not([aria-busy="true"]),
  :not([aria-busy="true"])::before,
  :not([aria-busy="true"])::after {
    background-attachment: initial !important;
    animation-duration: 1ms !important;
    animation-delay: -1ms !important;
    animation-iteration-count: 1 !important;
    scroll-behavior: auto !important;
    transition-delay: 0s !important;
    transition-duration: 0s !important;
  }
}

#mount#mount {
  /* --primary: rgb(227, 59, 126); */
  --primary: #ea4c89;
  --primary-hover: #f082ac;
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  --switch-checked-background-color: var(--primary);
}

li.select-link.select-link:hover > ul {
  display: none;
}
li.select-link.select-link > ul {
  display: none;
}
li.select-link.select-link a:focus ~ ul {
  display: none;
}

li.select-link.select-link a:active ~ ul {
  display: none;
}
li.select-link-active.select-link-active > ul {
  display: flex;
}
li.select-link-active.select-link-active:hover > ul {
  display: flex;
}

li.select-link-active.select-link-active a:focus ~ ul {
  display: flex;
}

li.select-link-active.select-link-active a:active ~ ul {
  display: flex;
}
ul.select-link-ul.select-link-ul {
  right: 0px;
  left: auto;
}

a.select-link-selected {
  background-color: var(--primary-focus);
}
.immersive-translate-no-select {
  -webkit-touch-callout: none; /* iOS Safari */
  -webkit-user-select: none; /* Safari */
  -khtml-user-select: none; /* Konqueror HTML */
  -moz-user-select: none; /* Old versions of Firefox */
  -ms-user-select: none; /* Internet Explorer/Edge */
  user-select: none;
}

/* li[role="list"].no-arrow > a::after { */
/*   background-image: none; */
/*   width: 0; */
/*   color: var(--color); */
/* } */
li[role="list"].no-arrow {
  margin-left: 8px;
  padding-right: 0;
}
li[role="list"] > a::after {
  -webkit-margin-start: 0.2rem;
  margin-inline-start: 0.2rem;
}

li[role="list"].no-arrow > a,
li[role="list"].no-arrow > a:link,
li[role="list"].no-arrow > a:visited {
  color: var(--secondary);
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 4px;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  padding-left: 8px;
  text-overflow: ellipsis;
  color: var(--color);

}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}
select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.muted {
  color: var(--muted-color);
}

.select.button-select {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
  cursor: pointer;
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 16px;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
  -webkit-appearance: button;
  margin: 0;
  margin-bottom: 0px;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

html {
  font-size: 16px;
  --font-size: 16px;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

#mount#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --text-black-2: #222222;
  --text-gray-2: #222222;
  --text-gray-6: #666666;
  --text-gray-9: #999999;
  --text-gray-c2: #c2c2c2;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --popup-footer-background-color: #0d0d0d;
    --popup-content-background-color: #191919;
    --popup-item-background-color: #272727;
    --popup-item-hover-background-color: #333333;
    --text-black-2: #ffffff;
    --text-gray-2: #dbdbdb;
    --text-gray-6: #b3b3b3;
    --text-gray-9: #777777;
    --text-gray-c2: #5b5b5b;
  }
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --text-black-2: #ffffff;
  --text-gray-2: #dbdbdb;
  --text-gray-6: #b3b3b3;
  --text-gray-9: #777777;
  --text-gray-c2: #5b5b5b;
}

.text-balck {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

#mount {
  min-width: 268px;
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
}

select.translate-service {
  color: var(--text-black-2);
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {

  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

select.min-select-secondary {
  color: var(--color);
}

select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
  border-radius: 12px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: center;
  justify-content: space-between;
}

.widgets-container> :not(:last-child) {
  margin-right: 8px;
}

.widget-item {
  display: flex;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  height: 44px;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
}

.widget-item svg {
  fill: var(--text-gray-2);
}

.setting svg {
  fill: var(--text-gray-6);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}
html {
  font-size: 17px;
}

#mount#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}


/* float-ball */
.immersive-translate-float-ball-container {
  position: fixed;
  padding: 0;
  z-index: 2147483647;
  top: 335px;
  width: 56px;
  display: flex;
  flex-direction: column;
  display: none;
}

.immersive-translate-float-ball-container.left {
  align-items: flex-start;
  left: 0;
}

.immersive-translate-float-ball-container.right {
  align-items: flex-end;
  right: 0;
}

.immersive-translate-float-ball-btn {
  background: linear-gradient(320.9deg, #DB3B7B 26.47%, #FFCEE2 88.86%);
  height: 36px;
  width: 56px;
  box-shadow: 2px 6px 10px 0px #0E121629;
}

.immersive-translate-float-ball-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
}


.immersive-translate-float-ball-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}



.immersive-translate-float-ball-btn div {
  background: linear-gradient(140.91deg, #FF87B7 12.61%, #EC4C8C 76.89%);
  height: 34px;
  width: 54px;
  margin: 1px;
  display: flex;
  align-items: center;
}


.immersive-translate-float-ball-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.immersive-translate-float-ball-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}


.immersive-translate-float-ball-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.immersive-translate-float-ball-translated-img {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 20px;
}

.btn-animate {
  cursor: pointer;
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.immersive-translate-float-ball-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0E121614;
  border: none;
  overflow: hidden;
}

.immersive-translate-float-ball-close-content {
  padding: 22px;
  width: 320px;
}

.immersive-translate-float-ball-close-title {
  font-weight: 500;
  color: var(--h2-color);
}

.immersive-translate-float-ball-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.immersive-translate-float-ball-radio-sel,
.immersive-translate-float-ball-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.immersive-translate-float-ball-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.immersive-translate-float-ball-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.immersive-translate-float-ball-radio-nor {
  border: 2px solid #D3D4D6;
}



.immersive-translate-float-ball-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.immersive-translate-float-ball-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
}


.immersive-translate-float-ball-guide-container {
  width: 312px;
  transform: translateY(-50%);
}

.immersive-translate-float-ball-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 100%;
}

.immersive-translate-float-ball-guide-bg.left {
  transform: scaleX(-1);
}

.immersive-translate-float-ball-guide-content {
  margin: 16px 32px 60px 21px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.immersive-translate-float-ball-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.immersive-translate-float-ball-guide-img {
  width: 235px;
  margin-top: 16px;
}


.immersive-translate-float-ball-guide-message {
  font-size: 16px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-top: 10px;
}


.immersive-translate-float-ball-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.immersive-translate-float-ball-more-buttons {
  box-shadow: 0px 2px 10px 0px #00000014;
  border: 1px solid var(--float-ball-more-button-border-color);
  background: var(--float-ball-more-button-background-color);
  width: 36px;
  display: flex;
  flex-direction: column;
  border-radius: 18px;
  margin-right: 8px;
}

.immersive-translate-float-ball-more-button {
  width: 36px;
  height: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}

.immersive-translate-float-ball-more-buttons svg path {
  fill: var(--float-ball-more-button-svg-color);
}

/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: white;
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}</style><div id="mount" style="display: block;"><div class="immersive-translate-float-ball-container right notranslate" style="z-index: 2147483647; pointer-events: none; top: 230px; display: flex;"><div title="Close Floating Button" class="btn-animate" style="transform: translateX(100%); padding: 4px;"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div><div style="position: relative; pointer-events: all; display: inline-block;"><div><div class="immersive-translate-float-ball-btn  right btn-animate " style="transform: translateX(20px); opacity: 0.5;"><div><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24" class="immersive-translate-float-ball-logo-img"><path fill="none" d="M0 0h24v24H0z"></path><path d="M5 15v2a2 2 0 0 0 1.85 1.995L7 19h3v2H7a4 4 0 0 1-4-4v-2h2zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2zm-1 2.885L15.753 16h2.492L17 12.885zM8 2v2h4v7H8v3H6v-3H2V4h4V2h2zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3zM6 6H4v3h2V6zm4 0H8v3h2V6z" fill="rgba(255,255,255,1)"></path></svg></div></div></div></div><div class="immersive-translate-float-ball-more-buttons btn-animate" style="margin-top: 12px; transform: translateX(60px);"><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="immersive-translate-float-ball-more-button"><svg width="18" height="18" viewBox="0 0 18 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.6861 1L15.2353 4.54635V7.11765V14.6471V15.5882C15.2353 15.9627 15.0866 16.3217 14.8218 16.5865C14.557 16.8513 14.198 17 13.8235 17H4.41176C4.03734 17 3.67825 16.8513 3.4135 16.5865C3.14874 16.3217 3 15.9627 3 15.5882V14.6471V7.11765V2.41176C3 2.03734 3.14874 1.67825 3.4135 1.4135C3.67825 1.14874 4.03734 1 4.41176 1H11.6861ZM11.8692 3.17882V4.74212H13.4334L11.8692 3.17882ZM4.41171 15.5882V14.647V2.41176H10.4574L10.4578 6.15341H13.8235V14.647V15.5882H4.41171ZM12.7739 7.51746H5.46094V8.6155H12.7739V7.51746ZM5.46094 9.98805H12.7739V11.0861H5.46094V9.98805ZM9.5127 12.36H5.46094V13.458H9.5127V12.36Z" fill="#6C6F73"></path></svg></div></div></div><div class="btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="immersive-translate-float-ball-more-button"><svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M6.55741 0L9.06847 0.00329403C9.84824 0.00470579 10.4802 0.636235 10.4812 1.41647L10.4821 1.82588C10.9687 2.0278 11.4297 2.28671 11.8553 2.59718L12.1913 2.40329C12.516 2.21676 12.9013 2.1665 13.2629 2.26352C13.6246 2.36055 13.933 2.59695 14.1207 2.92094L15.3795 5.09365C15.5601 5.40546 15.6149 5.7744 15.5328 6.12523C15.4507 6.47606 15.2378 6.78235 14.9376 6.98165L14.8609 7.02871L14.5235 7.22353C14.5819 7.76273 14.5736 8.30708 14.4986 8.84424L14.7372 8.98259C15.0496 9.16307 15.2812 9.45606 15.3848 9.80165C15.4884 10.1472 15.456 10.5193 15.2944 10.8419L15.2553 10.9153L14.076 12.9576C13.8955 13.27 13.6025 13.5017 13.2569 13.6053C12.9113 13.7088 12.5392 13.6765 12.2167 13.5148L12.1433 13.4753L11.8172 13.2871C11.4074 13.5817 10.9651 13.8283 10.4991 14.0221L10.4995 14.5831C10.5 14.9434 10.3629 15.2904 10.1163 15.5532C9.86972 15.816 9.53215 15.9748 9.17247 15.9972L9.08306 16L6.57153 15.9967C6.19697 15.9961 5.83793 15.847 5.57312 15.5821C5.30831 15.3172 5.15932 14.9581 5.15883 14.5835L5.15788 13.9073C4.76852 13.7244 4.39771 13.5044 4.05059 13.2504L3.44918 13.5967C3.12448 13.7834 2.73902 13.8337 2.37726 13.7367C2.01551 13.6397 1.70698 13.4032 1.5193 13.0791L0.260473 10.9064C0.0799611 10.5945 0.0252226 10.2255 0.107423 9.87467C0.189623 9.52384 0.402569 9.21757 0.702826 9.01835L0.779062 8.97129L1.3913 8.61835C1.34424 8.17129 1.34188 7.71765 1.38706 7.26494L0.707532 6.87247C0.395061 6.69207 0.163305 6.39911 0.0596515 6.05351C-0.0440025 5.70791 -0.0117246 5.33577 0.149885 5.01318L0.189415 4.93976L1.36871 2.89741C1.54919 2.58502 1.84218 2.35337 2.18777 2.2498C2.53336 2.14624 2.90547 2.17859 3.228 2.34023L3.30141 2.37976L3.89436 2.72188C4.28027 2.42082 4.69854 2.1637 5.14141 1.95529L5.14047 1.41694C5.14001 1.05657 5.27707 0.709596 5.52367 0.446813C5.77028 0.184029 6.10786 0.0252343 6.46753 0.00282354L6.55741 0ZM6.55553 1.41506L6.55694 2.85271L5.74377 3.23576C5.39553 3.39906 5.06706 3.60094 4.764 3.83718L4.01247 4.424L2.62941 3.62494L2.59365 3.60518L1.41483 5.64753L2.88636 6.49694L2.79506 7.40612C2.75968 7.7598 2.76078 8.11619 2.79836 8.46965L2.8953 9.38541L1.48494 10.1976L2.7433 12.3704L4.14377 11.5647L4.88636 12.1087C5.15997 12.309 5.45231 12.4823 5.7593 12.6264L6.57106 13.008L6.57388 14.5816L9.08447 14.5849L9.08306 13.0791L9.95553 12.7158C10.3216 12.5635 10.6689 12.3698 10.9908 12.1384L11.7329 11.6047L12.8506 12.2499L14.0289 10.2075L12.9654 9.592L13.0972 8.64847C13.1561 8.22659 13.1628 7.79904 13.1169 7.37553L13.0181 6.45882L14.1555 5.80235L12.8967 3.62965L11.7645 4.28235L11.0214 3.74024C10.686 3.4956 10.3229 3.29152 9.93953 3.13224L9.06894 2.77082L9.06659 1.41835L6.55553 1.41506ZM9.37153 5.47624C10.0214 5.85201 10.4955 6.47036 10.6898 7.19547C10.8841 7.92058 10.7827 8.69316 10.4078 9.34353C10.2223 9.66543 9.97517 9.9476 9.68053 10.1739C9.38589 10.4002 9.04953 10.5662 8.69068 10.6623C8.33183 10.7585 7.95754 10.7829 7.58923 10.7343C7.22092 10.6856 6.86582 10.5648 6.54424 10.3788C5.89445 10.003 5.4204 9.38458 5.2262 8.65948C5.032 7.93438 5.13352 7.16184 5.50847 6.51153C5.69395 6.18963 5.94107 5.90746 6.23571 5.68117C6.53034 5.45488 6.86671 5.28891 7.22556 5.19275C7.58441 5.09659 7.9587 5.07213 8.32701 5.12077C8.69532 5.16942 9.05042 5.29021 9.372 5.47624H9.37153ZM6.73388 7.21835C6.54638 7.54388 6.49567 7.9305 6.5929 8.29336C6.69012 8.65623 6.92733 8.96571 7.25247 9.15388C7.41305 9.24679 7.59037 9.30712 7.77429 9.33143C7.9582 9.35574 8.14511 9.34355 8.32431 9.29556C8.50351 9.24757 8.67149 9.16472 8.81864 9.05174C8.96579 8.93877 9.08923 8.7979 9.18188 8.63718C9.55883 7.98353 9.356 7.15435 8.73435 6.74494L8.66377 6.70118L8.59035 6.66165C8.26834 6.49988 7.89663 6.46742 7.55145 6.57093C7.20626 6.67444 6.91375 6.90608 6.73388 7.21835Z" fill="#6C6F73"></path></svg></div></div></div></div></div></div></template></div></html>